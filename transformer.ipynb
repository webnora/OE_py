{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s_qNSzzyaCbD"
   },
   "source": [
    "##### Copyright 2019 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "jmjh290raIky"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "import pandas as pd\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J0Qjg6vuaHNt"
   },
   "source": [
    "# Transformer model for language understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AOpGoE2T-YXS"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/text/transformer\">\n",
    "    <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />\n",
    "    View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/transformer.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
    "    Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/transformer.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
    "    View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/text/transformer.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M-f8TnGpE_ex"
   },
   "source": [
    "This tutorial trains a <a href=\"https://arxiv.org/abs/1706.03762\" class=\"external\">Transformer model</a> to translate Portuguese to English. This is an advanced example that assumes knowledge of [text generation](text_generation.ipynb) and [attention](nmt_with_attention.ipynb).\n",
    "\n",
    "The core idea behind the Transformer model is *self-attention*—the ability to attend to different positions of the input sequence to compute a representation of that sequence. Transformer creates stacks of self-attention layers and is explained below in the sections *Scaled dot product attention* and *Multi-head attention*.\n",
    "\n",
    "A transformer model handles variable-sized input using stacks of self-attention layers instead of [RNNs](text_classification_rnn.ipynb) or [CNNs](../images/intro_to_cnns.ipynb). This general architecture has a number of advantages:\n",
    "\n",
    "* It make no assumptions about the temporal/spatial relationships across the data. This is ideal for processing a set of objects (for example, [StarCraft units](https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/#block-8)).\n",
    "* Layer outputs can be calculated in parallel, instead of a series like an RNN.\n",
    "* Distant items can affect each other's output without passing through many RNN-steps, or convolution layers (see [Scene Memory Transformer](https://arxiv.org/pdf/1903.03878.pdf) for example).\n",
    "* It can learn long-range dependencies. This is a challenge in many sequence tasks.\n",
    "\n",
    "The downsides of this architecture are:\n",
    "\n",
    "* For a time-series, the output for a time-step is calculated from the *entire history* instead of only the inputs and current hidden-state. This _may_ be less efficient.   \n",
    "* If the input *does* have a  temporal/spatial relationship, like text, some positional encoding must be added or the model will effectively see a bag of words. \n",
    "\n",
    "After training the model in this notebook, you will be able to input a Portuguese sentence and return the English translation.\n",
    "\n",
    "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/attention_map_portuguese.png\" width=\"800\" alt=\"Attention heatmap\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JjJJyJTZYebt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_datasets in /opt/conda/lib/python3.8/site-packages (4.2.0)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.8/site-packages (from tensorflow_datasets) (0.3.3)\n",
      "Requirement already satisfied: protobuf>=3.12.2 in /opt/conda/lib/python3.8/site-packages (from tensorflow_datasets) (3.15.6)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.8/site-packages (from tensorflow_datasets) (0.12.0)\n",
      "Requirement already satisfied: termcolor in /opt/conda/lib/python3.8/site-packages (from tensorflow_datasets) (1.1.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from tensorflow_datasets) (1.19.4)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from tensorflow_datasets) (1.15.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow_datasets) (2.25.0)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.8/site-packages (from tensorflow_datasets) (0.18.2)\n",
      "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /opt/conda/lib/python3.8/site-packages (from tensorflow_datasets) (5.1.2)\n",
      "Requirement already satisfied: tensorflow-metadata in /opt/conda/lib/python3.8/site-packages (from tensorflow_datasets) (0.29.0)\n",
      "Requirement already satisfied: attrs>=18.1.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow_datasets) (20.2.0)\n",
      "Requirement already satisfied: promise in /opt/conda/lib/python3.8/site-packages (from tensorflow_datasets) (2.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.8/site-packages (from tensorflow_datasets) (4.51.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->tensorflow_datasets) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->tensorflow_datasets) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->tensorflow_datasets) (2020.6.20)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->tensorflow_datasets) (1.25.11)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow-metadata->tensorflow_datasets) (1.53.0)\n",
      "Collecting tensorflow_text\n",
      "  Using cached tensorflow_text-2.4.3-cp38-cp38-manylinux1_x86_64.whl (3.4 MB)\n",
      "Collecting tensorflow-hub>=0.8.0\n",
      "  Using cached tensorflow_hub-0.11.0-py2.py3-none-any.whl (107 kB)\n",
      "Collecting tensorflow<2.5,>=2.4.0\n",
      "  Downloading tensorflow-2.4.1-cp38-cp38-manylinux2010_x86_64.whl (394.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 394.4 MB 39 kB/s  eta 0:00:01     |███████████████▋                | 191.9 MB 1.5 MB/s eta 0:02:19\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow-hub>=0.8.0->tensorflow_text) (1.19.4)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow-hub>=0.8.0->tensorflow_text) (3.15.6)\n",
      "Collecting google-pasta~=0.2\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 932 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras-preprocessing~=1.1.2\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 764 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio~=1.32.0\n",
      "  Downloading grpcio-1.32.0-cp38-cp38-manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8 MB 930 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting astunparse~=1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: six~=1.15.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (1.15.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (1.1.0)\n",
      "Collecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting h5py~=2.10.0\n",
      "  Downloading h5py-2.10.0-cp38-cp38-manylinux1_x86_64.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 439 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wrapt~=1.12.1\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
      "Collecting opt-einsum~=3.3.0\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 597 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.8/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (0.35.1)\n",
      "Collecting tensorboard~=2.4\n",
      "  Downloading tensorboard-2.4.1-py3-none-any.whl (10.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.6 MB 931 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: absl-py~=0.10 in /opt/conda/lib/python3.8/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (0.12.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /opt/conda/lib/python3.8/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (3.7.4.3)\n",
      "Collecting flatbuffers~=1.12.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting tensorflow-estimator<2.5.0,>=2.4.0\n",
      "  Downloading tensorflow_estimator-2.4.0-py2.py3-none-any.whl (462 kB)\n",
      "\u001b[K     |████████████████████████████████| 462 kB 811 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting werkzeug>=0.11.15\n",
      "  Downloading Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n",
      "\u001b[K     |████████████████████████████████| 298 kB 927 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 1.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.28.0-py2.py3-none-any.whl (136 kB)\n",
      "\u001b[K     |████████████████████████████████| 136 kB 957 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (2.25.0)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.3-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (49.6.0.post20201009)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "\u001b[K     |████████████████████████████████| 781 kB 987 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.1-py3-none-any.whl (12 kB)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.6\"\n",
      "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 1.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (2020.6.20)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting pyasn1>=0.1.3\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 1.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (3.0.1)\n",
      "Building wheels for collected packages: wrapt\n",
      "  Building wheel for wrapt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wrapt: filename=wrapt-1.12.1-cp38-cp38-linux_x86_64.whl size=81738 sha256=e000dcd01753a27bb3b3ade7687ccd1c03fb77006848d70b660e3847e826abc7\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/5f/fd/9e/b6cf5890494cb8ef0b5eaff72e5d55a70fb56316007d6dfe73\n",
      "Successfully built wrapt\n",
      "Installing collected packages: tensorflow-hub, google-pasta, keras-preprocessing, grpcio, astunparse, gast, h5py, wrapt, opt-einsum, werkzeug, markdown, cachetools, pyasn1, rsa, pyasn1-modules, google-auth, requests-oauthlib, google-auth-oauthlib, tensorboard-plugin-wit, tensorboard, flatbuffers, tensorflow-estimator, tensorflow, tensorflow-text\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.1.0\n",
      "    Uninstalling h5py-3.1.0:\n",
      "      Successfully uninstalled h5py-3.1.0\n",
      "Successfully installed astunparse-1.6.3 cachetools-4.2.1 flatbuffers-1.12 gast-0.3.3 google-auth-1.28.0 google-auth-oauthlib-0.4.3 google-pasta-0.2.0 grpcio-1.32.0 h5py-2.10.0 keras-preprocessing-1.1.2 markdown-3.3.4 opt-einsum-3.3.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.7.2 tensorboard-2.4.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.4.1 tensorflow-estimator-2.4.0 tensorflow-hub-0.11.0 tensorflow-text-2.4.3 werkzeug-1.0.1 wrapt-1.12.1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "try:\n",
    "    !pip install tensorflow_datasets\n",
    "    !pip install tensorflow_text\n",
    "#     !pip install tf-nightly\n",
    "except Exception:\n",
    "  pass\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fd1NWMxjfsDd"
   },
   "source": [
    "## Setup input pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t4_Qt8W1hJE_"
   },
   "source": [
    "Use [TFDS](https://www.tensorflow.org/datasets) to load the [Portugese-English translation dataset](https://github.com/neulab/word-embeddings-for-nmt) from the [TED Talks Open Translation Project](https://www.ted.com/participate/translate).\n",
    "\n",
    "This dataset contains approximately 50000 training examples, 1100 validation examples, and 2000 test examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8q9t4FmN96eN"
   },
   "outputs": [],
   "source": [
    "# examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True,\n",
    "#                                as_supervised=True)\n",
    "# train_examples, val_examples = examples['train'], examples['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET_SIZE 2673 val_size 134\n"
     ]
    }
   ],
   "source": [
    "DATASET = 'data/iswoc-treebank.tsv'\n",
    "wc = !cat $DATASET | wc -l\n",
    "DATASET_SIZE = int(wc[0])\n",
    "train_size = int(0.95 * DATASET_SIZE)\n",
    "print('DATASET_SIZE', DATASET_SIZE, 'val_size', DATASET_SIZE - train_size)\n",
    "full_dataset = tf.data.experimental.CsvDataset(DATASET, [tf.string] * 2, field_delim='\\t')\n",
    "full_dataset = full_dataset.shuffle(buffer_size=10)\n",
    "train_examples = full_dataset.take(train_size)\n",
    "val_examples = full_dataset.skip(train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ðæs ðægn Philippus n æs na gefullod on Gode forþan þe cristendom n æs þa gyt geond eall cuð and seo reþe æhtnyss þa gyt n æs gestylled\n",
      "þes þegen Philippus ne wesan na gefullian on God forðam þe Cristendom ne wesan þa git geond eal cunnan and se reðe eahtnes þa git ne wesan gestillan\n"
     ]
    }
   ],
   "source": [
    "for a, b in train_examples.take(1):\n",
    "    tf.print(a)\n",
    "    tf.print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RCEKotqosGfq"
   },
   "source": [
    "Create a custom subwords tokenizer from the training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KVBg5Q8tBk5z"
   },
   "outputs": [],
   "source": [
    "tokenizer_forma = 'data/tokenizer_forma'\n",
    "tokenizer_lemma = 'data/tokenizer_lemma'\n",
    "if os.path.isfile(tokenizer_forma + \".subwords\") and os.path.isfile(tokenizer_lemma + \".subwords\"):\n",
    "    tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.load_from_file(tokenizer_forma)\n",
    "    tokenizer_pt = tfds.deprecated.text.SubwordTextEncoder.load_from_file(tokenizer_lemma)\n",
    "else:\n",
    "    tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "        (en.numpy() for pt, en in train_examples), target_vocab_size=2**13)\n",
    "    tokenizer_pt = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "        (pt.numpy() for pt, en in train_examples), target_vocab_size=2**13)\n",
    "    tokenizer_en.save_to_file(tokenizer_forma)\n",
    "    tokenizer_pt.save_to_file(tokenizer_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4DYWukNFkGQN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized string is [111, 5, 11, 338, 728, 155, 105, 4, 1195, 3791, 3722, 9, 4, 3807, 3885, 3866, 3809, 3811, 3806, 3791, 3793, 3807, 3800, 237]\n",
      "The original string: Eugenia þa þæt æðele mæden wel þeah on wisdome and on uðwytegunge\n"
     ]
    }
   ],
   "source": [
    "sample_string = 'Eugenia þa þæt æðele mæden wel þeah on wisdome and on uðwytegunge'\n",
    "\n",
    "tokenized_string = tokenizer_en.encode(sample_string)\n",
    "print ('Tokenized string is {}'.format(tokenized_string))\n",
    "\n",
    "original_string = tokenizer_en.decode(tokenized_string)\n",
    "print ('The original string: {}'.format(original_string))\n",
    "\n",
    "assert original_string == sample_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o9KJWJjrsZ4Y"
   },
   "source": [
    "The tokenizer encodes the string by breaking it into subwords if the word is not in its dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bf2ntBxjkqK6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111 ----> Eugenia \n",
      "5 ----> þa \n",
      "11 ----> þæt \n",
      "338 ----> æðele \n",
      "728 ----> mæden \n",
      "155 ----> wel \n",
      "105 ----> þeah \n",
      "4 ----> on \n",
      "1195 ----> wisdom\n",
      "3791 ----> e\n",
      "3722 ---->  \n",
      "9 ----> and \n",
      "4 ----> on \n",
      "3807 ----> u\n",
      "3885 ----> �\n",
      "3866 ----> �\n",
      "3809 ----> w\n",
      "3811 ----> y\n",
      "3806 ----> t\n",
      "3791 ----> e\n",
      "3793 ----> g\n",
      "3807 ----> u\n",
      "3800 ----> n\n",
      "237 ----> ge\n"
     ]
    }
   ],
   "source": [
    "for ts in tokenized_string:\n",
    "  print ('{} ----> {}'.format(ts, tokenizer_en.decode([ts])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bcRp7VcQ5m6g"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kGi4PoVakxdc"
   },
   "source": [
    "Add a start and end token to the input and target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UZwnPr4R055s"
   },
   "outputs": [],
   "source": [
    "def encode(lang1, lang2):\n",
    "  lang1 = [tokenizer_pt.vocab_size] + tokenizer_pt.encode(\n",
    "      lang1.numpy()) + [tokenizer_pt.vocab_size+1]\n",
    "\n",
    "  lang2 = [tokenizer_en.vocab_size] + tokenizer_en.encode(\n",
    "      lang2.numpy()) + [tokenizer_en.vocab_size+1]\n",
    "  \n",
    "  return lang1, lang2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tx1sFbR-9fRs"
   },
   "source": [
    "You want to use `Dataset.map` to apply this function to each element of the dataset.  `Dataset.map` runs in graph mode.\n",
    "\n",
    "* Graph tensors do not have a value. \n",
    "* In graph mode you can only use TensorFlow Ops and functions. \n",
    "\n",
    "So you can't `.map` this function directly: You need to wrap it in a `tf.py_function`. The `tf.py_function` will pass regular tensors (with a value and a `.numpy()` method to access it), to the wrapped python function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mah1cS-P70Iz"
   },
   "outputs": [],
   "source": [
    "def tf_encode(pt, en):\n",
    "  result_pt, result_en = tf.py_function(encode, [pt, en], [tf.int64, tf.int64])\n",
    "  result_pt.set_shape([None])\n",
    "  result_en.set_shape([None])\n",
    "\n",
    "  return result_pt, result_en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6JrGp5Gek6Ql"
   },
   "source": [
    "Note: To keep this example small and relatively fast, drop examples with a length of over 40 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2QEgbjntk6Yf"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c081xPGv1CPI"
   },
   "outputs": [],
   "source": [
    "def filter_max_length(x, y, max_length=MAX_LENGTH):\n",
    "  return tf.logical_and(tf.size(x) <= max_length,\n",
    "                        tf.size(y) <= max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9mk9AZdZ5bcS"
   },
   "outputs": [],
   "source": [
    "train_dataset = train_examples.map(tf_encode)\n",
    "train_dataset = train_dataset.filter(filter_max_length)\n",
    "# cache the dataset to memory to get a speedup while reading from it.\n",
    "train_dataset = train_dataset.cache()\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE)\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "val_dataset = val_examples.map(tf_encode)\n",
    "val_dataset = val_dataset.filter(filter_max_length).padded_batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_fXvfYVfQr2n"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(60, 38), dtype=int64, numpy=\n",
       " array([[6906,   57, 1575, ...,    0,    0,    0],\n",
       "        [6906, 6717,  576, ...,    0,    0,    0],\n",
       "        [6906, 3755, 1229, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [6906, 6724,  576, ...,    0,    0,    0],\n",
       "        [6906, 6731, 2132, ...,    0,    0,    0],\n",
       "        [6906, 5550, 6756, ...,    0,    0,    0]])>,\n",
       " <tf.Tensor: shape=(60, 40), dtype=int64, numpy=\n",
       " array([[3946,   69, 1279, ...,    0,    0,    0],\n",
       "        [3946, 3174, 1551, ...,    0,    0,    0],\n",
       "        [3946, 2405, 1047, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [3946, 3796, 3791, ...,    0,    0,    0],\n",
       "        [3946, 3803, 3807, ...,    0,    0,    0],\n",
       "        [3946, 3789, 1666, ...,    0,    0,    0]])>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_batch, en_batch = next(iter(val_dataset))\n",
    "pt_batch, en_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nBQuibYA4n0n"
   },
   "source": [
    "## Positional encoding\n",
    "\n",
    "Since this model doesn't contain any recurrence or convolution, positional encoding is added to give the model some information about the relative position of the words in the sentence. \n",
    "\n",
    "The positional encoding vector is added to the embedding vector. Embeddings represent a token in a d-dimensional space where tokens with similar meaning will be closer to each other. But the embeddings do not encode the relative position of words in a sentence. So after adding the positional encoding, words will be closer to each other based on the *similarity of their meaning and their position in the sentence*, in the d-dimensional space.\n",
    "\n",
    "See the notebook on [positional encoding](https://github.com/tensorflow/examples/blob/master/community/en/position_encoding.ipynb) to learn more about it. The formula for calculating the positional encoding is as follows:\n",
    "\n",
    "$$\\Large{PE_{(pos, 2i)} = sin(pos / 10000^{2i / d_{model}})} $$\n",
    "$$\\Large{PE_{(pos, 2i+1)} = cos(pos / 10000^{2i / d_{model}})} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WhIOZjMNKujn"
   },
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "  return pos * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Rz82wEs5biZ"
   },
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "  \n",
    "  # apply sin to even indices in the array; 2i\n",
    "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "  \n",
    "  # apply cos to odd indices in the array; 2i+1\n",
    "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "  pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    \n",
    "  return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1kLCla68EloE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 50, 512)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pos_encoding = positional_encoding(50, 512)\n",
    "print (pos_encoding.shape)\n",
    "\n",
    "plt.pcolormesh(pos_encoding[0], cmap='RdBu')\n",
    "plt.xlabel('Depth')\n",
    "plt.xlim((0, 512))\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a_b4ou4TYqUN"
   },
   "source": [
    "## Masking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s42Uydjkv0hF"
   },
   "source": [
    "Mask all the pad tokens in the batch of sequence. It ensures that the model does not treat padding as the input. The mask indicates where pad value `0` is present: it outputs a `1` at those locations, and a `0` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U2i8-e1s8ti9"
   },
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "  \n",
    "  # add extra dimensions to add the padding\n",
    "  # to the attention logits.\n",
    "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A7BYeBCNvi7n"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 1, 1, 5), dtype=float32, numpy=\n",
       "array([[[[0., 0., 1., 1., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., 1., 1.]]],\n",
       "\n",
       "\n",
       "       [[[1., 1., 1., 0., 0.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])\n",
    "create_padding_mask(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z0hzukDBgVom"
   },
   "source": [
    "The look-ahead mask is used to mask the future tokens in a sequence. In other words, the mask indicates which entries should not be used.\n",
    "\n",
    "This means that to predict the third word, only the first and second word will be used. Similarly to predict the fourth word, only the first, second and the third word will be used and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dVxS8OPI9uI0"
   },
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "  return mask  # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yxKGuXxaBeeE"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[0., 1., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.random.uniform((1, 3))\n",
    "temp = create_look_ahead_mask(x.shape[1])\n",
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xluDl5cXYy4y"
   },
   "source": [
    "## Scaled dot product attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vsxEE_-Wa1gF"
   },
   "source": [
    "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/scaled_attention.png\" width=\"500\" alt=\"scaled_dot_product_attention\">\n",
    "\n",
    "The attention function used by the transformer takes three inputs: Q (query), K (key), V (value). The equation used to calculate the attention weights is:\n",
    "\n",
    "$$\\Large{Attention(Q, K, V) = softmax_k(\\frac{QK^T}{\\sqrt{d_k}}) V} $$\n",
    "\n",
    "The dot-product attention is scaled by a factor of square root of the depth. This is done because for large values of depth, the dot product grows large in magnitude pushing the softmax function where it has small gradients resulting in a very hard softmax. \n",
    "\n",
    "For example, consider that `Q` and `K` have a mean of 0 and variance of 1. Their matrix multiplication will have a mean of 0 and variance of `dk`. Hence, *square root of `dk`* is used for scaling (and not any other number) because the matmul of `Q` and `K` should have a mean of 0 and variance of 1, and you get a gentler softmax.\n",
    "\n",
    "The mask is multiplied with -1e9 (close to negative infinity). This is done because the mask is summed with the scaled matrix multiplication of Q and K and is applied immediately before a softmax. The goal is to zero out these cells, and large negative inputs to softmax are near zero in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LazzUq3bJ5SH"
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "  \"\"\"Calculate the attention weights.\n",
    "  q, k, v must have matching leading dimensions.\n",
    "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "  The mask has different shapes depending on its type(padding or look ahead) \n",
    "  but it must be broadcastable for addition.\n",
    "  \n",
    "  Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable \n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "    \n",
    "  Returns:\n",
    "    output, attention_weights\n",
    "  \"\"\"\n",
    "\n",
    "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "  \n",
    "  # scale matmul_qk\n",
    "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "  # add the mask to the scaled tensor.\n",
    "  if mask is not None:\n",
    "    scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "  # add up to 1.\n",
    "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "  return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FiqETnhCkoXh"
   },
   "source": [
    "As the softmax normalization is done on K, its values decide the amount of importance given to Q.\n",
    "\n",
    "The output represents the multiplication of the attention weights and the V (value) vector. This ensures that the words you want to focus on are kept as-is and the irrelevant words are flushed out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n90YjClyInFy"
   },
   "outputs": [],
   "source": [
    "def print_out(q, k, v):\n",
    "  temp_out, temp_attn = scaled_dot_product_attention(\n",
    "      q, k, v, None)\n",
    "  print ('Attention weights are:')\n",
    "  print (temp_attn)\n",
    "  print ('Output is:')\n",
    "  print (temp_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yAzUAf2DPlNt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor([[0. 1. 0. 0.]], shape=(1, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor([[10.  0.]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "temp_k = tf.constant([[10,0,0],\n",
    "                      [0,10,0],\n",
    "                      [0,0,10],\n",
    "                      [0,0,10]], dtype=tf.float32)  # (4, 3)\n",
    "\n",
    "temp_v = tf.constant([[   1,0],\n",
    "                      [  10,0],\n",
    "                      [ 100,5],\n",
    "                      [1000,6]], dtype=tf.float32)  # (4, 2)\n",
    "\n",
    "# This `query` aligns with the second `key`,\n",
    "# so the second `value` is returned.\n",
    "temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zg6k-fGhgXra"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor([[0.  0.  0.5 0.5]], shape=(1, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor([[550.    5.5]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# This query aligns with a repeated key (third and fourth), \n",
    "# so all associated values get averaged.\n",
    "temp_q = tf.constant([[0, 0, 10]], dtype=tf.float32)  # (1, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UAq3YOzUgXhb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor([[0.5 0.5 0.  0. ]], shape=(1, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor([[5.5 0. ]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# This query aligns equally with the first and second key, \n",
    "# so their values get averaged.\n",
    "temp_q = tf.constant([[10, 10, 0]], dtype=tf.float32)  # (1, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aOz-4_XIhaTP"
   },
   "source": [
    "Pass all the queries together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6dlU8Tm-hYrF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor(\n",
      "[[0.  0.  0.5 0.5]\n",
      " [0.  1.  0.  0. ]\n",
      " [0.5 0.5 0.  0. ]], shape=(3, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor(\n",
      "[[550.    5.5]\n",
      " [ 10.    0. ]\n",
      " [  5.5   0. ]], shape=(3, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "temp_q = tf.constant([[0, 0, 10], [0, 10, 0], [10, 10, 0]], dtype=tf.float32)  # (3, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kmzGPEy64qmA"
   },
   "source": [
    "## Multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fz5BMC8Kaoqo"
   },
   "source": [
    "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/multi_head_attention.png\" width=\"500\" alt=\"multi-head attention\">\n",
    "\n",
    "\n",
    "Multi-head attention consists of four parts:\n",
    "*    Linear layers and split into heads.\n",
    "*    Scaled dot-product attention.\n",
    "*    Concatenation of heads.\n",
    "*    Final linear layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JPmbr6F1C-v_"
   },
   "source": [
    "Each multi-head attention block gets three inputs; Q (query), K (key), V (value). These are put through linear (Dense) layers and split up into multiple heads. \n",
    "\n",
    "The `scaled_dot_product_attention` defined above is applied to each head (broadcasted for efficiency). An appropriate mask must be used in the attention step.  The attention output for each head is then concatenated (using `tf.transpose`, and `tf.reshape`) and put through a final `Dense` layer.\n",
    "\n",
    "Instead of one single attention head, Q, K, and V are split into multiple heads because it allows the model to jointly attend to information at different positions from different representational spaces. After the split each head has a reduced dimensionality, so the total computation cost is the same as a single head attention with full dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BSV3PPKsYecw"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "    \n",
    "    assert d_model % self.num_heads == 0\n",
    "    \n",
    "    self.depth = d_model // self.num_heads\n",
    "    \n",
    "    self.wq = tf.keras.layers.Dense(d_model)\n",
    "    self.wk = tf.keras.layers.Dense(d_model)\n",
    "    self.wv = tf.keras.layers.Dense(d_model)\n",
    "    \n",
    "    self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "  def split_heads(self, x, batch_size):\n",
    "    \"\"\"Split the last dimension into (num_heads, depth).\n",
    "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "    \"\"\"\n",
    "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "  def call(self, v, k, q, mask):\n",
    "    batch_size = tf.shape(q)[0]\n",
    "    \n",
    "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "    \n",
    "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "    \n",
    "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "        q, k, v, mask)\n",
    "    \n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "    concat_attention = tf.reshape(scaled_attention, \n",
    "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "        \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0D8FJue5lDyZ"
   },
   "source": [
    "Create a `MultiHeadAttention` layer to try out. At each location in the sequence, `y`, the `MultiHeadAttention` runs all 8 attention heads across all other locations in the sequence, returning a new vector of the same length at each location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hu94p-_-2_BX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([1, 60, 512]), TensorShape([1, 8, 60, 60]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
    "y = tf.random.uniform((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\n",
    "out, attn = temp_mha(y, k=y, q=y, mask=None)\n",
    "out.shape, attn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RdDqGayx67vv"
   },
   "source": [
    "## Point wise feed forward network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gBqzJXGfHK3X"
   },
   "source": [
    "Point wise feed forward network consists of two fully-connected layers with a ReLU activation in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ET7xLt0yCT6Z"
   },
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "  return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mytb1lPyOHLB"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 50, 512])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_ffn = point_wise_feed_forward_network(512, 2048)\n",
    "sample_ffn(tf.random.uniform((64, 50, 512))).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7e7hKcxn6-zd"
   },
   "source": [
    "## Encoder and decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yScbC0MUH8dS"
   },
   "source": [
    "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/transformer.png\" width=\"600\" alt=\"transformer\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MfYJG-Kvgwy2"
   },
   "source": [
    "The transformer model follows the same general pattern as a standard [sequence to sequence with attention model](nmt_with_attention.ipynb). \n",
    "\n",
    "* The input sentence is passed through `N` encoder layers that generates an output for each word/token in the sequence.\n",
    "* The decoder attends on the encoder's output and its own input (self-attention) to predict the next word. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QFv-FNYUmvpn"
   },
   "source": [
    "### Encoder layer\n",
    "\n",
    "Each encoder layer consists of sublayers:\n",
    "\n",
    "1.   Multi-head attention (with padding mask) \n",
    "2.    Point wise feed forward networks. \n",
    "\n",
    "Each of these sublayers has a residual connection around it followed by a layer normalization. Residual connections help in avoiding the vanishing gradient problem in deep networks.\n",
    "\n",
    "The output of each sublayer is `LayerNorm(x + Sublayer(x))`. The normalization is done on the `d_model` (last) axis. There are N encoder layers in the transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ncyS-Ms3i2x_"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(EncoderLayer, self).__init__()\n",
    "\n",
    "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "  def call(self, x, training, mask):\n",
    "\n",
    "    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "    attn_output = self.dropout1(attn_output, training=training)\n",
    "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "    \n",
    "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "    ffn_output = self.dropout2(ffn_output, training=training)\n",
    "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "    \n",
    "    return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AzZRXdO0mI48"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 43, 512])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_encoder_layer = EncoderLayer(512, 8, 2048)\n",
    "\n",
    "sample_encoder_layer_output = sample_encoder_layer(\n",
    "    tf.random.uniform((64, 43, 512)), False, None)\n",
    "\n",
    "sample_encoder_layer_output.shape  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6LO_48Owmx_o"
   },
   "source": [
    "### Decoder layer\n",
    "\n",
    "Each decoder layer consists of sublayers:\n",
    "\n",
    "1.   Masked multi-head attention (with look ahead mask and padding mask)\n",
    "2.   Multi-head attention (with padding mask). V (value) and K (key) receive the *encoder output* as inputs. Q (query) receives the *output from the masked multi-head attention sublayer.*\n",
    "3.   Point wise feed forward networks\n",
    "\n",
    "Each of these sublayers has a residual connection around it followed by a layer normalization. The output of each sublayer is `LayerNorm(x + Sublayer(x))`. The normalization is done on the `d_model` (last) axis.\n",
    "\n",
    "There are N decoder layers in the transformer.\n",
    "\n",
    "As Q receives the output from decoder's first attention block, and K receives the encoder output, the attention weights represent the importance given to the decoder's input based on the encoder's output. In other words, the decoder predicts the next word by looking at the encoder output and self-attending to its own output. See the demonstration above in the scaled dot product attention section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9SoX0-vd1hue"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(DecoderLayer, self).__init__()\n",
    "\n",
    "    self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "    self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    " \n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    \n",
    "  def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "    # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "    attn1 = self.dropout1(attn1, training=training)\n",
    "    out1 = self.layernorm1(attn1 + x)\n",
    "    \n",
    "    attn2, attn_weights_block2 = self.mha2(\n",
    "        enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "    attn2 = self.dropout2(attn2, training=training)\n",
    "    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "    \n",
    "    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "    ffn_output = self.dropout3(ffn_output, training=training)\n",
    "    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "    \n",
    "    return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ne2Bqx8k71l0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 50, 512])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder_layer = DecoderLayer(512, 8, 2048)\n",
    "\n",
    "sample_decoder_layer_output, _, _ = sample_decoder_layer(\n",
    "    tf.random.uniform((64, 50, 512)), sample_encoder_layer_output, \n",
    "    False, None, None)\n",
    "\n",
    "sample_decoder_layer_output.shape  # (batch_size, target_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SE1H51Ajm0q1"
   },
   "source": [
    "### Encoder\n",
    "\n",
    "The `Encoder` consists of:\n",
    "1.   Input Embedding\n",
    "2.   Positional Encoding\n",
    "3.   N encoder layers\n",
    "\n",
    "The input is put through an embedding which is summed with the positional encoding. The output of this summation is the input to the encoder layers. The output of the encoder is the input to the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jpEox7gJ8FCI"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "    super(Encoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "    \n",
    "    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "    self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
    "                                            self.d_model)\n",
    "    \n",
    "    \n",
    "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                       for _ in range(num_layers)]\n",
    "  \n",
    "    self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "  def call(self, x, training, mask):\n",
    "\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    \n",
    "    # adding embedding and position encoding.\n",
    "    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "    x = self.dropout(x, training=training)\n",
    "    \n",
    "    for i in range(self.num_layers):\n",
    "      x = self.enc_layers[i](x, training, mask)\n",
    "    \n",
    "    return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8QG9nueFQKXx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 62, 512)\n"
     ]
    }
   ],
   "source": [
    "sample_encoder = Encoder(num_layers=2, d_model=512, num_heads=8, \n",
    "                         dff=2048, input_vocab_size=8500,\n",
    "                         maximum_position_encoding=10000)\n",
    "temp_input = tf.random.uniform((64, 62), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "sample_encoder_output = sample_encoder(temp_input, training=False, mask=None)\n",
    "\n",
    "print (sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p-uO6ls8m2O5"
   },
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZtT7PKzrXkNr"
   },
   "source": [
    " The `Decoder` consists of:\n",
    "1.   Output Embedding\n",
    "2.   Positional Encoding\n",
    "3.   N decoder layers\n",
    "\n",
    "The target is put through an embedding which is summed with the positional encoding. The output of this summation is the input to the decoder layers. The output of the decoder is the input to the final linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d5_d5-PLQXwY"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "    super(Decoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "    \n",
    "    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "    \n",
    "    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
    "                       for _ in range(num_layers)]\n",
    "    self.dropout = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "  def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    attention_weights = {}\n",
    "    \n",
    "    x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x += self.pos_encoding[:, :seq_len, :]\n",
    "    \n",
    "    x = self.dropout(x, training=training)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                             look_ahead_mask, padding_mask)\n",
    "      \n",
    "      attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "      attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "    \n",
    "    # x.shape == (batch_size, target_seq_len, d_model)\n",
    "    return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a1jXoAMRZyvu"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 26, 512]), TensorShape([64, 8, 26, 62]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder = Decoder(num_layers=2, d_model=512, num_heads=8, \n",
    "                         dff=2048, target_vocab_size=8000,\n",
    "                         maximum_position_encoding=5000)\n",
    "temp_input = tf.random.uniform((64, 26), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "output, attn = sample_decoder(temp_input, \n",
    "                              enc_output=sample_encoder_output, \n",
    "                              training=False,\n",
    "                              look_ahead_mask=None, \n",
    "                              padding_mask=None)\n",
    "\n",
    "output.shape, attn['decoder_layer2_block2'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y54xnJnuYgJ7"
   },
   "source": [
    "## Create the Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uERO1y54cOKq"
   },
   "source": [
    "Transformer consists of the encoder, decoder and a final linear layer. The output of the decoder is the input to the linear layer and its output is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PED3bIpOYkBu"
   },
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
    "               target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "    super(Transformer, self).__init__()\n",
    "\n",
    "    self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
    "                           input_vocab_size, pe_input, rate)\n",
    "\n",
    "    self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
    "                           target_vocab_size, pe_target, rate)\n",
    "\n",
    "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "    \n",
    "  def call(self, inp, tar, training, enc_padding_mask, \n",
    "           look_ahead_mask, dec_padding_mask):\n",
    "\n",
    "    enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "    \n",
    "    # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "    dec_output, attention_weights = self.decoder(\n",
    "        tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "    \n",
    "    final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "    \n",
    "    return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tJ4fbQcIkHW1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 36, 8000])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_transformer = Transformer(\n",
    "    num_layers=2, d_model=512, num_heads=8, dff=2048, \n",
    "    input_vocab_size=8500, target_vocab_size=8000, \n",
    "    pe_input=10000, pe_target=6000)\n",
    "\n",
    "temp_input = tf.random.uniform((64, 38), dtype=tf.int64, minval=0, maxval=200)\n",
    "temp_target = tf.random.uniform((64, 36), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "fn_out, _ = sample_transformer(temp_input, temp_target, training=False, \n",
    "                               enc_padding_mask=None, \n",
    "                               look_ahead_mask=None,\n",
    "                               dec_padding_mask=None)\n",
    "\n",
    "fn_out.shape  # (batch_size, tar_seq_len, target_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wsINyf1VEQLC"
   },
   "source": [
    "## Set hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zVjWCxFNcgbt"
   },
   "source": [
    "To keep this example small and relatively fast, the values for *num_layers, d_model, and dff* have been reduced. \n",
    "\n",
    "The values used in the base model of transformer were; *num_layers=6*, *d_model = 512*, *dff = 2048*. See the [paper](https://arxiv.org/abs/1706.03762) for all the other versions of the transformer.\n",
    "\n",
    "Note: By changing the values below, you can get the model that achieved state of the art on many tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lnJn5SLA2ahP"
   },
   "outputs": [],
   "source": [
    "num_layers = 3\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "\n",
    "input_vocab_size = tokenizer_pt.vocab_size + 2\n",
    "target_vocab_size = tokenizer_en.vocab_size + 2\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xYEGhEOtzn5W"
   },
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GOmWW--yP3zx"
   },
   "source": [
    "Use the Adam optimizer with a custom learning rate scheduler according to the formula in the [paper](https://arxiv.org/abs/1706.03762).\n",
    "\n",
    "$$\\Large{lrate = d_{model}^{-0.5} * min(step{\\_}num^{-0.5}, step{\\_}num * warmup{\\_}steps^{-1.5})}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iYQdOO1axwEI"
   },
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "    \n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "    \n",
    "  def __call__(self, step):\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "    \n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7r4scdulztRx"
   },
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
    "                                     epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f33ZCgvHpPdG"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Train Step')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp_learning_rate_schedule = CustomSchedule(d_model)\n",
    "\n",
    "plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YgkDE7hzo8r5"
   },
   "source": [
    "## Loss and metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oxGJtoDuYIHL"
   },
   "source": [
    "Since the target sequences are padded, it is important to apply a padding mask when calculating the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MlhsJMm0TW_B"
   },
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "67oqVHiT0Eiu"
   },
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "  \n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "phlyxMnm-Tpx"
   },
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "    name='train_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aeHumfr7zmMa"
   },
   "source": [
    "## Training and checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UiysUa--4tOU"
   },
   "outputs": [],
   "source": [
    "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
    "                          input_vocab_size, target_vocab_size, \n",
    "                          pe_input=input_vocab_size, \n",
    "                          pe_target=target_vocab_size,\n",
    "                          rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZOJUSB1T8GjM"
   },
   "outputs": [],
   "source": [
    "def create_masks(inp, tar):\n",
    "  # Encoder padding mask\n",
    "  enc_padding_mask = create_padding_mask(inp)\n",
    "  \n",
    "  # Used in the 2nd attention block in the decoder.\n",
    "  # This padding mask is used to mask the encoder outputs.\n",
    "  dec_padding_mask = create_padding_mask(inp)\n",
    "  \n",
    "  # Used in the 1st attention block in the decoder.\n",
    "  # It is used to pad and mask future tokens in the input received by \n",
    "  # the decoder.\n",
    "  look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "  dec_target_padding_mask = create_padding_mask(tar)\n",
    "  combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "  \n",
    "  return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fzuf06YZp66w"
   },
   "source": [
    "Create the checkpoint path and the checkpoint manager. This will be used to save checkpoints every `n` epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hNhuYfllndLZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint restored!! Accuracy 0.0000\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=50)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "  accuracy_saved = float(train_accuracy.result())\n",
    "  print ('Latest checkpoint restored!! Accuracy {:.4f}'.format(accuracy_saved))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf checkpoints/train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Di_Yaa1gf9r"
   },
   "source": [
    "The target is divided into tar_inp and tar_real. tar_inp is passed as an input to the decoder. `tar_real` is that same input shifted by 1: At each location in `tar_input`, `tar_real` contains the  next token that should be predicted.\n",
    "\n",
    "For example, `sentence` = \"SOS A lion in the jungle is sleeping EOS\"\n",
    "\n",
    "`tar_inp` =  \"SOS A lion in the jungle is sleeping\"\n",
    "\n",
    "`tar_real` = \"A lion in the jungle is sleeping EOS\"\n",
    "\n",
    "The transformer is an auto-regressive model: it makes predictions one part at a time, and uses its output so far to decide what to do next. \n",
    "\n",
    "During training this example uses teacher-forcing (like in the [text generation tutorial](./text_generation.ipynb)). Teacher forcing is passing the true output to the next time step regardless of what the model predicts at the current time step.\n",
    "\n",
    "As the transformer predicts each word, *self-attention* allows it to look at the previous words in the input sequence to better predict the next word.\n",
    "\n",
    "To prevent the model from peaking at the expected output the model uses a look-ahead mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iJwmp9OE29oj"
   },
   "outputs": [],
   "source": [
    "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
    "# execution. The function specializes to the precise shape of the argument\n",
    "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
    "# batch sizes (the last batch is smaller), use input_signature to specify\n",
    "# more generic shapes.\n",
    "\n",
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "]\n",
    "\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "def train_step(inp, tar):\n",
    "  tar_inp = tar[:, :-1]\n",
    "  tar_real = tar[:, 1:]\n",
    "  \n",
    "  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "  \n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions, _ = transformer(inp, tar_inp, \n",
    "                                 True, \n",
    "                                 enc_padding_mask, \n",
    "                                 combined_mask, \n",
    "                                 dec_padding_mask)\n",
    "    loss = loss_function(tar_real, predictions)\n",
    "\n",
    "  gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "  \n",
    "  train_loss(loss)\n",
    "  train_accuracy(tar_real, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qM2PDWGDJ_8V"
   },
   "source": [
    "Portuguese is used as the input language and English is the target language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LKpoA6q1sJFj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bbvmaKNiznHZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 0.0026 Accuracy 0.3650\n",
      "Epoch 1 Batch 15 Loss 0.0227 Accuracy 0.3513\n",
      "Epoch 1 Batch 30 Loss 0.0134 Accuracy 0.3473\n",
      "Saving checkpoint for epoch 1 at ./checkpoints/train/ckpt-21\n",
      "Epoch 1 Loss 0.0143 Accuracy 0.3516\n",
      "Time taken for 1 epoch: 19.745041131973267 secs\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.0020 Accuracy 0.3388\n",
      "Epoch 2 Batch 15 Loss 0.0214 Accuracy 0.3495\n",
      "Epoch 2 Batch 30 Loss 0.0120 Accuracy 0.3572\n",
      "Saving checkpoint for epoch 2 at ./checkpoints/train/ckpt-22\n",
      "Epoch 2 Loss 0.0099 Accuracy 0.3557\n",
      "Time taken for 1 epoch: 8.2515869140625 secs\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.0015 Accuracy 0.3540\n",
      "Epoch 3 Batch 15 Loss 0.0029 Accuracy 0.3553\n",
      "Epoch 3 Batch 30 Loss 0.0084 Accuracy 0.3545\n",
      "Epoch 3 Loss 0.0071 Accuracy 0.3538\n",
      "Time taken for 1 epoch: 7.765164613723755 secs\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.0010 Accuracy 0.3755\n",
      "Epoch 4 Batch 15 Loss 0.0060 Accuracy 0.3555\n",
      "Epoch 4 Batch 30 Loss 0.0075 Accuracy 0.3527\n",
      "Epoch 4 Loss 0.0068 Accuracy 0.3518\n",
      "Time taken for 1 epoch: 7.654939651489258 secs\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.0032 Accuracy 0.3608\n",
      "Epoch 5 Batch 15 Loss 0.0067 Accuracy 0.3482\n",
      "Epoch 5 Batch 30 Loss 0.0049 Accuracy 0.3502\n",
      "Epoch 5 Loss 0.0066 Accuracy 0.3498\n",
      "Time taken for 1 epoch: 7.920173645019531 secs\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.0007 Accuracy 0.3680\n",
      "Epoch 6 Batch 15 Loss 0.0071 Accuracy 0.3528\n",
      "Epoch 6 Batch 30 Loss 0.0057 Accuracy 0.3460\n",
      "Epoch 6 Loss 0.0060 Accuracy 0.3487\n",
      "Time taken for 1 epoch: 7.834611415863037 secs\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.0009 Accuracy 0.3513\n",
      "Epoch 7 Batch 15 Loss 0.0056 Accuracy 0.3525\n",
      "Epoch 7 Batch 30 Loss 0.0044 Accuracy 0.3518\n",
      "Epoch 7 Loss 0.0049 Accuracy 0.3480\n",
      "Time taken for 1 epoch: 7.909019708633423 secs\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.0063 Accuracy 0.4163\n",
      "Epoch 8 Batch 15 Loss 0.0049 Accuracy 0.3613\n",
      "Epoch 8 Batch 30 Loss 0.0038 Accuracy 0.3545\n",
      "Epoch 8 Loss 0.0044 Accuracy 0.3536\n",
      "Time taken for 1 epoch: 7.677950859069824 secs\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.0003 Accuracy 0.3936\n",
      "Epoch 9 Batch 15 Loss 0.0034 Accuracy 0.3617\n",
      "Epoch 9 Batch 30 Loss 0.0028 Accuracy 0.3595\n",
      "Saving checkpoint for epoch 9 at ./checkpoints/train/ckpt-23\n",
      "Epoch 9 Loss 0.0031 Accuracy 0.3603\n",
      "Time taken for 1 epoch: 7.913736820220947 secs\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.0038 Accuracy 0.3604\n",
      "Epoch 10 Batch 15 Loss 0.0033 Accuracy 0.3428\n",
      "Epoch 10 Batch 30 Loss 0.0030 Accuracy 0.3514\n",
      "Epoch 10 Loss 0.0033 Accuracy 0.3484\n",
      "Time taken for 1 epoch: 7.873113393783569 secs\n",
      "\n",
      "Epoch 11 Batch 0 Loss 0.0066 Accuracy 0.3097\n",
      "Epoch 11 Batch 15 Loss 0.0031 Accuracy 0.3728\n",
      "Epoch 11 Batch 30 Loss 0.0043 Accuracy 0.3532\n",
      "Epoch 11 Loss 0.0038 Accuracy 0.3478\n",
      "Time taken for 1 epoch: 8.040895223617554 secs\n",
      "\n",
      "Epoch 12 Batch 0 Loss 0.0010 Accuracy 0.4339\n",
      "Epoch 12 Batch 15 Loss 0.0026 Accuracy 0.3619\n",
      "Epoch 12 Batch 30 Loss 0.0032 Accuracy 0.3572\n",
      "Epoch 12 Loss 0.0034 Accuracy 0.3583\n",
      "Time taken for 1 epoch: 7.666791200637817 secs\n",
      "\n",
      "Epoch 13 Batch 0 Loss 0.0006 Accuracy 0.3448\n",
      "Epoch 13 Batch 15 Loss 0.0030 Accuracy 0.3568\n",
      "Epoch 13 Batch 30 Loss 0.0029 Accuracy 0.3575\n",
      "Epoch 13 Loss 0.0028 Accuracy 0.3546\n",
      "Time taken for 1 epoch: 7.711990118026733 secs\n",
      "\n",
      "Epoch 14 Batch 0 Loss 0.0002 Accuracy 0.2885\n",
      "Epoch 14 Batch 15 Loss 0.0026 Accuracy 0.3538\n",
      "Epoch 14 Batch 30 Loss 0.0024 Accuracy 0.3494\n",
      "Epoch 14 Loss 0.0027 Accuracy 0.3534\n",
      "Time taken for 1 epoch: 7.742199897766113 secs\n",
      "\n",
      "Epoch 15 Batch 0 Loss 0.0009 Accuracy 0.3740\n",
      "Epoch 15 Batch 15 Loss 0.0049 Accuracy 0.3741\n",
      "Epoch 15 Batch 30 Loss 0.0041 Accuracy 0.3626\n",
      "Saving checkpoint for epoch 15 at ./checkpoints/train/ckpt-24\n",
      "Epoch 15 Loss 0.0037 Accuracy 0.3604\n",
      "Time taken for 1 epoch: 8.084525346755981 secs\n",
      "\n",
      "Epoch 16 Batch 0 Loss 0.0004 Accuracy 0.3735\n",
      "Epoch 16 Batch 15 Loss 0.0036 Accuracy 0.3483\n",
      "Epoch 16 Batch 30 Loss 0.0038 Accuracy 0.3563\n",
      "Epoch 16 Loss 0.0033 Accuracy 0.3557\n",
      "Time taken for 1 epoch: 7.752238750457764 secs\n",
      "\n",
      "Epoch 17 Batch 0 Loss 0.0028 Accuracy 0.3883\n",
      "Epoch 17 Batch 15 Loss 0.0033 Accuracy 0.3390\n",
      "Epoch 17 Batch 30 Loss 0.0028 Accuracy 0.3514\n",
      "Epoch 17 Loss 0.0033 Accuracy 0.3468\n",
      "Time taken for 1 epoch: 7.861598968505859 secs\n",
      "\n",
      "Epoch 18 Batch 0 Loss 0.0012 Accuracy 0.3160\n",
      "Epoch 18 Batch 15 Loss 0.0044 Accuracy 0.3624\n",
      "Epoch 18 Batch 30 Loss 0.0041 Accuracy 0.3582\n",
      "Epoch 18 Loss 0.0044 Accuracy 0.3592\n",
      "Time taken for 1 epoch: 7.552466154098511 secs\n",
      "\n",
      "Epoch 19 Batch 0 Loss 0.0041 Accuracy 0.3997\n",
      "Epoch 19 Batch 15 Loss 0.0043 Accuracy 0.3553\n",
      "Epoch 19 Batch 30 Loss 0.0037 Accuracy 0.3523\n",
      "Epoch 19 Loss 0.0035 Accuracy 0.3491\n",
      "Time taken for 1 epoch: 7.9159440994262695 secs\n",
      "\n",
      "Epoch 20 Batch 0 Loss 0.0052 Accuracy 0.3378\n",
      "Epoch 20 Batch 15 Loss 0.0021 Accuracy 0.3550\n",
      "Epoch 20 Batch 30 Loss 0.0020 Accuracy 0.3576\n",
      "Epoch 20 Loss 0.0021 Accuracy 0.3572\n",
      "Time taken for 1 epoch: 7.679747819900513 secs\n",
      "\n",
      "Epoch 21 Batch 0 Loss 0.0034 Accuracy 0.3193\n",
      "Epoch 21 Batch 15 Loss 0.0027 Accuracy 0.3372\n",
      "Epoch 21 Batch 30 Loss 0.0021 Accuracy 0.3529\n",
      "Epoch 21 Loss 0.0020 Accuracy 0.3556\n",
      "Time taken for 1 epoch: 7.782665252685547 secs\n",
      "\n",
      "Epoch 22 Batch 0 Loss 0.0044 Accuracy 0.3192\n",
      "Epoch 22 Batch 15 Loss 0.0027 Accuracy 0.3615\n",
      "Epoch 22 Batch 30 Loss 0.0028 Accuracy 0.3530\n",
      "Epoch 22 Loss 0.0028 Accuracy 0.3567\n",
      "Time taken for 1 epoch: 7.7018983364105225 secs\n",
      "\n",
      "Epoch 23 Batch 0 Loss 0.0004 Accuracy 0.4106\n",
      "Epoch 23 Batch 15 Loss 0.0032 Accuracy 0.3507\n",
      "Epoch 23 Batch 30 Loss 0.0033 Accuracy 0.3522\n",
      "Epoch 23 Loss 0.0029 Accuracy 0.3500\n",
      "Time taken for 1 epoch: 7.865468502044678 secs\n",
      "\n",
      "Epoch 24 Batch 0 Loss 0.0003 Accuracy 0.3623\n",
      "Epoch 24 Batch 15 Loss 0.0022 Accuracy 0.3609\n",
      "Epoch 24 Batch 30 Loss 0.0023 Accuracy 0.3621\n",
      "Epoch 24 Loss 0.0022 Accuracy 0.3595\n",
      "Time taken for 1 epoch: 7.550058603286743 secs\n",
      "\n",
      "Epoch 25 Batch 0 Loss 0.0002 Accuracy 0.3589\n",
      "Epoch 25 Batch 15 Loss 0.0016 Accuracy 0.3458\n",
      "Epoch 25 Batch 30 Loss 0.0025 Accuracy 0.3506\n",
      "Epoch 25 Loss 0.0029 Accuracy 0.3499\n",
      "Time taken for 1 epoch: 7.827766418457031 secs\n",
      "\n",
      "Epoch 26 Batch 0 Loss 0.0094 Accuracy 0.2973\n",
      "Epoch 26 Batch 15 Loss 0.0029 Accuracy 0.3466\n",
      "Epoch 26 Batch 30 Loss 0.0028 Accuracy 0.3436\n",
      "Epoch 26 Loss 0.0027 Accuracy 0.3528\n",
      "Time taken for 1 epoch: 7.796313524246216 secs\n",
      "\n",
      "Epoch 27 Batch 0 Loss 0.0016 Accuracy 0.3906\n",
      "Epoch 27 Batch 15 Loss 0.0030 Accuracy 0.3537\n",
      "Epoch 27 Batch 30 Loss 0.0031 Accuracy 0.3543\n",
      "Epoch 27 Loss 0.0027 Accuracy 0.3558\n",
      "Time taken for 1 epoch: 7.8053200244903564 secs\n",
      "\n",
      "Epoch 28 Batch 0 Loss 0.0009 Accuracy 0.3478\n",
      "Epoch 28 Batch 15 Loss 0.0030 Accuracy 0.3509\n",
      "Epoch 28 Batch 30 Loss 0.0025 Accuracy 0.3502\n",
      "Epoch 28 Loss 0.0022 Accuracy 0.3527\n",
      "Time taken for 1 epoch: 7.785586833953857 secs\n",
      "\n",
      "Epoch 29 Batch 0 Loss 0.0007 Accuracy 0.3199\n",
      "Epoch 29 Batch 15 Loss 0.0024 Accuracy 0.3616\n",
      "Epoch 29 Batch 30 Loss 0.0023 Accuracy 0.3543\n",
      "Epoch 29 Loss 0.0024 Accuracy 0.3513\n",
      "Time taken for 1 epoch: 7.757374048233032 secs\n",
      "\n",
      "Epoch 30 Batch 0 Loss 0.0042 Accuracy 0.3481\n",
      "Epoch 30 Batch 15 Loss 0.0031 Accuracy 0.3646\n",
      "Epoch 30 Batch 30 Loss 0.0033 Accuracy 0.3616\n",
      "Epoch 30 Loss 0.0033 Accuracy 0.3579\n",
      "Time taken for 1 epoch: 7.638094902038574 secs\n",
      "\n",
      "Epoch 31 Batch 0 Loss 0.0072 Accuracy 0.3894\n",
      "Epoch 31 Batch 15 Loss 0.0033 Accuracy 0.3594\n",
      "Epoch 31 Batch 30 Loss 0.0027 Accuracy 0.3526\n",
      "Epoch 31 Loss 0.0046 Accuracy 0.3547\n",
      "Time taken for 1 epoch: 7.733249187469482 secs\n",
      "\n",
      "Epoch 32 Batch 0 Loss 0.0004 Accuracy 0.3730\n",
      "Epoch 32 Batch 15 Loss 0.0071 Accuracy 0.3602\n",
      "Epoch 32 Batch 30 Loss 0.0048 Accuracy 0.3550\n",
      "Epoch 32 Loss 0.0048 Accuracy 0.3556\n",
      "Time taken for 1 epoch: 7.729085683822632 secs\n",
      "\n",
      "Epoch 33 Batch 0 Loss 0.0004 Accuracy 0.3085\n",
      "Epoch 33 Batch 15 Loss 0.0018 Accuracy 0.3449\n",
      "Epoch 33 Batch 30 Loss 0.0027 Accuracy 0.3511\n",
      "Epoch 33 Loss 0.0025 Accuracy 0.3528\n",
      "Time taken for 1 epoch: 7.72929835319519 secs\n",
      "\n",
      "Epoch 34 Batch 0 Loss 0.0005 Accuracy 0.3943\n",
      "Epoch 34 Batch 15 Loss 0.0034 Accuracy 0.3373\n",
      "Epoch 34 Batch 30 Loss 0.0026 Accuracy 0.3466\n",
      "Epoch 34 Loss 0.0023 Accuracy 0.3482\n",
      "Time taken for 1 epoch: 7.913389444351196 secs\n",
      "\n",
      "Epoch 35 Batch 0 Loss 0.0005 Accuracy 0.3402\n",
      "Epoch 35 Batch 15 Loss 0.0013 Accuracy 0.3484\n",
      "Epoch 35 Batch 30 Loss 0.0012 Accuracy 0.3496\n",
      "Epoch 35 Loss 0.0013 Accuracy 0.3524\n",
      "Time taken for 1 epoch: 7.675655126571655 secs\n",
      "\n",
      "Epoch 36 Batch 0 Loss 0.0019 Accuracy 0.3653\n",
      "Epoch 36 Batch 15 Loss 0.0017 Accuracy 0.3563\n",
      "Epoch 36 Batch 30 Loss 0.0015 Accuracy 0.3551\n",
      "Epoch 36 Loss 0.0016 Accuracy 0.3554\n",
      "Time taken for 1 epoch: 7.6321940422058105 secs\n",
      "\n",
      "Epoch 37 Batch 0 Loss 0.0010 Accuracy 0.3243\n",
      "Epoch 37 Batch 15 Loss 0.0025 Accuracy 0.3484\n",
      "Epoch 37 Batch 30 Loss 0.0024 Accuracy 0.3495\n",
      "Epoch 37 Loss 0.0025 Accuracy 0.3537\n",
      "Time taken for 1 epoch: 7.767884254455566 secs\n",
      "\n",
      "Epoch 38 Batch 0 Loss 0.0006 Accuracy 0.3807\n",
      "Epoch 38 Batch 15 Loss 0.0031 Accuracy 0.3534\n",
      "Epoch 38 Batch 30 Loss 0.0033 Accuracy 0.3553\n",
      "Epoch 38 Loss 0.0037 Accuracy 0.3548\n",
      "Time taken for 1 epoch: 7.8211729526519775 secs\n",
      "\n",
      "Epoch 39 Batch 0 Loss 0.0033 Accuracy 0.3377\n",
      "Epoch 39 Batch 15 Loss 0.0023 Accuracy 0.3453\n",
      "Epoch 39 Batch 30 Loss 0.0023 Accuracy 0.3508\n",
      "Epoch 39 Loss 0.0038 Accuracy 0.3529\n",
      "Time taken for 1 epoch: 7.6665050983428955 secs\n",
      "\n",
      "Epoch 40 Batch 0 Loss 0.0002 Accuracy 0.3286\n",
      "Epoch 40 Batch 15 Loss 0.0069 Accuracy 0.3583\n",
      "Epoch 40 Batch 30 Loss 0.0056 Accuracy 0.3455\n",
      "Epoch 40 Loss 0.0052 Accuracy 0.3472\n",
      "Time taken for 1 epoch: 7.95142388343811 secs\n",
      "\n",
      "Epoch 41 Batch 0 Loss 0.0047 Accuracy 0.3237\n",
      "Epoch 41 Batch 15 Loss 0.0028 Accuracy 0.3450\n",
      "Epoch 41 Batch 30 Loss 0.0027 Accuracy 0.3575\n",
      "Epoch 41 Loss 0.0025 Accuracy 0.3600\n",
      "Time taken for 1 epoch: 7.618464708328247 secs\n",
      "\n",
      "Epoch 42 Batch 0 Loss 0.0002 Accuracy 0.3826\n",
      "Epoch 42 Batch 15 Loss 0.0010 Accuracy 0.3573\n",
      "Epoch 42 Batch 30 Loss 0.0020 Accuracy 0.3608\n",
      "Epoch 42 Loss 0.0022 Accuracy 0.3577\n",
      "Time taken for 1 epoch: 7.692048072814941 secs\n",
      "\n",
      "Epoch 43 Batch 0 Loss 0.0004 Accuracy 0.3970\n",
      "Epoch 43 Batch 15 Loss 0.0019 Accuracy 0.3468\n",
      "Epoch 43 Batch 30 Loss 0.0023 Accuracy 0.3518\n",
      "Epoch 43 Loss 0.0022 Accuracy 0.3541\n",
      "Time taken for 1 epoch: 7.645621299743652 secs\n",
      "\n",
      "Epoch 44 Batch 0 Loss 0.0008 Accuracy 0.3835\n",
      "Epoch 44 Batch 15 Loss 0.0013 Accuracy 0.3374\n",
      "Epoch 44 Batch 30 Loss 0.0015 Accuracy 0.3447\n",
      "Epoch 44 Loss 0.0017 Accuracy 0.3493\n",
      "Time taken for 1 epoch: 7.885851144790649 secs\n",
      "\n",
      "Epoch 45 Batch 0 Loss 0.0002 Accuracy 0.3808\n",
      "Epoch 45 Batch 15 Loss 0.0016 Accuracy 0.3617\n",
      "Epoch 45 Batch 30 Loss 0.0020 Accuracy 0.3599\n",
      "Epoch 45 Loss 0.0020 Accuracy 0.3529\n",
      "Time taken for 1 epoch: 7.774858236312866 secs\n",
      "\n",
      "Epoch 46 Batch 0 Loss 0.0006 Accuracy 0.4359\n",
      "Epoch 46 Batch 15 Loss 0.0024 Accuracy 0.3565\n",
      "Epoch 46 Batch 30 Loss 0.0022 Accuracy 0.3524\n",
      "Epoch 46 Loss 0.0022 Accuracy 0.3539\n",
      "Time taken for 1 epoch: 7.904819965362549 secs\n",
      "\n",
      "Epoch 47 Batch 0 Loss 0.0002 Accuracy 0.3332\n",
      "Epoch 47 Batch 15 Loss 0.0031 Accuracy 0.3526\n",
      "Epoch 47 Batch 30 Loss 0.0034 Accuracy 0.3511\n",
      "Epoch 47 Loss 0.0030 Accuracy 0.3509\n",
      "Time taken for 1 epoch: 7.782625675201416 secs\n",
      "\n",
      "Epoch 48 Batch 0 Loss 0.0017 Accuracy 0.4074\n",
      "Epoch 48 Batch 15 Loss 0.0020 Accuracy 0.3568\n",
      "Epoch 48 Batch 30 Loss 0.0027 Accuracy 0.3589\n",
      "Epoch 48 Loss 0.0023 Accuracy 0.3536\n",
      "Time taken for 1 epoch: 7.738042593002319 secs\n",
      "\n",
      "Epoch 49 Batch 0 Loss 0.0059 Accuracy 0.3629\n",
      "Epoch 49 Batch 15 Loss 0.0028 Accuracy 0.3395\n",
      "Epoch 49 Batch 30 Loss 0.0024 Accuracy 0.3543\n",
      "Epoch 49 Loss 0.0028 Accuracy 0.3567\n",
      "Time taken for 1 epoch: 7.714922189712524 secs\n",
      "\n",
      "Epoch 50 Batch 0 Loss 0.0005 Accuracy 0.4525\n",
      "Epoch 50 Batch 15 Loss 0.0010 Accuracy 0.3660\n",
      "Epoch 50 Batch 30 Loss 0.0018 Accuracy 0.3538\n",
      "Epoch 50 Loss 0.0018 Accuracy 0.3541\n",
      "Time taken for 1 epoch: 7.878788709640503 secs\n",
      "\n",
      "Epoch 51 Batch 0 Loss 0.0061 Accuracy 0.3433\n",
      "Epoch 51 Batch 15 Loss 0.0019 Accuracy 0.3617\n",
      "Epoch 51 Batch 30 Loss 0.0022 Accuracy 0.3569\n",
      "Epoch 51 Loss 0.0023 Accuracy 0.3562\n",
      "Time taken for 1 epoch: 7.665699243545532 secs\n",
      "\n",
      "Epoch 52 Batch 0 Loss 0.0028 Accuracy 0.4129\n",
      "Epoch 52 Batch 15 Loss 0.0028 Accuracy 0.3485\n",
      "Epoch 52 Batch 30 Loss 0.0030 Accuracy 0.3506\n",
      "Epoch 52 Loss 0.0032 Accuracy 0.3522\n",
      "Time taken for 1 epoch: 7.824730634689331 secs\n",
      "\n",
      "Epoch 53 Batch 0 Loss 0.0045 Accuracy 0.3820\n",
      "Epoch 53 Batch 15 Loss 0.0031 Accuracy 0.3435\n",
      "Epoch 53 Batch 30 Loss 0.0037 Accuracy 0.3565\n",
      "Epoch 53 Loss 0.0033 Accuracy 0.3546\n",
      "Time taken for 1 epoch: 7.71462082862854 secs\n",
      "\n",
      "Epoch 54 Batch 0 Loss 0.0002 Accuracy 0.3854\n",
      "Epoch 54 Batch 15 Loss 0.0023 Accuracy 0.3507\n",
      "Epoch 54 Batch 30 Loss 0.0026 Accuracy 0.3501\n",
      "Epoch 54 Loss 0.0024 Accuracy 0.3555\n",
      "Time taken for 1 epoch: 7.7175984382629395 secs\n",
      "\n",
      "Epoch 55 Batch 0 Loss 0.0007 Accuracy 0.3130\n",
      "Epoch 55 Batch 15 Loss 0.0017 Accuracy 0.3497\n",
      "Epoch 55 Batch 30 Loss 0.0027 Accuracy 0.3579\n",
      "Epoch 55 Loss 0.0026 Accuracy 0.3535\n",
      "Time taken for 1 epoch: 7.812883615493774 secs\n",
      "\n",
      "Epoch 56 Batch 0 Loss 0.0024 Accuracy 0.3958\n",
      "Epoch 56 Batch 15 Loss 0.0030 Accuracy 0.3414\n",
      "Epoch 56 Batch 30 Loss 0.0026 Accuracy 0.3544\n",
      "Epoch 56 Loss 0.0025 Accuracy 0.3533\n",
      "Time taken for 1 epoch: 7.702689170837402 secs\n",
      "\n",
      "Epoch 57 Batch 0 Loss 0.0003 Accuracy 0.3813\n",
      "Epoch 57 Batch 15 Loss 0.0010 Accuracy 0.3638\n",
      "Epoch 57 Batch 30 Loss 0.0018 Accuracy 0.3592\n",
      "Epoch 57 Loss 0.0021 Accuracy 0.3590\n",
      "Time taken for 1 epoch: 7.734030485153198 secs\n",
      "\n",
      "Epoch 58 Batch 0 Loss 0.0009 Accuracy 0.3626\n",
      "Epoch 58 Batch 15 Loss 0.0023 Accuracy 0.3637\n",
      "Epoch 58 Batch 30 Loss 0.0024 Accuracy 0.3543\n",
      "Epoch 58 Loss 0.0022 Accuracy 0.3548\n",
      "Time taken for 1 epoch: 7.642745494842529 secs\n",
      "\n",
      "Epoch 59 Batch 0 Loss 0.0006 Accuracy 0.3543\n",
      "Epoch 59 Batch 15 Loss 0.0027 Accuracy 0.3476\n",
      "Epoch 59 Batch 30 Loss 0.0024 Accuracy 0.3474\n",
      "Epoch 59 Loss 0.0022 Accuracy 0.3492\n",
      "Time taken for 1 epoch: 7.962489128112793 secs\n",
      "\n",
      "Epoch 60 Batch 0 Loss 0.0103 Accuracy 0.3252\n",
      "Epoch 60 Batch 15 Loss 0.0026 Accuracy 0.3474\n",
      "Epoch 60 Batch 30 Loss 0.0021 Accuracy 0.3423\n",
      "Epoch 60 Loss 0.0019 Accuracy 0.3439\n",
      "Time taken for 1 epoch: 8.119736909866333 secs\n",
      "\n",
      "Epoch 61 Batch 0 Loss 0.0011 Accuracy 0.3504\n",
      "Epoch 61 Batch 15 Loss 0.0011 Accuracy 0.3554\n",
      "Epoch 61 Batch 30 Loss 0.0017 Accuracy 0.3530\n",
      "Epoch 61 Loss 0.0017 Accuracy 0.3556\n",
      "Time taken for 1 epoch: 7.708042621612549 secs\n",
      "\n",
      "Epoch 62 Batch 0 Loss 0.0003 Accuracy 0.3212\n",
      "Epoch 62 Batch 15 Loss 0.0023 Accuracy 0.3522\n",
      "Epoch 62 Batch 30 Loss 0.0021 Accuracy 0.3520\n",
      "Epoch 62 Loss 0.0019 Accuracy 0.3538\n",
      "Time taken for 1 epoch: 7.700897693634033 secs\n",
      "\n",
      "Epoch 63 Batch 0 Loss 0.0006 Accuracy 0.2795\n",
      "Epoch 63 Batch 15 Loss 0.0027 Accuracy 0.3478\n",
      "Epoch 63 Batch 30 Loss 0.0027 Accuracy 0.3513\n",
      "Epoch 63 Loss 0.0025 Accuracy 0.3538\n",
      "Time taken for 1 epoch: 7.736708879470825 secs\n",
      "\n",
      "Epoch 64 Batch 0 Loss 0.0012 Accuracy 0.3842\n",
      "Epoch 64 Batch 15 Loss 0.0014 Accuracy 0.3548\n",
      "Epoch 64 Batch 30 Loss 0.0014 Accuracy 0.3487\n",
      "Epoch 64 Loss 0.0013 Accuracy 0.3517\n",
      "Time taken for 1 epoch: 7.705542802810669 secs\n",
      "\n",
      "Epoch 65 Batch 0 Loss 0.0021 Accuracy 0.3146\n",
      "Epoch 65 Batch 15 Loss 0.0028 Accuracy 0.3467\n",
      "Epoch 65 Batch 30 Loss 0.0021 Accuracy 0.3504\n",
      "Epoch 65 Loss 0.0020 Accuracy 0.3533\n",
      "Time taken for 1 epoch: 7.720247030258179 secs\n",
      "\n",
      "Epoch 66 Batch 0 Loss 0.0035 Accuracy 0.3922\n",
      "Epoch 66 Batch 15 Loss 0.0041 Accuracy 0.3634\n",
      "Epoch 66 Batch 30 Loss 0.0038 Accuracy 0.3522\n",
      "Epoch 66 Loss 0.0034 Accuracy 0.3522\n",
      "Time taken for 1 epoch: 7.823329210281372 secs\n",
      "\n",
      "Epoch 67 Batch 0 Loss 0.0029 Accuracy 0.3514\n",
      "Epoch 67 Batch 15 Loss 0.0022 Accuracy 0.3590\n",
      "Epoch 67 Batch 30 Loss 0.0018 Accuracy 0.3611\n",
      "Saving checkpoint for epoch 67 at ./checkpoints/train/ckpt-25\n",
      "Epoch 67 Loss 0.0019 Accuracy 0.3606\n",
      "Time taken for 1 epoch: 8.012776136398315 secs\n",
      "\n",
      "Epoch 68 Batch 0 Loss 0.0004 Accuracy 0.3643\n",
      "Epoch 68 Batch 15 Loss 0.0019 Accuracy 0.3389\n",
      "Epoch 68 Batch 30 Loss 0.0023 Accuracy 0.3464\n",
      "Epoch 68 Loss 0.0028 Accuracy 0.3484\n",
      "Time taken for 1 epoch: 7.871511459350586 secs\n",
      "\n",
      "Epoch 69 Batch 0 Loss 0.0004 Accuracy 0.2943\n",
      "Epoch 69 Batch 15 Loss 0.0026 Accuracy 0.3534\n",
      "Epoch 69 Batch 30 Loss 0.0030 Accuracy 0.3518\n",
      "Epoch 69 Loss 0.0026 Accuracy 0.3531\n",
      "Time taken for 1 epoch: 7.734257459640503 secs\n",
      "\n",
      "Epoch 70 Batch 0 Loss 0.0003 Accuracy 0.3263\n",
      "Epoch 70 Batch 15 Loss 0.0017 Accuracy 0.3437\n",
      "Epoch 70 Batch 30 Loss 0.0017 Accuracy 0.3466\n",
      "Epoch 70 Loss 0.0017 Accuracy 0.3515\n",
      "Time taken for 1 epoch: 7.907585144042969 secs\n",
      "\n",
      "Epoch 71 Batch 0 Loss 0.0052 Accuracy 0.3484\n",
      "Epoch 71 Batch 15 Loss 0.0019 Accuracy 0.3432\n",
      "Epoch 71 Batch 30 Loss 0.0025 Accuracy 0.3495\n",
      "Epoch 71 Loss 0.0024 Accuracy 0.3512\n",
      "Time taken for 1 epoch: 7.822875499725342 secs\n",
      "\n",
      "Epoch 72 Batch 0 Loss 0.0032 Accuracy 0.4192\n",
      "Epoch 72 Batch 15 Loss 0.0028 Accuracy 0.3504\n",
      "Epoch 72 Batch 30 Loss 0.0023 Accuracy 0.3498\n",
      "Epoch 72 Loss 0.0022 Accuracy 0.3530\n",
      "Time taken for 1 epoch: 7.764694690704346 secs\n",
      "\n",
      "Epoch 73 Batch 0 Loss 0.0010 Accuracy 0.3273\n",
      "Epoch 73 Batch 15 Loss 0.0014 Accuracy 0.3431\n",
      "Epoch 73 Batch 30 Loss 0.0013 Accuracy 0.3453\n",
      "Epoch 73 Loss 0.0015 Accuracy 0.3488\n",
      "Time taken for 1 epoch: 7.949660062789917 secs\n",
      "\n",
      "Epoch 74 Batch 0 Loss 0.0132 Accuracy 0.3299\n",
      "Epoch 74 Batch 15 Loss 0.0024 Accuracy 0.3506\n",
      "Epoch 74 Batch 30 Loss 0.0021 Accuracy 0.3528\n",
      "Epoch 74 Loss 0.0017 Accuracy 0.3514\n",
      "Time taken for 1 epoch: 7.763503789901733 secs\n",
      "\n",
      "Epoch 75 Batch 0 Loss 0.0027 Accuracy 0.3323\n",
      "Epoch 75 Batch 15 Loss 0.0020 Accuracy 0.3422\n",
      "Epoch 75 Batch 30 Loss 0.0019 Accuracy 0.3455\n",
      "Epoch 75 Loss 0.0019 Accuracy 0.3483\n",
      "Time taken for 1 epoch: 7.9933576583862305 secs\n",
      "\n",
      "Epoch 76 Batch 0 Loss 0.0047 Accuracy 0.3491\n",
      "Epoch 76 Batch 15 Loss 0.0016 Accuracy 0.3523\n",
      "Epoch 76 Batch 30 Loss 0.0023 Accuracy 0.3502\n",
      "Epoch 76 Loss 0.0021 Accuracy 0.3533\n",
      "Time taken for 1 epoch: 7.790689706802368 secs\n",
      "\n",
      "Epoch 77 Batch 0 Loss 0.0144 Accuracy 0.3290\n",
      "Epoch 77 Batch 15 Loss 0.0032 Accuracy 0.3565\n",
      "Epoch 77 Batch 30 Loss 0.0025 Accuracy 0.3555\n",
      "Epoch 77 Loss 0.0023 Accuracy 0.3528\n",
      "Time taken for 1 epoch: 7.705709934234619 secs\n",
      "\n",
      "Epoch 78 Batch 0 Loss 0.0069 Accuracy 0.3750\n",
      "Epoch 78 Batch 15 Loss 0.0026 Accuracy 0.3535\n",
      "Epoch 78 Batch 30 Loss 0.0024 Accuracy 0.3570\n",
      "Epoch 78 Loss 0.0025 Accuracy 0.3595\n",
      "Time taken for 1 epoch: 7.586896181106567 secs\n",
      "\n",
      "Epoch 79 Batch 0 Loss 0.0003 Accuracy 0.3424\n",
      "Epoch 79 Batch 15 Loss 0.0033 Accuracy 0.3481\n",
      "Epoch 79 Batch 30 Loss 0.0022 Accuracy 0.3490\n",
      "Epoch 79 Loss 0.0024 Accuracy 0.3500\n",
      "Time taken for 1 epoch: 7.687283039093018 secs\n",
      "\n",
      "Epoch 80 Batch 0 Loss 0.0002 Accuracy 0.3496\n",
      "Epoch 80 Batch 15 Loss 0.0031 Accuracy 0.3575\n",
      "Epoch 80 Batch 30 Loss 0.0028 Accuracy 0.3538\n",
      "Epoch 80 Loss 0.0025 Accuracy 0.3562\n",
      "Time taken for 1 epoch: 7.749655485153198 secs\n",
      "\n",
      "Epoch 81 Batch 0 Loss 0.0048 Accuracy 0.3813\n",
      "Epoch 81 Batch 15 Loss 0.0022 Accuracy 0.3742\n",
      "Epoch 81 Batch 30 Loss 0.0020 Accuracy 0.3559\n",
      "Epoch 81 Loss 0.0019 Accuracy 0.3556\n",
      "Time taken for 1 epoch: 7.531373977661133 secs\n",
      "\n",
      "Epoch 82 Batch 0 Loss 0.0005 Accuracy 0.3812\n",
      "Epoch 82 Batch 15 Loss 0.0027 Accuracy 0.3591\n",
      "Epoch 82 Batch 30 Loss 0.0021 Accuracy 0.3498\n",
      "Epoch 82 Loss 0.0021 Accuracy 0.3526\n",
      "Time taken for 1 epoch: 7.856403112411499 secs\n",
      "\n",
      "Epoch 83 Batch 0 Loss 0.0033 Accuracy 0.3804\n",
      "Epoch 83 Batch 15 Loss 0.0019 Accuracy 0.3471\n",
      "Epoch 83 Batch 30 Loss 0.0021 Accuracy 0.3526\n",
      "Epoch 83 Loss 0.0018 Accuracy 0.3526\n",
      "Time taken for 1 epoch: 7.895156383514404 secs\n",
      "\n",
      "Epoch 84 Batch 0 Loss 0.0016 Accuracy 0.3107\n",
      "Epoch 84 Batch 15 Loss 0.0030 Accuracy 0.3578\n",
      "Epoch 84 Batch 30 Loss 0.0030 Accuracy 0.3591\n",
      "Epoch 84 Loss 0.0032 Accuracy 0.3568\n",
      "Time taken for 1 epoch: 7.637279987335205 secs\n",
      "\n",
      "Epoch 85 Batch 0 Loss 0.0004 Accuracy 0.3816\n",
      "Epoch 85 Batch 15 Loss 0.0018 Accuracy 0.3485\n",
      "Epoch 85 Batch 30 Loss 0.0025 Accuracy 0.3516\n",
      "Epoch 85 Loss 0.0026 Accuracy 0.3489\n",
      "Time taken for 1 epoch: 7.884242534637451 secs\n",
      "\n",
      "Epoch 86 Batch 0 Loss 0.0008 Accuracy 0.3769\n",
      "Epoch 86 Batch 15 Loss 0.0014 Accuracy 0.3455\n",
      "Epoch 86 Batch 30 Loss 0.0020 Accuracy 0.3474\n",
      "Epoch 86 Loss 0.0022 Accuracy 0.3458\n",
      "Time taken for 1 epoch: 7.894691705703735 secs\n",
      "\n",
      "Epoch 87 Batch 0 Loss 0.0020 Accuracy 0.3330\n",
      "Epoch 87 Batch 15 Loss 0.0027 Accuracy 0.3497\n",
      "Epoch 87 Batch 30 Loss 0.0029 Accuracy 0.3521\n",
      "Epoch 87 Loss 0.0029 Accuracy 0.3513\n",
      "Time taken for 1 epoch: 7.843010663986206 secs\n",
      "\n",
      "Epoch 88 Batch 0 Loss 0.0032 Accuracy 0.3466\n",
      "Epoch 88 Batch 15 Loss 0.0014 Accuracy 0.3645\n",
      "Epoch 88 Batch 30 Loss 0.0014 Accuracy 0.3545\n",
      "Epoch 88 Loss 0.0016 Accuracy 0.3532\n",
      "Time taken for 1 epoch: 7.6778600215911865 secs\n",
      "\n",
      "Epoch 89 Batch 0 Loss 0.0005 Accuracy 0.3350\n",
      "Epoch 89 Batch 15 Loss 0.0013 Accuracy 0.3330\n",
      "Epoch 89 Batch 30 Loss 0.0011 Accuracy 0.3448\n",
      "Epoch 89 Loss 0.0010 Accuracy 0.3496\n",
      "Time taken for 1 epoch: 7.847702503204346 secs\n",
      "\n",
      "Epoch 90 Batch 0 Loss 0.0006 Accuracy 0.3960\n",
      "Epoch 90 Batch 15 Loss 0.0005 Accuracy 0.3456\n",
      "Epoch 90 Batch 30 Loss 0.0009 Accuracy 0.3489\n",
      "Epoch 90 Loss 0.0009 Accuracy 0.3546\n",
      "Time taken for 1 epoch: 7.755138158798218 secs\n",
      "\n",
      "Epoch 91 Batch 0 Loss 0.0020 Accuracy 0.3861\n",
      "Epoch 91 Batch 15 Loss 0.0015 Accuracy 0.3446\n",
      "Epoch 91 Batch 30 Loss 0.0015 Accuracy 0.3504\n",
      "Epoch 91 Loss 0.0018 Accuracy 0.3516\n",
      "Time taken for 1 epoch: 7.8416056632995605 secs\n",
      "\n",
      "Epoch 92 Batch 0 Loss 0.0002 Accuracy 0.3521\n",
      "Epoch 92 Batch 15 Loss 0.0021 Accuracy 0.3604\n",
      "Epoch 92 Batch 30 Loss 0.0019 Accuracy 0.3610\n",
      "Epoch 92 Loss 0.0020 Accuracy 0.3580\n",
      "Time taken for 1 epoch: 7.691117763519287 secs\n",
      "\n",
      "Epoch 93 Batch 0 Loss 0.0019 Accuracy 0.3379\n",
      "Epoch 93 Batch 15 Loss 0.0013 Accuracy 0.3681\n",
      "Epoch 93 Batch 30 Loss 0.0022 Accuracy 0.3595\n",
      "Epoch 93 Loss 0.0023 Accuracy 0.3589\n",
      "Time taken for 1 epoch: 7.6099278926849365 secs\n",
      "\n",
      "Epoch 94 Batch 0 Loss 0.0023 Accuracy 0.4195\n",
      "Epoch 94 Batch 15 Loss 0.0030 Accuracy 0.3605\n",
      "Epoch 94 Batch 30 Loss 0.0023 Accuracy 0.3564\n",
      "Epoch 94 Loss 0.0022 Accuracy 0.3567\n",
      "Time taken for 1 epoch: 7.6580188274383545 secs\n",
      "\n",
      "Epoch 95 Batch 0 Loss 0.0012 Accuracy 0.3945\n",
      "Epoch 95 Batch 15 Loss 0.0022 Accuracy 0.3456\n",
      "Epoch 95 Batch 30 Loss 0.0022 Accuracy 0.3523\n",
      "Epoch 95 Loss 0.0022 Accuracy 0.3505\n",
      "Time taken for 1 epoch: 7.897796630859375 secs\n",
      "\n",
      "Epoch 96 Batch 0 Loss 0.0008 Accuracy 0.3452\n",
      "Epoch 96 Batch 15 Loss 0.0014 Accuracy 0.3493\n",
      "Epoch 96 Batch 30 Loss 0.0018 Accuracy 0.3482\n",
      "Epoch 96 Loss 0.0017 Accuracy 0.3530\n",
      "Time taken for 1 epoch: 7.831279516220093 secs\n",
      "\n",
      "Epoch 97 Batch 0 Loss 0.0002 Accuracy 0.4043\n",
      "Epoch 97 Batch 15 Loss 0.0017 Accuracy 0.3581\n",
      "Epoch 97 Batch 30 Loss 0.0024 Accuracy 0.3618\n",
      "Epoch 97 Loss 0.0026 Accuracy 0.3572\n",
      "Time taken for 1 epoch: 7.618875503540039 secs\n",
      "\n",
      "Epoch 98 Batch 0 Loss 0.0004 Accuracy 0.3245\n",
      "Epoch 98 Batch 15 Loss 0.0018 Accuracy 0.3494\n",
      "Epoch 98 Batch 30 Loss 0.0015 Accuracy 0.3468\n",
      "Epoch 98 Loss 0.0017 Accuracy 0.3500\n",
      "Time taken for 1 epoch: 7.858564615249634 secs\n",
      "\n",
      "Epoch 99 Batch 0 Loss 0.0001 Accuracy 0.2899\n",
      "Epoch 99 Batch 15 Loss 0.0009 Accuracy 0.3639\n",
      "Epoch 99 Batch 30 Loss 0.0010 Accuracy 0.3549\n",
      "Epoch 99 Loss 0.0014 Accuracy 0.3526\n",
      "Time taken for 1 epoch: 7.826598405838013 secs\n",
      "\n",
      "Epoch 100 Batch 0 Loss 0.0005 Accuracy 0.3243\n",
      "Epoch 100 Batch 15 Loss 0.0017 Accuracy 0.3638\n",
      "Epoch 100 Batch 30 Loss 0.0020 Accuracy 0.3567\n",
      "Epoch 100 Loss 0.0025 Accuracy 0.3576\n",
      "Time taken for 1 epoch: 7.735279321670532 secs\n",
      "\n",
      "Epoch 101 Batch 0 Loss 0.0022 Accuracy 0.3560\n",
      "Epoch 101 Batch 15 Loss 0.0025 Accuracy 0.3772\n",
      "Epoch 101 Batch 30 Loss 0.0028 Accuracy 0.3558\n",
      "Epoch 101 Loss 0.0024 Accuracy 0.3585\n",
      "Time taken for 1 epoch: 7.714662075042725 secs\n",
      "\n",
      "Epoch 102 Batch 0 Loss 0.0016 Accuracy 0.3468\n",
      "Epoch 102 Batch 15 Loss 0.0023 Accuracy 0.3499\n",
      "Epoch 102 Batch 30 Loss 0.0018 Accuracy 0.3516\n",
      "Epoch 102 Loss 0.0017 Accuracy 0.3531\n",
      "Time taken for 1 epoch: 7.767961025238037 secs\n",
      "\n",
      "Epoch 103 Batch 0 Loss 0.0016 Accuracy 0.4499\n",
      "Epoch 103 Batch 15 Loss 0.0017 Accuracy 0.3475\n",
      "Epoch 103 Batch 30 Loss 0.0019 Accuracy 0.3510\n",
      "Epoch 103 Loss 0.0019 Accuracy 0.3529\n",
      "Time taken for 1 epoch: 7.774165153503418 secs\n",
      "\n",
      "Epoch 104 Batch 0 Loss 0.0025 Accuracy 0.3330\n",
      "Epoch 104 Batch 15 Loss 0.0018 Accuracy 0.3545\n",
      "Epoch 104 Batch 30 Loss 0.0014 Accuracy 0.3560\n",
      "Epoch 104 Loss 0.0014 Accuracy 0.3545\n",
      "Time taken for 1 epoch: 7.742015600204468 secs\n",
      "\n",
      "Epoch 105 Batch 0 Loss 0.0021 Accuracy 0.3101\n",
      "Epoch 105 Batch 15 Loss 0.0007 Accuracy 0.3419\n",
      "Epoch 105 Batch 30 Loss 0.0014 Accuracy 0.3497\n",
      "Epoch 105 Loss 0.0014 Accuracy 0.3507\n",
      "Time taken for 1 epoch: 7.903199672698975 secs\n",
      "\n",
      "Epoch 106 Batch 0 Loss 0.0072 Accuracy 0.3594\n",
      "Epoch 106 Batch 15 Loss 0.0021 Accuracy 0.3689\n",
      "Epoch 106 Batch 30 Loss 0.0015 Accuracy 0.3549\n",
      "Epoch 106 Loss 0.0017 Accuracy 0.3548\n",
      "Time taken for 1 epoch: 7.796245098114014 secs\n",
      "\n",
      "Epoch 107 Batch 0 Loss 0.0002 Accuracy 0.3504\n",
      "Epoch 107 Batch 15 Loss 0.0019 Accuracy 0.3526\n",
      "Epoch 107 Batch 30 Loss 0.0016 Accuracy 0.3563\n",
      "Epoch 107 Loss 0.0020 Accuracy 0.3571\n",
      "Time taken for 1 epoch: 7.73237943649292 secs\n",
      "\n",
      "Epoch 108 Batch 0 Loss 0.0005 Accuracy 0.3070\n",
      "Epoch 108 Batch 15 Loss 0.0015 Accuracy 0.3575\n",
      "Epoch 108 Batch 30 Loss 0.0020 Accuracy 0.3539\n",
      "Epoch 108 Loss 0.0020 Accuracy 0.3522\n",
      "Time taken for 1 epoch: 7.917217969894409 secs\n",
      "\n",
      "Epoch 109 Batch 0 Loss 0.0020 Accuracy 0.4262\n",
      "Epoch 109 Batch 15 Loss 0.0035 Accuracy 0.3579\n",
      "Epoch 109 Batch 30 Loss 0.0031 Accuracy 0.3529\n",
      "Epoch 109 Loss 0.0042 Accuracy 0.3500\n",
      "Time taken for 1 epoch: 7.92051362991333 secs\n",
      "\n",
      "Epoch 110 Batch 0 Loss 0.0075 Accuracy 0.3581\n",
      "Epoch 110 Batch 15 Loss 0.0044 Accuracy 0.3627\n",
      "Epoch 110 Batch 30 Loss 0.0036 Accuracy 0.3566\n",
      "Epoch 110 Loss 0.0038 Accuracy 0.3522\n",
      "Time taken for 1 epoch: 7.811557292938232 secs\n",
      "\n",
      "Epoch 111 Batch 0 Loss 0.0005 Accuracy 0.4420\n",
      "Epoch 111 Batch 15 Loss 0.0039 Accuracy 0.3422\n",
      "Epoch 111 Batch 30 Loss 0.0033 Accuracy 0.3527\n",
      "Epoch 111 Loss 0.0029 Accuracy 0.3507\n",
      "Time taken for 1 epoch: 7.811690092086792 secs\n",
      "\n",
      "Epoch 112 Batch 0 Loss 0.0020 Accuracy 0.3735\n",
      "Epoch 112 Batch 15 Loss 0.0019 Accuracy 0.3596\n",
      "Epoch 112 Batch 30 Loss 0.0019 Accuracy 0.3575\n",
      "Epoch 112 Loss 0.0019 Accuracy 0.3559\n",
      "Time taken for 1 epoch: 7.683959722518921 secs\n",
      "\n",
      "Epoch 113 Batch 0 Loss 0.0034 Accuracy 0.3446\n",
      "Epoch 113 Batch 15 Loss 0.0014 Accuracy 0.3659\n",
      "Epoch 113 Batch 30 Loss 0.0015 Accuracy 0.3579\n",
      "Epoch 113 Loss 0.0015 Accuracy 0.3540\n",
      "Time taken for 1 epoch: 7.711190223693848 secs\n",
      "\n",
      "Epoch 114 Batch 0 Loss 0.0002 Accuracy 0.2965\n",
      "Epoch 114 Batch 15 Loss 0.0011 Accuracy 0.3529\n",
      "Epoch 114 Batch 30 Loss 0.0016 Accuracy 0.3526\n",
      "Epoch 114 Loss 0.0017 Accuracy 0.3518\n",
      "Time taken for 1 epoch: 7.791811227798462 secs\n",
      "\n",
      "Epoch 115 Batch 0 Loss 0.0002 Accuracy 0.3897\n",
      "Epoch 115 Batch 15 Loss 0.0018 Accuracy 0.3540\n",
      "Epoch 115 Batch 30 Loss 0.0017 Accuracy 0.3531\n",
      "Epoch 115 Loss 0.0020 Accuracy 0.3511\n",
      "Time taken for 1 epoch: 7.843703031539917 secs\n",
      "\n",
      "Epoch 116 Batch 0 Loss 0.0008 Accuracy 0.3759\n",
      "Epoch 116 Batch 15 Loss 0.0019 Accuracy 0.3585\n",
      "Epoch 116 Batch 30 Loss 0.0016 Accuracy 0.3565\n",
      "Epoch 116 Loss 0.0025 Accuracy 0.3541\n",
      "Time taken for 1 epoch: 7.777061700820923 secs\n",
      "\n",
      "Epoch 117 Batch 0 Loss 0.0005 Accuracy 0.4052\n",
      "Epoch 117 Batch 15 Loss 0.0057 Accuracy 0.3539\n",
      "Epoch 117 Batch 30 Loss 0.0043 Accuracy 0.3531\n",
      "Epoch 117 Loss 0.0040 Accuracy 0.3551\n",
      "Time taken for 1 epoch: 7.658290863037109 secs\n",
      "\n",
      "Epoch 118 Batch 0 Loss 0.0005 Accuracy 0.3859\n",
      "Epoch 118 Batch 15 Loss 0.0021 Accuracy 0.3500\n",
      "Epoch 118 Batch 30 Loss 0.0023 Accuracy 0.3563\n",
      "Epoch 118 Loss 0.0021 Accuracy 0.3511\n",
      "Time taken for 1 epoch: 7.835552215576172 secs\n",
      "\n",
      "Epoch 119 Batch 0 Loss 0.0005 Accuracy 0.4291\n",
      "Epoch 119 Batch 15 Loss 0.0008 Accuracy 0.3434\n",
      "Epoch 119 Batch 30 Loss 0.0012 Accuracy 0.3418\n",
      "Epoch 119 Loss 0.0012 Accuracy 0.3483\n",
      "Time taken for 1 epoch: 7.951201915740967 secs\n",
      "\n",
      "Epoch 120 Batch 0 Loss 0.0001 Accuracy 0.4062\n",
      "Epoch 120 Batch 15 Loss 0.0024 Accuracy 0.3681\n",
      "Epoch 120 Batch 30 Loss 0.0020 Accuracy 0.3503\n",
      "Epoch 120 Loss 0.0019 Accuracy 0.3486\n",
      "Time taken for 1 epoch: 7.915917158126831 secs\n",
      "\n",
      "Epoch 121 Batch 0 Loss 0.0004 Accuracy 0.3433\n",
      "Epoch 121 Batch 15 Loss 0.0012 Accuracy 0.3795\n",
      "Epoch 121 Batch 30 Loss 0.0020 Accuracy 0.3623\n",
      "Saving checkpoint for epoch 121 at ./checkpoints/train/ckpt-26\n",
      "Epoch 121 Loss 0.0018 Accuracy 0.3611\n",
      "Time taken for 1 epoch: 8.160457372665405 secs\n",
      "\n",
      "Epoch 122 Batch 0 Loss 0.0008 Accuracy 0.4444\n",
      "Epoch 122 Batch 15 Loss 0.0014 Accuracy 0.3469\n",
      "Epoch 122 Batch 30 Loss 0.0012 Accuracy 0.3489\n",
      "Epoch 122 Loss 0.0012 Accuracy 0.3496\n",
      "Time taken for 1 epoch: 7.900045394897461 secs\n",
      "\n",
      "Epoch 123 Batch 0 Loss 0.0006 Accuracy 0.3695\n",
      "Epoch 123 Batch 15 Loss 0.0012 Accuracy 0.3567\n",
      "Epoch 123 Batch 30 Loss 0.0013 Accuracy 0.3549\n",
      "Epoch 123 Loss 0.0014 Accuracy 0.3573\n",
      "Time taken for 1 epoch: 7.606213569641113 secs\n",
      "\n",
      "Epoch 124 Batch 0 Loss 0.0004 Accuracy 0.3764\n",
      "Epoch 124 Batch 15 Loss 0.0019 Accuracy 0.3540\n",
      "Epoch 124 Batch 30 Loss 0.0016 Accuracy 0.3524\n",
      "Epoch 124 Loss 0.0018 Accuracy 0.3533\n",
      "Time taken for 1 epoch: 7.742831230163574 secs\n",
      "\n",
      "Epoch 125 Batch 0 Loss 0.0003 Accuracy 0.3476\n",
      "Epoch 125 Batch 15 Loss 0.0019 Accuracy 0.3523\n",
      "Epoch 125 Batch 30 Loss 0.0018 Accuracy 0.3547\n",
      "Epoch 125 Loss 0.0021 Accuracy 0.3552\n",
      "Time taken for 1 epoch: 7.7576024532318115 secs\n",
      "\n",
      "Epoch 126 Batch 0 Loss 0.0007 Accuracy 0.3442\n",
      "Epoch 126 Batch 15 Loss 0.0019 Accuracy 0.3523\n",
      "Epoch 126 Batch 30 Loss 0.0021 Accuracy 0.3577\n",
      "Epoch 126 Loss 0.0023 Accuracy 0.3550\n",
      "Time taken for 1 epoch: 7.661098957061768 secs\n",
      "\n",
      "Epoch 127 Batch 0 Loss 0.0035 Accuracy 0.3318\n",
      "Epoch 127 Batch 15 Loss 0.0014 Accuracy 0.3404\n",
      "Epoch 127 Batch 30 Loss 0.0018 Accuracy 0.3477\n",
      "Epoch 127 Loss 0.0018 Accuracy 0.3504\n",
      "Time taken for 1 epoch: 7.784298658370972 secs\n",
      "\n",
      "Epoch 128 Batch 0 Loss 0.0004 Accuracy 0.3570\n",
      "Epoch 128 Batch 15 Loss 0.0027 Accuracy 0.3676\n",
      "Epoch 128 Batch 30 Loss 0.0020 Accuracy 0.3620\n",
      "Saving checkpoint for epoch 128 at ./checkpoints/train/ckpt-27\n",
      "Epoch 128 Loss 0.0018 Accuracy 0.3635\n",
      "Time taken for 1 epoch: 8.0223867893219 secs\n",
      "\n",
      "Epoch 129 Batch 0 Loss 0.0017 Accuracy 0.3387\n",
      "Epoch 129 Batch 15 Loss 0.0008 Accuracy 0.3491\n",
      "Epoch 129 Batch 30 Loss 0.0014 Accuracy 0.3561\n",
      "Epoch 129 Loss 0.0016 Accuracy 0.3538\n",
      "Time taken for 1 epoch: 7.710737228393555 secs\n",
      "\n",
      "Epoch 130 Batch 0 Loss 0.0002 Accuracy 0.2977\n",
      "Epoch 130 Batch 15 Loss 0.0010 Accuracy 0.3440\n",
      "Epoch 130 Batch 30 Loss 0.0013 Accuracy 0.3460\n",
      "Epoch 130 Loss 0.0014 Accuracy 0.3444\n",
      "Time taken for 1 epoch: 8.180694580078125 secs\n",
      "\n",
      "Epoch 131 Batch 0 Loss 0.0004 Accuracy 0.4219\n",
      "Epoch 131 Batch 15 Loss 0.0012 Accuracy 0.3524\n",
      "Epoch 131 Batch 30 Loss 0.0015 Accuracy 0.3566\n",
      "Epoch 131 Loss 0.0016 Accuracy 0.3525\n",
      "Time taken for 1 epoch: 7.933889627456665 secs\n",
      "\n",
      "Epoch 132 Batch 0 Loss 0.0005 Accuracy 0.3247\n",
      "Epoch 132 Batch 15 Loss 0.0018 Accuracy 0.3558\n",
      "Epoch 132 Batch 30 Loss 0.0025 Accuracy 0.3559\n",
      "Epoch 132 Loss 0.0021 Accuracy 0.3502\n",
      "Time taken for 1 epoch: 7.931139230728149 secs\n",
      "\n",
      "Epoch 133 Batch 0 Loss 0.0015 Accuracy 0.4047\n",
      "Epoch 133 Batch 15 Loss 0.0019 Accuracy 0.3573\n",
      "Epoch 133 Batch 30 Loss 0.0019 Accuracy 0.3540\n",
      "Epoch 133 Loss 0.0022 Accuracy 0.3499\n",
      "Time taken for 1 epoch: 7.836206674575806 secs\n",
      "\n",
      "Epoch 134 Batch 0 Loss 0.0001 Accuracy 0.3306\n",
      "Epoch 134 Batch 15 Loss 0.0019 Accuracy 0.3598\n",
      "Epoch 134 Batch 30 Loss 0.0017 Accuracy 0.3517\n",
      "Epoch 134 Loss 0.0016 Accuracy 0.3565\n",
      "Time taken for 1 epoch: 7.666079044342041 secs\n",
      "\n",
      "Epoch 135 Batch 0 Loss 0.0077 Accuracy 0.4046\n",
      "Epoch 135 Batch 15 Loss 0.0017 Accuracy 0.3538\n",
      "Epoch 135 Batch 30 Loss 0.0017 Accuracy 0.3546\n",
      "Epoch 135 Loss 0.0016 Accuracy 0.3527\n",
      "Time taken for 1 epoch: 7.793377161026001 secs\n",
      "\n",
      "Epoch 136 Batch 0 Loss 0.0086 Accuracy 0.4735\n",
      "Epoch 136 Batch 15 Loss 0.0021 Accuracy 0.3612\n",
      "Epoch 136 Batch 30 Loss 0.0017 Accuracy 0.3600\n",
      "Epoch 136 Loss 0.0015 Accuracy 0.3561\n",
      "Time taken for 1 epoch: 7.6641387939453125 secs\n",
      "\n",
      "Epoch 137 Batch 0 Loss 0.0002 Accuracy 0.4190\n",
      "Epoch 137 Batch 15 Loss 0.0019 Accuracy 0.3591\n",
      "Epoch 137 Batch 30 Loss 0.0018 Accuracy 0.3553\n",
      "Epoch 137 Loss 0.0017 Accuracy 0.3538\n",
      "Time taken for 1 epoch: 7.763965129852295 secs\n",
      "\n",
      "Epoch 138 Batch 0 Loss 0.0002 Accuracy 0.3291\n",
      "Epoch 138 Batch 15 Loss 0.0015 Accuracy 0.3555\n",
      "Epoch 138 Batch 30 Loss 0.0019 Accuracy 0.3531\n",
      "Epoch 138 Loss 0.0016 Accuracy 0.3493\n",
      "Time taken for 1 epoch: 7.865106821060181 secs\n",
      "\n",
      "Epoch 139 Batch 0 Loss 0.0002 Accuracy 0.3739\n",
      "Epoch 139 Batch 15 Loss 0.0016 Accuracy 0.3445\n",
      "Epoch 139 Batch 30 Loss 0.0015 Accuracy 0.3461\n",
      "Epoch 139 Loss 0.0014 Accuracy 0.3498\n",
      "Time taken for 1 epoch: 7.880014419555664 secs\n",
      "\n",
      "Epoch 140 Batch 0 Loss 0.0018 Accuracy 0.3854\n",
      "Epoch 140 Batch 15 Loss 0.0018 Accuracy 0.3546\n",
      "Epoch 140 Batch 30 Loss 0.0023 Accuracy 0.3468\n",
      "Epoch 140 Loss 0.0023 Accuracy 0.3484\n",
      "Time taken for 1 epoch: 7.8527305126190186 secs\n",
      "\n",
      "Epoch 141 Batch 0 Loss 0.0003 Accuracy 0.3869\n",
      "Epoch 141 Batch 15 Loss 0.0024 Accuracy 0.3617\n",
      "Epoch 141 Batch 30 Loss 0.0025 Accuracy 0.3579\n",
      "Epoch 141 Loss 0.0025 Accuracy 0.3567\n",
      "Time taken for 1 epoch: 7.672025680541992 secs\n",
      "\n",
      "Epoch 142 Batch 0 Loss 0.0003 Accuracy 0.3708\n",
      "Epoch 142 Batch 15 Loss 0.0020 Accuracy 0.3535\n",
      "Epoch 142 Batch 30 Loss 0.0020 Accuracy 0.3535\n",
      "Epoch 142 Loss 0.0017 Accuracy 0.3552\n",
      "Time taken for 1 epoch: 7.706026315689087 secs\n",
      "\n",
      "Epoch 143 Batch 0 Loss 0.0022 Accuracy 0.3471\n",
      "Epoch 143 Batch 15 Loss 0.0011 Accuracy 0.3584\n",
      "Epoch 143 Batch 30 Loss 0.0012 Accuracy 0.3549\n",
      "Epoch 143 Loss 0.0015 Accuracy 0.3553\n",
      "Time taken for 1 epoch: 7.746487379074097 secs\n",
      "\n",
      "Epoch 144 Batch 0 Loss 0.0029 Accuracy 0.3968\n",
      "Epoch 144 Batch 15 Loss 0.0017 Accuracy 0.3567\n",
      "Epoch 144 Batch 30 Loss 0.0015 Accuracy 0.3601\n",
      "Epoch 144 Loss 0.0014 Accuracy 0.3556\n",
      "Time taken for 1 epoch: 7.6334123611450195 secs\n",
      "\n",
      "Epoch 145 Batch 0 Loss 0.0044 Accuracy 0.3239\n",
      "Epoch 145 Batch 15 Loss 0.0013 Accuracy 0.3449\n",
      "Epoch 145 Batch 30 Loss 0.0017 Accuracy 0.3535\n",
      "Epoch 145 Loss 0.0016 Accuracy 0.3541\n",
      "Time taken for 1 epoch: 7.730165243148804 secs\n",
      "\n",
      "Epoch 146 Batch 0 Loss 0.0006 Accuracy 0.3906\n",
      "Epoch 146 Batch 15 Loss 0.0014 Accuracy 0.3588\n",
      "Epoch 146 Batch 30 Loss 0.0013 Accuracy 0.3571\n",
      "Epoch 146 Loss 0.0015 Accuracy 0.3532\n",
      "Time taken for 1 epoch: 7.827726364135742 secs\n",
      "\n",
      "Epoch 147 Batch 0 Loss 0.0002 Accuracy 0.3281\n",
      "Epoch 147 Batch 15 Loss 0.0021 Accuracy 0.3578\n",
      "Epoch 147 Batch 30 Loss 0.0025 Accuracy 0.3541\n",
      "Epoch 147 Loss 0.0024 Accuracy 0.3517\n",
      "Time taken for 1 epoch: 7.901951789855957 secs\n",
      "\n",
      "Epoch 148 Batch 0 Loss 0.0012 Accuracy 0.3572\n",
      "Epoch 148 Batch 15 Loss 0.0011 Accuracy 0.3529\n",
      "Epoch 148 Batch 30 Loss 0.0019 Accuracy 0.3514\n",
      "Epoch 148 Loss 0.0017 Accuracy 0.3507\n",
      "Time taken for 1 epoch: 7.868043661117554 secs\n",
      "\n",
      "Epoch 149 Batch 0 Loss 0.0011 Accuracy 0.2932\n",
      "Epoch 149 Batch 15 Loss 0.0014 Accuracy 0.3574\n",
      "Epoch 149 Batch 30 Loss 0.0016 Accuracy 0.3501\n",
      "Epoch 149 Loss 0.0018 Accuracy 0.3506\n",
      "Time taken for 1 epoch: 7.843360185623169 secs\n",
      "\n",
      "Epoch 150 Batch 0 Loss 0.0049 Accuracy 0.3613\n",
      "Epoch 150 Batch 15 Loss 0.0016 Accuracy 0.3495\n",
      "Epoch 150 Batch 30 Loss 0.0019 Accuracy 0.3565\n",
      "Epoch 150 Loss 0.0016 Accuracy 0.3534\n",
      "Time taken for 1 epoch: 7.821059703826904 secs\n",
      "\n",
      "Epoch 151 Batch 0 Loss 0.0002 Accuracy 0.3473\n",
      "Epoch 151 Batch 15 Loss 0.0023 Accuracy 0.3595\n",
      "Epoch 151 Batch 30 Loss 0.0016 Accuracy 0.3492\n",
      "Epoch 151 Loss 0.0022 Accuracy 0.3537\n",
      "Time taken for 1 epoch: 7.709022760391235 secs\n",
      "\n",
      "Epoch 152 Batch 0 Loss 0.0012 Accuracy 0.3010\n",
      "Epoch 152 Batch 15 Loss 0.0023 Accuracy 0.3641\n",
      "Epoch 152 Batch 30 Loss 0.0019 Accuracy 0.3589\n",
      "Epoch 152 Loss 0.0016 Accuracy 0.3534\n",
      "Time taken for 1 epoch: 7.772970676422119 secs\n",
      "\n",
      "Epoch 153 Batch 0 Loss 0.0053 Accuracy 0.3265\n",
      "Epoch 153 Batch 15 Loss 0.0017 Accuracy 0.3590\n",
      "Epoch 153 Batch 30 Loss 0.0012 Accuracy 0.3554\n",
      "Epoch 153 Loss 0.0013 Accuracy 0.3549\n",
      "Time taken for 1 epoch: 7.74104118347168 secs\n",
      "\n",
      "Epoch 154 Batch 0 Loss 0.0011 Accuracy 0.3625\n",
      "Epoch 154 Batch 15 Loss 0.0014 Accuracy 0.3630\n",
      "Epoch 154 Batch 30 Loss 0.0013 Accuracy 0.3543\n",
      "Epoch 154 Loss 0.0013 Accuracy 0.3546\n",
      "Time taken for 1 epoch: 7.665427207946777 secs\n",
      "\n",
      "Epoch 155 Batch 0 Loss 0.0004 Accuracy 0.3817\n",
      "Epoch 155 Batch 15 Loss 0.0010 Accuracy 0.3591\n",
      "Epoch 155 Batch 30 Loss 0.0014 Accuracy 0.3562\n",
      "Epoch 155 Loss 0.0014 Accuracy 0.3572\n",
      "Time taken for 1 epoch: 7.649445533752441 secs\n",
      "\n",
      "Epoch 156 Batch 0 Loss 0.0004 Accuracy 0.3250\n",
      "Epoch 156 Batch 15 Loss 0.0009 Accuracy 0.3478\n",
      "Epoch 156 Batch 30 Loss 0.0012 Accuracy 0.3560\n",
      "Epoch 156 Loss 0.0014 Accuracy 0.3567\n",
      "Time taken for 1 epoch: 7.6880576610565186 secs\n",
      "\n",
      "Epoch 157 Batch 0 Loss 0.0001 Accuracy 0.3676\n",
      "Epoch 157 Batch 15 Loss 0.0020 Accuracy 0.3450\n",
      "Epoch 157 Batch 30 Loss 0.0017 Accuracy 0.3480\n",
      "Epoch 157 Loss 0.0018 Accuracy 0.3465\n",
      "Time taken for 1 epoch: 7.969269752502441 secs\n",
      "\n",
      "Epoch 158 Batch 0 Loss 0.0002 Accuracy 0.3719\n",
      "Epoch 158 Batch 15 Loss 0.0018 Accuracy 0.3593\n",
      "Epoch 158 Batch 30 Loss 0.0018 Accuracy 0.3519\n",
      "Epoch 158 Loss 0.0021 Accuracy 0.3539\n",
      "Time taken for 1 epoch: 7.755021572113037 secs\n",
      "\n",
      "Epoch 159 Batch 0 Loss 0.0007 Accuracy 0.3772\n",
      "Epoch 159 Batch 15 Loss 0.0017 Accuracy 0.3567\n",
      "Epoch 159 Batch 30 Loss 0.0018 Accuracy 0.3487\n",
      "Epoch 159 Loss 0.0018 Accuracy 0.3559\n",
      "Time taken for 1 epoch: 7.791802883148193 secs\n",
      "\n",
      "Epoch 160 Batch 0 Loss 0.0010 Accuracy 0.3241\n",
      "Epoch 160 Batch 15 Loss 0.0026 Accuracy 0.3565\n",
      "Epoch 160 Batch 30 Loss 0.0024 Accuracy 0.3530\n",
      "Epoch 160 Loss 0.0025 Accuracy 0.3522\n",
      "Time taken for 1 epoch: 7.781581401824951 secs\n",
      "\n",
      "Epoch 161 Batch 0 Loss 0.0027 Accuracy 0.3497\n",
      "Epoch 161 Batch 15 Loss 0.0019 Accuracy 0.3496\n",
      "Epoch 161 Batch 30 Loss 0.0018 Accuracy 0.3528\n",
      "Epoch 161 Loss 0.0018 Accuracy 0.3548\n",
      "Time taken for 1 epoch: 7.997744083404541 secs\n",
      "\n",
      "Epoch 162 Batch 0 Loss 0.0001 Accuracy 0.3141\n",
      "Epoch 162 Batch 15 Loss 0.0012 Accuracy 0.3404\n",
      "Epoch 162 Batch 30 Loss 0.0012 Accuracy 0.3467\n",
      "Epoch 162 Loss 0.0012 Accuracy 0.3474\n",
      "Time taken for 1 epoch: 7.983893632888794 secs\n",
      "\n",
      "Epoch 163 Batch 0 Loss 0.0002 Accuracy 0.4057\n",
      "Epoch 163 Batch 15 Loss 0.0014 Accuracy 0.3649\n",
      "Epoch 163 Batch 30 Loss 0.0016 Accuracy 0.3490\n",
      "Epoch 163 Loss 0.0017 Accuracy 0.3522\n",
      "Time taken for 1 epoch: 7.810122966766357 secs\n",
      "\n",
      "Epoch 164 Batch 0 Loss 0.0002 Accuracy 0.4073\n",
      "Epoch 164 Batch 15 Loss 0.0022 Accuracy 0.3537\n",
      "Epoch 164 Batch 30 Loss 0.0024 Accuracy 0.3582\n",
      "Epoch 164 Loss 0.0024 Accuracy 0.3581\n",
      "Time taken for 1 epoch: 7.741230249404907 secs\n",
      "\n",
      "Epoch 165 Batch 0 Loss 0.0050 Accuracy 0.3560\n",
      "Epoch 165 Batch 15 Loss 0.0023 Accuracy 0.3556\n",
      "Epoch 165 Batch 30 Loss 0.0021 Accuracy 0.3563\n",
      "Epoch 165 Loss 0.0019 Accuracy 0.3581\n",
      "Time taken for 1 epoch: 7.619927167892456 secs\n",
      "\n",
      "Epoch 166 Batch 0 Loss 0.0007 Accuracy 0.3025\n",
      "Epoch 166 Batch 15 Loss 0.0017 Accuracy 0.3623\n",
      "Epoch 166 Batch 30 Loss 0.0018 Accuracy 0.3527\n",
      "Epoch 166 Loss 0.0015 Accuracy 0.3591\n",
      "Time taken for 1 epoch: 7.665465593338013 secs\n",
      "\n",
      "Epoch 167 Batch 0 Loss 0.0009 Accuracy 0.3949\n",
      "Epoch 167 Batch 15 Loss 0.0012 Accuracy 0.3618\n",
      "Epoch 167 Batch 30 Loss 0.0015 Accuracy 0.3627\n",
      "Epoch 167 Loss 0.0015 Accuracy 0.3584\n",
      "Time taken for 1 epoch: 7.688578844070435 secs\n",
      "\n",
      "Epoch 168 Batch 0 Loss 0.0006 Accuracy 0.3679\n",
      "Epoch 168 Batch 15 Loss 0.0012 Accuracy 0.3519\n",
      "Epoch 168 Batch 30 Loss 0.0017 Accuracy 0.3540\n",
      "Epoch 168 Loss 0.0014 Accuracy 0.3573\n",
      "Time taken for 1 epoch: 7.73825478553772 secs\n",
      "\n",
      "Epoch 169 Batch 0 Loss 0.0009 Accuracy 0.3911\n",
      "Epoch 169 Batch 15 Loss 0.0017 Accuracy 0.3488\n",
      "Epoch 169 Batch 30 Loss 0.0013 Accuracy 0.3502\n",
      "Epoch 169 Loss 0.0013 Accuracy 0.3549\n",
      "Time taken for 1 epoch: 7.73266863822937 secs\n",
      "\n",
      "Epoch 170 Batch 0 Loss 0.0013 Accuracy 0.3030\n",
      "Epoch 170 Batch 15 Loss 0.0018 Accuracy 0.3474\n",
      "Epoch 170 Batch 30 Loss 0.0017 Accuracy 0.3529\n",
      "Epoch 170 Loss 0.0014 Accuracy 0.3509\n",
      "Time taken for 1 epoch: 7.8878843784332275 secs\n",
      "\n",
      "Epoch 171 Batch 0 Loss 0.0021 Accuracy 0.2931\n",
      "Epoch 171 Batch 15 Loss 0.0009 Accuracy 0.3451\n",
      "Epoch 171 Batch 30 Loss 0.0013 Accuracy 0.3466\n",
      "Epoch 171 Loss 0.0013 Accuracy 0.3505\n",
      "Time taken for 1 epoch: 7.883315324783325 secs\n",
      "\n",
      "Epoch 172 Batch 0 Loss 0.0002 Accuracy 0.3198\n",
      "Epoch 172 Batch 15 Loss 0.0016 Accuracy 0.3528\n",
      "Epoch 172 Batch 30 Loss 0.0012 Accuracy 0.3517\n",
      "Epoch 172 Loss 0.0013 Accuracy 0.3542\n",
      "Time taken for 1 epoch: 7.650449514389038 secs\n",
      "\n",
      "Epoch 173 Batch 0 Loss 0.0029 Accuracy 0.3256\n",
      "Epoch 173 Batch 15 Loss 0.0013 Accuracy 0.3528\n",
      "Epoch 173 Batch 30 Loss 0.0016 Accuracy 0.3485\n",
      "Epoch 173 Loss 0.0017 Accuracy 0.3506\n",
      "Time taken for 1 epoch: 7.841222286224365 secs\n",
      "\n",
      "Epoch 174 Batch 0 Loss 0.0002 Accuracy 0.3238\n",
      "Epoch 174 Batch 15 Loss 0.0014 Accuracy 0.3412\n",
      "Epoch 174 Batch 30 Loss 0.0015 Accuracy 0.3511\n",
      "Epoch 174 Loss 0.0015 Accuracy 0.3507\n",
      "Time taken for 1 epoch: 7.918834209442139 secs\n",
      "\n",
      "Epoch 175 Batch 0 Loss 0.0033 Accuracy 0.3818\n",
      "Epoch 175 Batch 15 Loss 0.0021 Accuracy 0.3573\n",
      "Epoch 175 Batch 30 Loss 0.0021 Accuracy 0.3550\n",
      "Epoch 175 Loss 0.0020 Accuracy 0.3553\n",
      "Time taken for 1 epoch: 7.762103796005249 secs\n",
      "\n",
      "Epoch 176 Batch 0 Loss 0.0017 Accuracy 0.4400\n",
      "Epoch 176 Batch 15 Loss 0.0021 Accuracy 0.3472\n",
      "Epoch 176 Batch 30 Loss 0.0018 Accuracy 0.3516\n",
      "Epoch 176 Loss 0.0016 Accuracy 0.3512\n",
      "Time taken for 1 epoch: 7.829903841018677 secs\n",
      "\n",
      "Epoch 177 Batch 0 Loss 0.0115 Accuracy 0.4122\n",
      "Epoch 177 Batch 15 Loss 0.0025 Accuracy 0.3446\n",
      "Epoch 177 Batch 30 Loss 0.0017 Accuracy 0.3427\n",
      "Epoch 177 Loss 0.0015 Accuracy 0.3499\n",
      "Time taken for 1 epoch: 7.794113636016846 secs\n",
      "\n",
      "Epoch 178 Batch 0 Loss 0.0005 Accuracy 0.3238\n",
      "Epoch 178 Batch 15 Loss 0.0008 Accuracy 0.3325\n",
      "Epoch 178 Batch 30 Loss 0.0013 Accuracy 0.3479\n",
      "Epoch 178 Loss 0.0012 Accuracy 0.3493\n",
      "Time taken for 1 epoch: 7.99206805229187 secs\n",
      "\n",
      "Epoch 179 Batch 0 Loss 0.0042 Accuracy 0.2905\n",
      "Epoch 179 Batch 15 Loss 0.0028 Accuracy 0.3480\n",
      "Epoch 179 Batch 30 Loss 0.0019 Accuracy 0.3565\n",
      "Epoch 179 Loss 0.0017 Accuracy 0.3575\n",
      "Time taken for 1 epoch: 7.639747619628906 secs\n",
      "\n",
      "Epoch 180 Batch 0 Loss 0.0004 Accuracy 0.3643\n",
      "Epoch 180 Batch 15 Loss 0.0020 Accuracy 0.3423\n",
      "Epoch 180 Batch 30 Loss 0.0017 Accuracy 0.3480\n",
      "Epoch 180 Loss 0.0019 Accuracy 0.3553\n",
      "Time taken for 1 epoch: 7.644175052642822 secs\n",
      "\n",
      "Epoch 181 Batch 0 Loss 0.0013 Accuracy 0.3291\n",
      "Epoch 181 Batch 15 Loss 0.0018 Accuracy 0.3565\n",
      "Epoch 181 Batch 30 Loss 0.0020 Accuracy 0.3572\n",
      "Epoch 181 Loss 0.0022 Accuracy 0.3541\n",
      "Time taken for 1 epoch: 7.729822158813477 secs\n",
      "\n",
      "Epoch 182 Batch 0 Loss 0.0100 Accuracy 0.3776\n",
      "Epoch 182 Batch 15 Loss 0.0024 Accuracy 0.3465\n",
      "Epoch 182 Batch 30 Loss 0.0023 Accuracy 0.3446\n",
      "Epoch 182 Loss 0.0021 Accuracy 0.3486\n",
      "Time taken for 1 epoch: 7.948438405990601 secs\n",
      "\n",
      "Epoch 183 Batch 0 Loss 0.0003 Accuracy 0.3831\n",
      "Epoch 183 Batch 15 Loss 0.0020 Accuracy 0.3504\n",
      "Epoch 183 Batch 30 Loss 0.0021 Accuracy 0.3495\n",
      "Epoch 183 Loss 0.0019 Accuracy 0.3498\n",
      "Time taken for 1 epoch: 7.790397644042969 secs\n",
      "\n",
      "Epoch 184 Batch 0 Loss 0.0001 Accuracy 0.3066\n",
      "Epoch 184 Batch 15 Loss 0.0010 Accuracy 0.3400\n",
      "Epoch 184 Batch 30 Loss 0.0020 Accuracy 0.3562\n",
      "Epoch 184 Loss 0.0020 Accuracy 0.3555\n",
      "Time taken for 1 epoch: 7.710752248764038 secs\n",
      "\n",
      "Epoch 185 Batch 0 Loss 0.0081 Accuracy 0.3627\n",
      "Epoch 185 Batch 15 Loss 0.0022 Accuracy 0.3639\n",
      "Epoch 185 Batch 30 Loss 0.0022 Accuracy 0.3587\n",
      "Epoch 185 Loss 0.0019 Accuracy 0.3593\n",
      "Time taken for 1 epoch: 7.6775617599487305 secs\n",
      "\n",
      "Epoch 186 Batch 0 Loss 0.0002 Accuracy 0.3984\n",
      "Epoch 186 Batch 15 Loss 0.0012 Accuracy 0.3493\n",
      "Epoch 186 Batch 30 Loss 0.0014 Accuracy 0.3548\n",
      "Epoch 186 Loss 0.0013 Accuracy 0.3535\n",
      "Time taken for 1 epoch: 7.7450783252716064 secs\n",
      "\n",
      "Epoch 187 Batch 0 Loss 0.0014 Accuracy 0.3257\n",
      "Epoch 187 Batch 15 Loss 0.0017 Accuracy 0.3580\n",
      "Epoch 187 Batch 30 Loss 0.0018 Accuracy 0.3514\n",
      "Epoch 187 Loss 0.0017 Accuracy 0.3513\n",
      "Time taken for 1 epoch: 7.728200197219849 secs\n",
      "\n",
      "Epoch 188 Batch 0 Loss 0.0003 Accuracy 0.3223\n",
      "Epoch 188 Batch 15 Loss 0.0021 Accuracy 0.3629\n",
      "Epoch 188 Batch 30 Loss 0.0017 Accuracy 0.3558\n",
      "Epoch 188 Loss 0.0017 Accuracy 0.3569\n",
      "Time taken for 1 epoch: 7.755696773529053 secs\n",
      "\n",
      "Epoch 189 Batch 0 Loss 0.0002 Accuracy 0.4093\n",
      "Epoch 189 Batch 15 Loss 0.0025 Accuracy 0.3424\n",
      "Epoch 189 Batch 30 Loss 0.0018 Accuracy 0.3506\n",
      "Epoch 189 Loss 0.0017 Accuracy 0.3536\n",
      "Time taken for 1 epoch: 7.831991910934448 secs\n",
      "\n",
      "Epoch 190 Batch 0 Loss 0.0002 Accuracy 0.3201\n",
      "Epoch 190 Batch 15 Loss 0.0008 Accuracy 0.3423\n",
      "Epoch 190 Batch 30 Loss 0.0014 Accuracy 0.3504\n",
      "Epoch 190 Loss 0.0015 Accuracy 0.3504\n",
      "Time taken for 1 epoch: 8.017712593078613 secs\n",
      "\n",
      "Epoch 191 Batch 0 Loss 0.0002 Accuracy 0.3173\n",
      "Epoch 191 Batch 15 Loss 0.0014 Accuracy 0.3431\n",
      "Epoch 191 Batch 30 Loss 0.0016 Accuracy 0.3521\n",
      "Epoch 191 Loss 0.0016 Accuracy 0.3560\n",
      "Time taken for 1 epoch: 7.702581882476807 secs\n",
      "\n",
      "Epoch 192 Batch 0 Loss 0.0015 Accuracy 0.3257\n",
      "Epoch 192 Batch 15 Loss 0.0014 Accuracy 0.3500\n",
      "Epoch 192 Batch 30 Loss 0.0014 Accuracy 0.3578\n",
      "Epoch 192 Loss 0.0013 Accuracy 0.3537\n",
      "Time taken for 1 epoch: 7.789680242538452 secs\n",
      "\n",
      "Epoch 193 Batch 0 Loss 0.0001 Accuracy 0.3037\n",
      "Epoch 193 Batch 15 Loss 0.0014 Accuracy 0.3639\n",
      "Epoch 193 Batch 30 Loss 0.0014 Accuracy 0.3535\n",
      "Epoch 193 Loss 0.0018 Accuracy 0.3513\n",
      "Time taken for 1 epoch: 7.827775239944458 secs\n",
      "\n",
      "Epoch 194 Batch 0 Loss 0.0012 Accuracy 0.3076\n",
      "Epoch 194 Batch 15 Loss 0.0010 Accuracy 0.3441\n",
      "Epoch 194 Batch 30 Loss 0.0011 Accuracy 0.3419\n",
      "Epoch 194 Loss 0.0012 Accuracy 0.3459\n",
      "Time taken for 1 epoch: 7.9622297286987305 secs\n",
      "\n",
      "Epoch 195 Batch 0 Loss 0.0049 Accuracy 0.3191\n",
      "Epoch 195 Batch 15 Loss 0.0019 Accuracy 0.3415\n",
      "Epoch 195 Batch 30 Loss 0.0015 Accuracy 0.3452\n",
      "Epoch 195 Loss 0.0015 Accuracy 0.3497\n",
      "Time taken for 1 epoch: 7.852146863937378 secs\n",
      "\n",
      "Epoch 196 Batch 0 Loss 0.0017 Accuracy 0.3861\n",
      "Epoch 196 Batch 15 Loss 0.0018 Accuracy 0.3489\n",
      "Epoch 196 Batch 30 Loss 0.0014 Accuracy 0.3588\n",
      "Epoch 196 Loss 0.0015 Accuracy 0.3579\n",
      "Time taken for 1 epoch: 7.6140711307525635 secs\n",
      "\n",
      "Epoch 197 Batch 0 Loss 0.0008 Accuracy 0.3357\n",
      "Epoch 197 Batch 15 Loss 0.0007 Accuracy 0.3451\n",
      "Epoch 197 Batch 30 Loss 0.0009 Accuracy 0.3438\n",
      "Epoch 197 Loss 0.0010 Accuracy 0.3476\n",
      "Time taken for 1 epoch: 7.97939920425415 secs\n",
      "\n",
      "Epoch 198 Batch 0 Loss 0.0007 Accuracy 0.3108\n",
      "Epoch 198 Batch 15 Loss 0.0018 Accuracy 0.3567\n",
      "Epoch 198 Batch 30 Loss 0.0014 Accuracy 0.3520\n",
      "Epoch 198 Loss 0.0016 Accuracy 0.3543\n",
      "Time taken for 1 epoch: 7.726284980773926 secs\n",
      "\n",
      "Epoch 199 Batch 0 Loss 0.0001 Accuracy 0.2752\n",
      "Epoch 199 Batch 15 Loss 0.0023 Accuracy 0.3571\n",
      "Epoch 199 Batch 30 Loss 0.0019 Accuracy 0.3540\n",
      "Epoch 199 Loss 0.0016 Accuracy 0.3552\n",
      "Time taken for 1 epoch: 7.718096733093262 secs\n",
      "\n",
      "Epoch 200 Batch 0 Loss 0.0003 Accuracy 0.2668\n",
      "Epoch 200 Batch 15 Loss 0.0017 Accuracy 0.3453\n",
      "Epoch 200 Batch 30 Loss 0.0013 Accuracy 0.3585\n",
      "Epoch 200 Loss 0.0013 Accuracy 0.3551\n",
      "Time taken for 1 epoch: 7.736891984939575 secs\n",
      "\n",
      "Epoch 201 Batch 0 Loss 0.0001 Accuracy 0.3303\n",
      "Epoch 201 Batch 15 Loss 0.0016 Accuracy 0.3533\n",
      "Epoch 201 Batch 30 Loss 0.0016 Accuracy 0.3525\n",
      "Epoch 201 Loss 0.0015 Accuracy 0.3493\n",
      "Time taken for 1 epoch: 7.921983957290649 secs\n",
      "\n",
      "Epoch 202 Batch 0 Loss 0.0017 Accuracy 0.3222\n",
      "Epoch 202 Batch 15 Loss 0.0020 Accuracy 0.3607\n",
      "Epoch 202 Batch 30 Loss 0.0020 Accuracy 0.3537\n",
      "Epoch 202 Loss 0.0016 Accuracy 0.3548\n",
      "Time taken for 1 epoch: 7.635670900344849 secs\n",
      "\n",
      "Epoch 203 Batch 0 Loss 0.0006 Accuracy 0.3714\n",
      "Epoch 203 Batch 15 Loss 0.0013 Accuracy 0.3518\n",
      "Epoch 203 Batch 30 Loss 0.0010 Accuracy 0.3524\n",
      "Epoch 203 Loss 0.0010 Accuracy 0.3505\n",
      "Time taken for 1 epoch: 7.884632110595703 secs\n",
      "\n",
      "Epoch 204 Batch 0 Loss 0.0001 Accuracy 0.4129\n",
      "Epoch 204 Batch 15 Loss 0.0004 Accuracy 0.3561\n",
      "Epoch 204 Batch 30 Loss 0.0013 Accuracy 0.3574\n",
      "Epoch 204 Loss 0.0011 Accuracy 0.3520\n",
      "Time taken for 1 epoch: 7.738087892532349 secs\n",
      "\n",
      "Epoch 205 Batch 0 Loss 0.0007 Accuracy 0.3438\n",
      "Epoch 205 Batch 15 Loss 0.0009 Accuracy 0.3620\n",
      "Epoch 205 Batch 30 Loss 0.0018 Accuracy 0.3547\n",
      "Epoch 205 Loss 0.0017 Accuracy 0.3548\n",
      "Time taken for 1 epoch: 7.717497110366821 secs\n",
      "\n",
      "Epoch 206 Batch 0 Loss 0.0014 Accuracy 0.3690\n",
      "Epoch 206 Batch 15 Loss 0.0010 Accuracy 0.3471\n",
      "Epoch 206 Batch 30 Loss 0.0014 Accuracy 0.3486\n",
      "Epoch 206 Loss 0.0014 Accuracy 0.3482\n",
      "Time taken for 1 epoch: 7.918238639831543 secs\n",
      "\n",
      "Epoch 207 Batch 0 Loss 0.0002 Accuracy 0.3308\n",
      "Epoch 207 Batch 15 Loss 0.0004 Accuracy 0.3538\n",
      "Epoch 207 Batch 30 Loss 0.0007 Accuracy 0.3560\n",
      "Epoch 207 Loss 0.0007 Accuracy 0.3559\n",
      "Time taken for 1 epoch: 7.688255310058594 secs\n",
      "\n",
      "Epoch 208 Batch 0 Loss 0.0018 Accuracy 0.3051\n",
      "Epoch 208 Batch 15 Loss 0.0012 Accuracy 0.3453\n",
      "Epoch 208 Batch 30 Loss 0.0017 Accuracy 0.3442\n",
      "Epoch 208 Loss 0.0017 Accuracy 0.3482\n",
      "Time taken for 1 epoch: 7.892996072769165 secs\n",
      "\n",
      "Epoch 209 Batch 0 Loss 0.0035 Accuracy 0.3540\n",
      "Epoch 209 Batch 15 Loss 0.0016 Accuracy 0.3574\n",
      "Epoch 209 Batch 30 Loss 0.0014 Accuracy 0.3548\n",
      "Epoch 209 Loss 0.0014 Accuracy 0.3568\n",
      "Time taken for 1 epoch: 7.686874628067017 secs\n",
      "\n",
      "Epoch 210 Batch 0 Loss 0.0028 Accuracy 0.3384\n",
      "Epoch 210 Batch 15 Loss 0.0017 Accuracy 0.3568\n",
      "Epoch 210 Batch 30 Loss 0.0017 Accuracy 0.3537\n",
      "Epoch 210 Loss 0.0016 Accuracy 0.3518\n",
      "Time taken for 1 epoch: 7.852604389190674 secs\n",
      "\n",
      "Epoch 211 Batch 0 Loss 0.0008 Accuracy 0.3273\n",
      "Epoch 211 Batch 15 Loss 0.0023 Accuracy 0.3691\n",
      "Epoch 211 Batch 30 Loss 0.0017 Accuracy 0.3634\n",
      "Epoch 211 Loss 0.0016 Accuracy 0.3575\n",
      "Time taken for 1 epoch: 7.655825614929199 secs\n",
      "\n",
      "Epoch 212 Batch 0 Loss 0.0001 Accuracy 0.3165\n",
      "Epoch 212 Batch 15 Loss 0.0006 Accuracy 0.3434\n",
      "Epoch 212 Batch 30 Loss 0.0009 Accuracy 0.3529\n",
      "Epoch 212 Loss 0.0009 Accuracy 0.3523\n",
      "Time taken for 1 epoch: 7.799224615097046 secs\n",
      "\n",
      "Epoch 213 Batch 0 Loss 0.0002 Accuracy 0.3187\n",
      "Epoch 213 Batch 15 Loss 0.0012 Accuracy 0.3527\n",
      "Epoch 213 Batch 30 Loss 0.0015 Accuracy 0.3547\n",
      "Epoch 213 Loss 0.0015 Accuracy 0.3522\n",
      "Time taken for 1 epoch: 7.756979942321777 secs\n",
      "\n",
      "Epoch 214 Batch 0 Loss 0.0044 Accuracy 0.3735\n",
      "Epoch 214 Batch 15 Loss 0.0015 Accuracy 0.3565\n",
      "Epoch 214 Batch 30 Loss 0.0014 Accuracy 0.3524\n",
      "Epoch 214 Loss 0.0014 Accuracy 0.3503\n",
      "Time taken for 1 epoch: 7.860453128814697 secs\n",
      "\n",
      "Epoch 215 Batch 0 Loss 0.0022 Accuracy 0.3333\n",
      "Epoch 215 Batch 15 Loss 0.0009 Accuracy 0.3543\n",
      "Epoch 215 Batch 30 Loss 0.0011 Accuracy 0.3535\n",
      "Epoch 215 Loss 0.0013 Accuracy 0.3528\n",
      "Time taken for 1 epoch: 7.826428651809692 secs\n",
      "\n",
      "Epoch 216 Batch 0 Loss 0.0053 Accuracy 0.3603\n",
      "Epoch 216 Batch 15 Loss 0.0015 Accuracy 0.3602\n",
      "Epoch 216 Batch 30 Loss 0.0017 Accuracy 0.3616\n",
      "Epoch 216 Loss 0.0016 Accuracy 0.3571\n",
      "Time taken for 1 epoch: 7.638083457946777 secs\n",
      "\n",
      "Epoch 217 Batch 0 Loss 0.0008 Accuracy 0.3366\n",
      "Epoch 217 Batch 15 Loss 0.0013 Accuracy 0.3466\n",
      "Epoch 217 Batch 30 Loss 0.0011 Accuracy 0.3497\n",
      "Epoch 217 Loss 0.0011 Accuracy 0.3502\n",
      "Time taken for 1 epoch: 7.963501214981079 secs\n",
      "\n",
      "Epoch 218 Batch 0 Loss 0.0007 Accuracy 0.3459\n",
      "Epoch 218 Batch 15 Loss 0.0019 Accuracy 0.3675\n",
      "Epoch 218 Batch 30 Loss 0.0018 Accuracy 0.3499\n",
      "Epoch 218 Loss 0.0016 Accuracy 0.3484\n",
      "Time taken for 1 epoch: 7.887406826019287 secs\n",
      "\n",
      "Epoch 219 Batch 0 Loss 0.0003 Accuracy 0.3897\n",
      "Epoch 219 Batch 15 Loss 0.0013 Accuracy 0.3596\n",
      "Epoch 219 Batch 30 Loss 0.0014 Accuracy 0.3556\n",
      "Epoch 219 Loss 0.0013 Accuracy 0.3580\n",
      "Time taken for 1 epoch: 7.722600936889648 secs\n",
      "\n",
      "Epoch 220 Batch 0 Loss 0.0011 Accuracy 0.4057\n",
      "Epoch 220 Batch 15 Loss 0.0014 Accuracy 0.3672\n",
      "Epoch 220 Batch 30 Loss 0.0016 Accuracy 0.3631\n",
      "Epoch 220 Loss 0.0014 Accuracy 0.3589\n",
      "Time taken for 1 epoch: 7.579009532928467 secs\n",
      "\n",
      "Epoch 221 Batch 0 Loss 0.0020 Accuracy 0.4328\n",
      "Epoch 221 Batch 15 Loss 0.0021 Accuracy 0.3557\n",
      "Epoch 221 Batch 30 Loss 0.0016 Accuracy 0.3570\n",
      "Epoch 221 Loss 0.0013 Accuracy 0.3524\n",
      "Time taken for 1 epoch: 7.771448135375977 secs\n",
      "\n",
      "Epoch 222 Batch 0 Loss 0.0037 Accuracy 0.3267\n",
      "Epoch 222 Batch 15 Loss 0.0018 Accuracy 0.3473\n",
      "Epoch 222 Batch 30 Loss 0.0014 Accuracy 0.3509\n",
      "Epoch 222 Loss 0.0012 Accuracy 0.3522\n",
      "Time taken for 1 epoch: 7.7749199867248535 secs\n",
      "\n",
      "Epoch 223 Batch 0 Loss 0.0007 Accuracy 0.3818\n",
      "Epoch 223 Batch 15 Loss 0.0017 Accuracy 0.3555\n",
      "Epoch 223 Batch 30 Loss 0.0017 Accuracy 0.3602\n",
      "Epoch 223 Loss 0.0014 Accuracy 0.3568\n",
      "Time taken for 1 epoch: 7.598037004470825 secs\n",
      "\n",
      "Epoch 224 Batch 0 Loss 0.0001 Accuracy 0.3298\n",
      "Epoch 224 Batch 15 Loss 0.0012 Accuracy 0.3618\n",
      "Epoch 224 Batch 30 Loss 0.0011 Accuracy 0.3625\n",
      "Epoch 224 Loss 0.0025 Accuracy 0.3587\n",
      "Time taken for 1 epoch: 7.569957733154297 secs\n",
      "\n",
      "Epoch 225 Batch 0 Loss 0.0026 Accuracy 0.3309\n",
      "Epoch 225 Batch 15 Loss 0.0028 Accuracy 0.3444\n",
      "Epoch 225 Batch 30 Loss 0.0028 Accuracy 0.3451\n",
      "Epoch 225 Loss 0.0024 Accuracy 0.3516\n",
      "Time taken for 1 epoch: 7.845397710800171 secs\n",
      "\n",
      "Epoch 226 Batch 0 Loss 0.0002 Accuracy 0.3252\n",
      "Epoch 226 Batch 15 Loss 0.0013 Accuracy 0.3539\n",
      "Epoch 226 Batch 30 Loss 0.0014 Accuracy 0.3606\n",
      "Epoch 226 Loss 0.0013 Accuracy 0.3535\n",
      "Time taken for 1 epoch: 7.821759223937988 secs\n",
      "\n",
      "Epoch 227 Batch 0 Loss 0.0001 Accuracy 0.3429\n",
      "Epoch 227 Batch 15 Loss 0.0017 Accuracy 0.3435\n",
      "Epoch 227 Batch 30 Loss 0.0013 Accuracy 0.3463\n",
      "Epoch 227 Loss 0.0011 Accuracy 0.3513\n",
      "Time taken for 1 epoch: 7.8103320598602295 secs\n",
      "\n",
      "Epoch 228 Batch 0 Loss 0.0002 Accuracy 0.3527\n",
      "Epoch 228 Batch 15 Loss 0.0013 Accuracy 0.3650\n",
      "Epoch 228 Batch 30 Loss 0.0011 Accuracy 0.3632\n",
      "Epoch 228 Loss 0.0010 Accuracy 0.3573\n",
      "Time taken for 1 epoch: 7.6798553466796875 secs\n",
      "\n",
      "Epoch 229 Batch 0 Loss 0.0002 Accuracy 0.3917\n",
      "Epoch 229 Batch 15 Loss 0.0006 Accuracy 0.3571\n",
      "Epoch 229 Batch 30 Loss 0.0007 Accuracy 0.3516\n",
      "Epoch 229 Loss 0.0007 Accuracy 0.3531\n",
      "Time taken for 1 epoch: 7.8115386962890625 secs\n",
      "\n",
      "Epoch 230 Batch 0 Loss 0.0001 Accuracy 0.3193\n",
      "Epoch 230 Batch 15 Loss 0.0010 Accuracy 0.3545\n",
      "Epoch 230 Batch 30 Loss 0.0011 Accuracy 0.3507\n",
      "Epoch 230 Loss 0.0037 Accuracy 0.3530\n",
      "Time taken for 1 epoch: 7.903659343719482 secs\n",
      "\n",
      "Epoch 231 Batch 0 Loss 0.0002 Accuracy 0.3960\n",
      "Epoch 231 Batch 15 Loss 0.0019 Accuracy 0.3620\n",
      "Epoch 231 Batch 30 Loss 0.0021 Accuracy 0.3517\n",
      "Epoch 231 Loss 0.0022 Accuracy 0.3542\n",
      "Time taken for 1 epoch: 7.752256155014038 secs\n",
      "\n",
      "Epoch 232 Batch 0 Loss 0.0002 Accuracy 0.4010\n",
      "Epoch 232 Batch 15 Loss 0.0013 Accuracy 0.3415\n",
      "Epoch 232 Batch 30 Loss 0.0015 Accuracy 0.3532\n",
      "Epoch 232 Loss 0.0014 Accuracy 0.3502\n",
      "Time taken for 1 epoch: 7.892385482788086 secs\n",
      "\n",
      "Epoch 233 Batch 0 Loss 0.0002 Accuracy 0.3740\n",
      "Epoch 233 Batch 15 Loss 0.0009 Accuracy 0.3736\n",
      "Epoch 233 Batch 30 Loss 0.0007 Accuracy 0.3588\n",
      "Epoch 233 Loss 0.0008 Accuracy 0.3560\n",
      "Time taken for 1 epoch: 7.727566719055176 secs\n",
      "\n",
      "Epoch 234 Batch 0 Loss 0.0001 Accuracy 0.2833\n",
      "Epoch 234 Batch 15 Loss 0.0012 Accuracy 0.3480\n",
      "Epoch 234 Batch 30 Loss 0.0009 Accuracy 0.3551\n",
      "Epoch 234 Loss 0.0009 Accuracy 0.3598\n",
      "Time taken for 1 epoch: 7.701815843582153 secs\n",
      "\n",
      "Epoch 235 Batch 0 Loss 0.0007 Accuracy 0.3199\n",
      "Epoch 235 Batch 15 Loss 0.0011 Accuracy 0.3531\n",
      "Epoch 235 Batch 30 Loss 0.0012 Accuracy 0.3504\n",
      "Epoch 235 Loss 0.0012 Accuracy 0.3533\n",
      "Time taken for 1 epoch: 7.7192018032073975 secs\n",
      "\n",
      "Epoch 236 Batch 0 Loss 0.0005 Accuracy 0.3770\n",
      "Epoch 236 Batch 15 Loss 0.0014 Accuracy 0.3708\n",
      "Epoch 236 Batch 30 Loss 0.0014 Accuracy 0.3641\n",
      "Epoch 236 Loss 0.0017 Accuracy 0.3610\n",
      "Time taken for 1 epoch: 7.587347507476807 secs\n",
      "\n",
      "Epoch 237 Batch 0 Loss 0.0044 Accuracy 0.3774\n",
      "Epoch 237 Batch 15 Loss 0.0026 Accuracy 0.3623\n",
      "Epoch 237 Batch 30 Loss 0.0021 Accuracy 0.3563\n",
      "Epoch 237 Loss 0.0019 Accuracy 0.3558\n",
      "Time taken for 1 epoch: 7.665776491165161 secs\n",
      "\n",
      "Epoch 238 Batch 0 Loss 0.0001 Accuracy 0.3723\n",
      "Epoch 238 Batch 15 Loss 0.0013 Accuracy 0.3506\n",
      "Epoch 238 Batch 30 Loss 0.0010 Accuracy 0.3604\n",
      "Epoch 238 Loss 0.0011 Accuracy 0.3597\n",
      "Time taken for 1 epoch: 7.611907958984375 secs\n",
      "\n",
      "Epoch 239 Batch 0 Loss 0.0001 Accuracy 0.3621\n",
      "Epoch 239 Batch 15 Loss 0.0008 Accuracy 0.3489\n",
      "Epoch 239 Batch 30 Loss 0.0009 Accuracy 0.3515\n",
      "Epoch 239 Loss 0.0011 Accuracy 0.3495\n",
      "Time taken for 1 epoch: 7.768726110458374 secs\n",
      "\n",
      "Epoch 240 Batch 0 Loss 0.0002 Accuracy 0.3258\n",
      "Epoch 240 Batch 15 Loss 0.0010 Accuracy 0.3437\n",
      "Epoch 240 Batch 30 Loss 0.0009 Accuracy 0.3469\n",
      "Epoch 240 Loss 0.0008 Accuracy 0.3500\n",
      "Time taken for 1 epoch: 7.874954462051392 secs\n",
      "\n",
      "Epoch 241 Batch 0 Loss 0.0008 Accuracy 0.3272\n",
      "Epoch 241 Batch 15 Loss 0.0016 Accuracy 0.3543\n",
      "Epoch 241 Batch 30 Loss 0.0013 Accuracy 0.3515\n",
      "Epoch 241 Loss 0.0014 Accuracy 0.3513\n",
      "Time taken for 1 epoch: 7.931082725524902 secs\n",
      "\n",
      "Epoch 242 Batch 0 Loss 0.0015 Accuracy 0.3336\n",
      "Epoch 242 Batch 15 Loss 0.0007 Accuracy 0.3503\n",
      "Epoch 242 Batch 30 Loss 0.0007 Accuracy 0.3551\n",
      "Epoch 242 Loss 0.0009 Accuracy 0.3560\n",
      "Time taken for 1 epoch: 7.682606220245361 secs\n",
      "\n",
      "Epoch 243 Batch 0 Loss 0.0001 Accuracy 0.4255\n",
      "Epoch 243 Batch 15 Loss 0.0014 Accuracy 0.3471\n",
      "Epoch 243 Batch 30 Loss 0.0015 Accuracy 0.3481\n",
      "Epoch 243 Loss 0.0015 Accuracy 0.3535\n",
      "Time taken for 1 epoch: 7.804425001144409 secs\n",
      "\n",
      "Epoch 244 Batch 0 Loss 0.0023 Accuracy 0.4425\n",
      "Epoch 244 Batch 15 Loss 0.0008 Accuracy 0.3623\n",
      "Epoch 244 Batch 30 Loss 0.0009 Accuracy 0.3649\n",
      "Saving checkpoint for epoch 244 at ./checkpoints/train/ckpt-28\n",
      "Epoch 244 Loss 0.0014 Accuracy 0.3638\n",
      "Time taken for 1 epoch: 8.018755912780762 secs\n",
      "\n",
      "Epoch 245 Batch 0 Loss 0.0056 Accuracy 0.3589\n",
      "Epoch 245 Batch 15 Loss 0.0010 Accuracy 0.3558\n",
      "Epoch 245 Batch 30 Loss 0.0012 Accuracy 0.3593\n",
      "Epoch 245 Loss 0.0012 Accuracy 0.3587\n",
      "Time taken for 1 epoch: 7.497153282165527 secs\n",
      "\n",
      "Epoch 246 Batch 0 Loss 0.0001 Accuracy 0.3661\n",
      "Epoch 246 Batch 15 Loss 0.0010 Accuracy 0.3552\n",
      "Epoch 246 Batch 30 Loss 0.0011 Accuracy 0.3542\n",
      "Epoch 246 Loss 0.0010 Accuracy 0.3517\n",
      "Time taken for 1 epoch: 7.733962535858154 secs\n",
      "\n",
      "Epoch 247 Batch 0 Loss 0.0012 Accuracy 0.4182\n",
      "Epoch 247 Batch 15 Loss 0.0008 Accuracy 0.3716\n",
      "Epoch 247 Batch 30 Loss 0.0008 Accuracy 0.3595\n",
      "Epoch 247 Loss 0.0010 Accuracy 0.3611\n",
      "Time taken for 1 epoch: 7.5600266456604 secs\n",
      "\n",
      "Epoch 248 Batch 0 Loss 0.0001 Accuracy 0.3750\n",
      "Epoch 248 Batch 15 Loss 0.0012 Accuracy 0.3593\n",
      "Epoch 248 Batch 30 Loss 0.0017 Accuracy 0.3550\n",
      "Epoch 248 Loss 0.0016 Accuracy 0.3581\n",
      "Time taken for 1 epoch: 7.57215690612793 secs\n",
      "\n",
      "Epoch 249 Batch 0 Loss 0.0003 Accuracy 0.2969\n",
      "Epoch 249 Batch 15 Loss 0.0017 Accuracy 0.3677\n",
      "Epoch 249 Batch 30 Loss 0.0016 Accuracy 0.3604\n",
      "Epoch 249 Loss 0.0016 Accuracy 0.3541\n",
      "Time taken for 1 epoch: 7.733027696609497 secs\n",
      "\n",
      "Epoch 250 Batch 0 Loss 0.0002 Accuracy 0.3486\n",
      "Epoch 250 Batch 15 Loss 0.0007 Accuracy 0.3531\n",
      "Epoch 250 Batch 30 Loss 0.0015 Accuracy 0.3545\n",
      "Epoch 250 Loss 0.0013 Accuracy 0.3564\n",
      "Time taken for 1 epoch: 7.629333734512329 secs\n",
      "\n",
      "Epoch 251 Batch 0 Loss 0.0007 Accuracy 0.4700\n",
      "Epoch 251 Batch 15 Loss 0.0024 Accuracy 0.3678\n",
      "Epoch 251 Batch 30 Loss 0.0016 Accuracy 0.3549\n",
      "Epoch 251 Loss 0.0014 Accuracy 0.3565\n",
      "Time taken for 1 epoch: 7.596967935562134 secs\n",
      "\n",
      "Epoch 252 Batch 0 Loss 0.0007 Accuracy 0.3707\n",
      "Epoch 252 Batch 15 Loss 0.0010 Accuracy 0.3520\n",
      "Epoch 252 Batch 30 Loss 0.0008 Accuracy 0.3520\n",
      "Epoch 252 Loss 0.0011 Accuracy 0.3517\n",
      "Time taken for 1 epoch: 7.827571153640747 secs\n",
      "\n",
      "Epoch 253 Batch 0 Loss 0.0002 Accuracy 0.3650\n",
      "Epoch 253 Batch 15 Loss 0.0009 Accuracy 0.3504\n",
      "Epoch 253 Batch 30 Loss 0.0010 Accuracy 0.3522\n",
      "Epoch 253 Loss 0.0011 Accuracy 0.3503\n",
      "Time taken for 1 epoch: 7.836493253707886 secs\n",
      "\n",
      "Epoch 254 Batch 0 Loss 0.0004 Accuracy 0.4174\n",
      "Epoch 254 Batch 15 Loss 0.0017 Accuracy 0.3536\n",
      "Epoch 254 Batch 30 Loss 0.0015 Accuracy 0.3560\n",
      "Epoch 254 Loss 0.0014 Accuracy 0.3511\n",
      "Time taken for 1 epoch: 7.8498921394348145 secs\n",
      "\n",
      "Epoch 255 Batch 0 Loss 0.0022 Accuracy 0.3083\n",
      "Epoch 255 Batch 15 Loss 0.0014 Accuracy 0.3579\n",
      "Epoch 255 Batch 30 Loss 0.0015 Accuracy 0.3629\n",
      "Epoch 255 Loss 0.0015 Accuracy 0.3616\n",
      "Time taken for 1 epoch: 7.506861686706543 secs\n",
      "\n",
      "Epoch 256 Batch 0 Loss 0.0038 Accuracy 0.3639\n",
      "Epoch 256 Batch 15 Loss 0.0015 Accuracy 0.3447\n",
      "Epoch 256 Batch 30 Loss 0.0017 Accuracy 0.3519\n",
      "Epoch 256 Loss 0.0019 Accuracy 0.3540\n",
      "Time taken for 1 epoch: 7.66750955581665 secs\n",
      "\n",
      "Epoch 257 Batch 0 Loss 0.0065 Accuracy 0.3684\n",
      "Epoch 257 Batch 15 Loss 0.0021 Accuracy 0.3562\n",
      "Epoch 257 Batch 30 Loss 0.0015 Accuracy 0.3572\n",
      "Epoch 257 Loss 0.0018 Accuracy 0.3532\n",
      "Time taken for 1 epoch: 7.790648698806763 secs\n",
      "\n",
      "Epoch 258 Batch 0 Loss 0.0013 Accuracy 0.3373\n",
      "Epoch 258 Batch 15 Loss 0.0009 Accuracy 0.3590\n",
      "Epoch 258 Batch 30 Loss 0.0009 Accuracy 0.3555\n",
      "Epoch 258 Loss 0.0014 Accuracy 0.3522\n",
      "Time taken for 1 epoch: 7.797427654266357 secs\n",
      "\n",
      "Epoch 259 Batch 0 Loss 0.0002 Accuracy 0.4012\n",
      "Epoch 259 Batch 15 Loss 0.0025 Accuracy 0.3652\n",
      "Epoch 259 Batch 30 Loss 0.0025 Accuracy 0.3586\n",
      "Epoch 259 Loss 0.0022 Accuracy 0.3519\n",
      "Time taken for 1 epoch: 7.744412183761597 secs\n",
      "\n",
      "Epoch 260 Batch 0 Loss 0.0001 Accuracy 0.3726\n",
      "Epoch 260 Batch 15 Loss 0.0016 Accuracy 0.3554\n",
      "Epoch 260 Batch 30 Loss 0.0013 Accuracy 0.3581\n",
      "Epoch 260 Loss 0.0011 Accuracy 0.3530\n",
      "Time taken for 1 epoch: 7.732213258743286 secs\n",
      "\n",
      "Epoch 261 Batch 0 Loss 0.0004 Accuracy 0.3388\n",
      "Epoch 261 Batch 15 Loss 0.0006 Accuracy 0.3500\n",
      "Epoch 261 Batch 30 Loss 0.0007 Accuracy 0.3524\n",
      "Epoch 261 Loss 0.0007 Accuracy 0.3561\n",
      "Time taken for 1 epoch: 7.676098108291626 secs\n",
      "\n",
      "Epoch 262 Batch 0 Loss 0.0043 Accuracy 0.3454\n",
      "Epoch 262 Batch 15 Loss 0.0013 Accuracy 0.3523\n",
      "Epoch 262 Batch 30 Loss 0.0011 Accuracy 0.3523\n",
      "Epoch 262 Loss 0.0010 Accuracy 0.3524\n",
      "Time taken for 1 epoch: 7.808758974075317 secs\n",
      "\n",
      "Epoch 263 Batch 0 Loss 0.0001 Accuracy 0.3415\n",
      "Epoch 263 Batch 15 Loss 0.0004 Accuracy 0.3557\n",
      "Epoch 263 Batch 30 Loss 0.0012 Accuracy 0.3580\n",
      "Epoch 263 Loss 0.0012 Accuracy 0.3557\n",
      "Time taken for 1 epoch: 7.750712633132935 secs\n",
      "\n",
      "Epoch 264 Batch 0 Loss 0.0005 Accuracy 0.4079\n",
      "Epoch 264 Batch 15 Loss 0.0016 Accuracy 0.3576\n",
      "Epoch 264 Batch 30 Loss 0.0012 Accuracy 0.3509\n",
      "Epoch 264 Loss 0.0013 Accuracy 0.3483\n",
      "Time taken for 1 epoch: 7.864096403121948 secs\n",
      "\n",
      "Epoch 265 Batch 0 Loss 0.0001 Accuracy 0.3241\n",
      "Epoch 265 Batch 15 Loss 0.0011 Accuracy 0.3373\n",
      "Epoch 265 Batch 30 Loss 0.0016 Accuracy 0.3442\n",
      "Epoch 265 Loss 0.0014 Accuracy 0.3478\n",
      "Time taken for 1 epoch: 7.976112604141235 secs\n",
      "\n",
      "Epoch 266 Batch 0 Loss 0.0037 Accuracy 0.3451\n",
      "Epoch 266 Batch 15 Loss 0.0013 Accuracy 0.3465\n",
      "Epoch 266 Batch 30 Loss 0.0013 Accuracy 0.3480\n",
      "Epoch 266 Loss 0.0011 Accuracy 0.3487\n",
      "Time taken for 1 epoch: 7.8260979652404785 secs\n",
      "\n",
      "Epoch 267 Batch 0 Loss 0.0003 Accuracy 0.3257\n",
      "Epoch 267 Batch 15 Loss 0.0011 Accuracy 0.3445\n",
      "Epoch 267 Batch 30 Loss 0.0017 Accuracy 0.3509\n",
      "Epoch 267 Loss 0.0017 Accuracy 0.3521\n",
      "Time taken for 1 epoch: 7.753955125808716 secs\n",
      "\n",
      "Epoch 268 Batch 0 Loss 0.0049 Accuracy 0.3594\n",
      "Epoch 268 Batch 15 Loss 0.0025 Accuracy 0.3591\n",
      "Epoch 268 Batch 30 Loss 0.0021 Accuracy 0.3548\n",
      "Epoch 268 Loss 0.0018 Accuracy 0.3552\n",
      "Time taken for 1 epoch: 7.764556407928467 secs\n",
      "\n",
      "Epoch 269 Batch 0 Loss 0.0002 Accuracy 0.3239\n",
      "Epoch 269 Batch 15 Loss 0.0007 Accuracy 0.3462\n",
      "Epoch 269 Batch 30 Loss 0.0008 Accuracy 0.3490\n",
      "Epoch 269 Loss 0.0007 Accuracy 0.3527\n",
      "Time taken for 1 epoch: 7.7668373584747314 secs\n",
      "\n",
      "Epoch 270 Batch 0 Loss 0.0003 Accuracy 0.4110\n",
      "Epoch 270 Batch 15 Loss 0.0003 Accuracy 0.3443\n",
      "Epoch 270 Batch 30 Loss 0.0006 Accuracy 0.3607\n",
      "Epoch 270 Loss 0.0007 Accuracy 0.3579\n",
      "Time taken for 1 epoch: 7.544351816177368 secs\n",
      "\n",
      "Epoch 271 Batch 0 Loss 0.0003 Accuracy 0.3628\n",
      "Epoch 271 Batch 15 Loss 0.0007 Accuracy 0.3524\n",
      "Epoch 271 Batch 30 Loss 0.0008 Accuracy 0.3521\n",
      "Epoch 271 Loss 0.0008 Accuracy 0.3510\n",
      "Time taken for 1 epoch: 7.818888425827026 secs\n",
      "\n",
      "Epoch 272 Batch 0 Loss 0.0022 Accuracy 0.2800\n",
      "Epoch 272 Batch 15 Loss 0.0016 Accuracy 0.3470\n",
      "Epoch 272 Batch 30 Loss 0.0018 Accuracy 0.3605\n",
      "Epoch 272 Loss 0.0015 Accuracy 0.3589\n",
      "Time taken for 1 epoch: 7.609466075897217 secs\n",
      "\n",
      "Epoch 273 Batch 0 Loss 0.0004 Accuracy 0.3338\n",
      "Epoch 273 Batch 15 Loss 0.0018 Accuracy 0.3476\n",
      "Epoch 273 Batch 30 Loss 0.0019 Accuracy 0.3559\n",
      "Epoch 273 Loss 0.0019 Accuracy 0.3545\n",
      "Time taken for 1 epoch: 7.734382390975952 secs\n",
      "\n",
      "Epoch 274 Batch 0 Loss 0.0002 Accuracy 0.4247\n",
      "Epoch 274 Batch 15 Loss 0.0015 Accuracy 0.3571\n",
      "Epoch 274 Batch 30 Loss 0.0014 Accuracy 0.3497\n",
      "Epoch 274 Loss 0.0013 Accuracy 0.3507\n",
      "Time taken for 1 epoch: 7.806358814239502 secs\n",
      "\n",
      "Epoch 275 Batch 0 Loss 0.0000 Accuracy 0.3677\n",
      "Epoch 275 Batch 15 Loss 0.0013 Accuracy 0.3498\n",
      "Epoch 275 Batch 30 Loss 0.0013 Accuracy 0.3533\n",
      "Epoch 275 Loss 0.0016 Accuracy 0.3547\n",
      "Time taken for 1 epoch: 7.782702445983887 secs\n",
      "\n",
      "Epoch 276 Batch 0 Loss 0.0012 Accuracy 0.3901\n",
      "Epoch 276 Batch 15 Loss 0.0016 Accuracy 0.3486\n",
      "Epoch 276 Batch 30 Loss 0.0014 Accuracy 0.3573\n",
      "Epoch 276 Loss 0.0014 Accuracy 0.3574\n",
      "Time taken for 1 epoch: 7.615269422531128 secs\n",
      "\n",
      "Epoch 277 Batch 0 Loss 0.0017 Accuracy 0.3776\n",
      "Epoch 277 Batch 15 Loss 0.0011 Accuracy 0.3516\n",
      "Epoch 277 Batch 30 Loss 0.0016 Accuracy 0.3554\n",
      "Epoch 277 Loss 0.0017 Accuracy 0.3574\n",
      "Time taken for 1 epoch: 8.076092958450317 secs\n",
      "\n",
      "Epoch 278 Batch 0 Loss 0.0005 Accuracy 0.3184\n",
      "Epoch 278 Batch 15 Loss 0.0018 Accuracy 0.3494\n",
      "Epoch 278 Batch 30 Loss 0.0018 Accuracy 0.3530\n",
      "Epoch 278 Loss 0.0025 Accuracy 0.3484\n",
      "Time taken for 1 epoch: 7.933168649673462 secs\n",
      "\n",
      "Epoch 279 Batch 0 Loss 0.0001 Accuracy 0.3087\n",
      "Epoch 279 Batch 15 Loss 0.0033 Accuracy 0.3523\n",
      "Epoch 279 Batch 30 Loss 0.0030 Accuracy 0.3591\n",
      "Epoch 279 Loss 0.0027 Accuracy 0.3551\n",
      "Time taken for 1 epoch: 7.7270286083221436 secs\n",
      "\n",
      "Epoch 280 Batch 0 Loss 0.0009 Accuracy 0.3770\n",
      "Epoch 280 Batch 15 Loss 0.0017 Accuracy 0.3643\n",
      "Epoch 280 Batch 30 Loss 0.0015 Accuracy 0.3537\n",
      "Epoch 280 Loss 0.0014 Accuracy 0.3490\n",
      "Time taken for 1 epoch: 8.005918741226196 secs\n",
      "\n",
      "Epoch 281 Batch 0 Loss 0.0001 Accuracy 0.3157\n",
      "Epoch 281 Batch 15 Loss 0.0006 Accuracy 0.3579\n",
      "Epoch 281 Batch 30 Loss 0.0012 Accuracy 0.3508\n",
      "Epoch 281 Loss 0.0012 Accuracy 0.3507\n",
      "Time taken for 1 epoch: 7.8381171226501465 secs\n",
      "\n",
      "Epoch 282 Batch 0 Loss 0.0001 Accuracy 0.3615\n",
      "Epoch 282 Batch 15 Loss 0.0005 Accuracy 0.3488\n",
      "Epoch 282 Batch 30 Loss 0.0006 Accuracy 0.3523\n",
      "Epoch 282 Loss 0.0006 Accuracy 0.3557\n",
      "Time taken for 1 epoch: 7.575350999832153 secs\n",
      "\n",
      "Epoch 283 Batch 0 Loss 0.0000 Accuracy 0.3530\n",
      "Epoch 283 Batch 15 Loss 0.0006 Accuracy 0.3639\n",
      "Epoch 283 Batch 30 Loss 0.0010 Accuracy 0.3555\n",
      "Epoch 283 Loss 0.0009 Accuracy 0.3575\n",
      "Time taken for 1 epoch: 7.617792129516602 secs\n",
      "\n",
      "Epoch 284 Batch 0 Loss 0.0021 Accuracy 0.3740\n",
      "Epoch 284 Batch 15 Loss 0.0009 Accuracy 0.3511\n",
      "Epoch 284 Batch 30 Loss 0.0013 Accuracy 0.3497\n",
      "Epoch 284 Loss 0.0012 Accuracy 0.3508\n",
      "Time taken for 1 epoch: 7.795159816741943 secs\n",
      "\n",
      "Epoch 285 Batch 0 Loss 0.0031 Accuracy 0.4520\n",
      "Epoch 285 Batch 15 Loss 0.0009 Accuracy 0.3532\n",
      "Epoch 285 Batch 30 Loss 0.0009 Accuracy 0.3637\n",
      "Epoch 285 Loss 0.0008 Accuracy 0.3564\n",
      "Time taken for 1 epoch: 7.655780076980591 secs\n",
      "\n",
      "Epoch 286 Batch 0 Loss 0.0079 Accuracy 0.3883\n",
      "Epoch 286 Batch 15 Loss 0.0012 Accuracy 0.3524\n",
      "Epoch 286 Batch 30 Loss 0.0010 Accuracy 0.3522\n",
      "Epoch 286 Loss 0.0012 Accuracy 0.3456\n",
      "Time taken for 1 epoch: 7.9819722175598145 secs\n",
      "\n",
      "Epoch 287 Batch 0 Loss 0.0001 Accuracy 0.3470\n",
      "Epoch 287 Batch 15 Loss 0.0019 Accuracy 0.3453\n",
      "Epoch 287 Batch 30 Loss 0.0014 Accuracy 0.3462\n",
      "Epoch 287 Loss 0.0015 Accuracy 0.3540\n",
      "Time taken for 1 epoch: 7.6971471309661865 secs\n",
      "\n",
      "Epoch 288 Batch 0 Loss 0.0000 Accuracy 0.3049\n",
      "Epoch 288 Batch 15 Loss 0.0018 Accuracy 0.3406\n",
      "Epoch 288 Batch 30 Loss 0.0017 Accuracy 0.3498\n",
      "Epoch 288 Loss 0.0017 Accuracy 0.3504\n",
      "Time taken for 1 epoch: 7.829583644866943 secs\n",
      "\n",
      "Epoch 289 Batch 0 Loss 0.0002 Accuracy 0.3973\n",
      "Epoch 289 Batch 15 Loss 0.0014 Accuracy 0.3469\n",
      "Epoch 289 Batch 30 Loss 0.0015 Accuracy 0.3477\n",
      "Epoch 289 Loss 0.0014 Accuracy 0.3469\n",
      "Time taken for 1 epoch: 8.01497483253479 secs\n",
      "\n",
      "Epoch 290 Batch 0 Loss 0.0015 Accuracy 0.3231\n",
      "Epoch 290 Batch 15 Loss 0.0015 Accuracy 0.3589\n",
      "Epoch 290 Batch 30 Loss 0.0014 Accuracy 0.3583\n",
      "Epoch 290 Loss 0.0013 Accuracy 0.3554\n",
      "Time taken for 1 epoch: 7.6599440574646 secs\n",
      "\n",
      "Epoch 291 Batch 0 Loss 0.0005 Accuracy 0.3258\n",
      "Epoch 291 Batch 15 Loss 0.0009 Accuracy 0.3560\n",
      "Epoch 291 Batch 30 Loss 0.0009 Accuracy 0.3538\n",
      "Epoch 291 Loss 0.0008 Accuracy 0.3554\n",
      "Time taken for 1 epoch: 7.722631216049194 secs\n",
      "\n",
      "Epoch 292 Batch 0 Loss 0.0012 Accuracy 0.3862\n",
      "Epoch 292 Batch 15 Loss 0.0008 Accuracy 0.3645\n",
      "Epoch 292 Batch 30 Loss 0.0012 Accuracy 0.3548\n",
      "Epoch 292 Loss 0.0012 Accuracy 0.3550\n",
      "Time taken for 1 epoch: 7.671661615371704 secs\n",
      "\n",
      "Epoch 293 Batch 0 Loss 0.0001 Accuracy 0.2998\n",
      "Epoch 293 Batch 15 Loss 0.0007 Accuracy 0.3431\n",
      "Epoch 293 Batch 30 Loss 0.0006 Accuracy 0.3497\n",
      "Epoch 293 Loss 0.0006 Accuracy 0.3468\n",
      "Time taken for 1 epoch: 7.986857652664185 secs\n",
      "\n",
      "Epoch 294 Batch 0 Loss 0.0007 Accuracy 0.3641\n",
      "Epoch 294 Batch 15 Loss 0.0011 Accuracy 0.3561\n",
      "Epoch 294 Batch 30 Loss 0.0013 Accuracy 0.3569\n",
      "Epoch 294 Loss 0.0012 Accuracy 0.3541\n",
      "Time taken for 1 epoch: 7.7935426235198975 secs\n",
      "\n",
      "Epoch 295 Batch 0 Loss 0.0002 Accuracy 0.3901\n",
      "Epoch 295 Batch 15 Loss 0.0010 Accuracy 0.3576\n",
      "Epoch 295 Batch 30 Loss 0.0011 Accuracy 0.3571\n",
      "Epoch 295 Loss 0.0010 Accuracy 0.3543\n",
      "Time taken for 1 epoch: 7.773665189743042 secs\n",
      "\n",
      "Epoch 296 Batch 0 Loss 0.0005 Accuracy 0.3109\n",
      "Epoch 296 Batch 15 Loss 0.0007 Accuracy 0.3507\n",
      "Epoch 296 Batch 30 Loss 0.0007 Accuracy 0.3503\n",
      "Epoch 296 Loss 0.0008 Accuracy 0.3493\n",
      "Time taken for 1 epoch: 7.779606342315674 secs\n",
      "\n",
      "Epoch 297 Batch 0 Loss 0.0001 Accuracy 0.3556\n",
      "Epoch 297 Batch 15 Loss 0.0007 Accuracy 0.3532\n",
      "Epoch 297 Batch 30 Loss 0.0006 Accuracy 0.3499\n",
      "Epoch 297 Loss 0.0005 Accuracy 0.3500\n",
      "Time taken for 1 epoch: 7.92364764213562 secs\n",
      "\n",
      "Epoch 298 Batch 0 Loss 0.0012 Accuracy 0.3726\n",
      "Epoch 298 Batch 15 Loss 0.0012 Accuracy 0.3487\n",
      "Epoch 298 Batch 30 Loss 0.0013 Accuracy 0.3486\n",
      "Epoch 298 Loss 0.0026 Accuracy 0.3479\n",
      "Time taken for 1 epoch: 7.924858570098877 secs\n",
      "\n",
      "Epoch 299 Batch 0 Loss 0.0035 Accuracy 0.3744\n",
      "Epoch 299 Batch 15 Loss 0.0013 Accuracy 0.3487\n",
      "Epoch 299 Batch 30 Loss 0.0012 Accuracy 0.3564\n",
      "Epoch 299 Loss 0.0012 Accuracy 0.3507\n",
      "Time taken for 1 epoch: 7.8912060260772705 secs\n",
      "\n",
      "Epoch 300 Batch 0 Loss 0.0001 Accuracy 0.4798\n",
      "Epoch 300 Batch 15 Loss 0.0010 Accuracy 0.3520\n",
      "Epoch 300 Batch 30 Loss 0.0012 Accuracy 0.3549\n",
      "Epoch 300 Loss 0.0012 Accuracy 0.3566\n",
      "Time taken for 1 epoch: 7.706799030303955 secs\n",
      "\n",
      "Epoch 301 Batch 0 Loss 0.0027 Accuracy 0.3527\n",
      "Epoch 301 Batch 15 Loss 0.0018 Accuracy 0.3573\n",
      "Epoch 301 Batch 30 Loss 0.0020 Accuracy 0.3560\n",
      "Epoch 301 Loss 0.0017 Accuracy 0.3520\n",
      "Time taken for 1 epoch: 7.793119430541992 secs\n",
      "\n",
      "Epoch 302 Batch 0 Loss 0.0016 Accuracy 0.2977\n",
      "Epoch 302 Batch 15 Loss 0.0006 Accuracy 0.3466\n",
      "Epoch 302 Batch 30 Loss 0.0010 Accuracy 0.3529\n",
      "Epoch 302 Loss 0.0009 Accuracy 0.3525\n",
      "Time taken for 1 epoch: 7.708812236785889 secs\n",
      "\n",
      "Epoch 303 Batch 0 Loss 0.0005 Accuracy 0.3372\n",
      "Epoch 303 Batch 15 Loss 0.0016 Accuracy 0.3582\n",
      "Epoch 303 Batch 30 Loss 0.0012 Accuracy 0.3538\n",
      "Epoch 303 Loss 0.0012 Accuracy 0.3566\n",
      "Time taken for 1 epoch: 7.538614988327026 secs\n",
      "\n",
      "Epoch 304 Batch 0 Loss 0.0027 Accuracy 0.3822\n",
      "Epoch 304 Batch 15 Loss 0.0013 Accuracy 0.3526\n",
      "Epoch 304 Batch 30 Loss 0.0011 Accuracy 0.3524\n",
      "Epoch 304 Loss 0.0011 Accuracy 0.3503\n",
      "Time taken for 1 epoch: 7.792908191680908 secs\n",
      "\n",
      "Epoch 305 Batch 0 Loss 0.0019 Accuracy 0.3273\n",
      "Epoch 305 Batch 15 Loss 0.0015 Accuracy 0.3551\n",
      "Epoch 305 Batch 30 Loss 0.0018 Accuracy 0.3489\n",
      "Epoch 305 Loss 0.0014 Accuracy 0.3512\n",
      "Time taken for 1 epoch: 7.821939706802368 secs\n",
      "\n",
      "Epoch 306 Batch 0 Loss 0.0005 Accuracy 0.3466\n",
      "Epoch 306 Batch 15 Loss 0.0012 Accuracy 0.3546\n",
      "Epoch 306 Batch 30 Loss 0.0011 Accuracy 0.3596\n",
      "Epoch 306 Loss 0.0011 Accuracy 0.3578\n",
      "Time taken for 1 epoch: 7.558546543121338 secs\n",
      "\n",
      "Epoch 307 Batch 0 Loss 0.0012 Accuracy 0.3472\n",
      "Epoch 307 Batch 15 Loss 0.0016 Accuracy 0.3533\n",
      "Epoch 307 Batch 30 Loss 0.0013 Accuracy 0.3500\n",
      "Epoch 307 Loss 0.0013 Accuracy 0.3511\n",
      "Time taken for 1 epoch: 7.8382203578948975 secs\n",
      "\n",
      "Epoch 308 Batch 0 Loss 0.0001 Accuracy 0.3218\n",
      "Epoch 308 Batch 15 Loss 0.0013 Accuracy 0.3494\n",
      "Epoch 308 Batch 30 Loss 0.0014 Accuracy 0.3496\n",
      "Epoch 308 Loss 0.0017 Accuracy 0.3508\n",
      "Time taken for 1 epoch: 7.8646016120910645 secs\n",
      "\n",
      "Epoch 309 Batch 0 Loss 0.0004 Accuracy 0.3494\n",
      "Epoch 309 Batch 15 Loss 0.0010 Accuracy 0.3628\n",
      "Epoch 309 Batch 30 Loss 0.0019 Accuracy 0.3562\n",
      "Epoch 309 Loss 0.0016 Accuracy 0.3580\n",
      "Time taken for 1 epoch: 7.669933795928955 secs\n",
      "\n",
      "Epoch 310 Batch 0 Loss 0.0002 Accuracy 0.3383\n",
      "Epoch 310 Batch 15 Loss 0.0016 Accuracy 0.3584\n",
      "Epoch 310 Batch 30 Loss 0.0011 Accuracy 0.3569\n",
      "Epoch 310 Loss 0.0010 Accuracy 0.3518\n",
      "Time taken for 1 epoch: 7.865398168563843 secs\n",
      "\n",
      "Epoch 311 Batch 0 Loss 0.0017 Accuracy 0.4447\n",
      "Epoch 311 Batch 15 Loss 0.0017 Accuracy 0.3674\n",
      "Epoch 311 Batch 30 Loss 0.0013 Accuracy 0.3635\n",
      "Saving checkpoint for epoch 311 at ./checkpoints/train/ckpt-29\n",
      "Epoch 311 Loss 0.0015 Accuracy 0.3677\n",
      "Time taken for 1 epoch: 7.954279899597168 secs\n",
      "\n",
      "Epoch 312 Batch 0 Loss 0.0002 Accuracy 0.3438\n",
      "Epoch 312 Batch 15 Loss 0.0009 Accuracy 0.3735\n",
      "Epoch 312 Batch 30 Loss 0.0009 Accuracy 0.3630\n",
      "Epoch 312 Loss 0.0008 Accuracy 0.3589\n",
      "Time taken for 1 epoch: 7.582080602645874 secs\n",
      "\n",
      "Epoch 313 Batch 0 Loss 0.0008 Accuracy 0.3823\n",
      "Epoch 313 Batch 15 Loss 0.0008 Accuracy 0.3536\n",
      "Epoch 313 Batch 30 Loss 0.0013 Accuracy 0.3531\n",
      "Epoch 313 Loss 0.0011 Accuracy 0.3577\n",
      "Time taken for 1 epoch: 7.619367837905884 secs\n",
      "\n",
      "Epoch 314 Batch 0 Loss 0.0015 Accuracy 0.3384\n",
      "Epoch 314 Batch 15 Loss 0.0016 Accuracy 0.3422\n",
      "Epoch 314 Batch 30 Loss 0.0013 Accuracy 0.3565\n",
      "Epoch 314 Loss 0.0012 Accuracy 0.3516\n",
      "Time taken for 1 epoch: 7.790835857391357 secs\n",
      "\n",
      "Epoch 315 Batch 0 Loss 0.0032 Accuracy 0.3644\n",
      "Epoch 315 Batch 15 Loss 0.0014 Accuracy 0.3554\n",
      "Epoch 315 Batch 30 Loss 0.0013 Accuracy 0.3522\n",
      "Epoch 315 Loss 0.0012 Accuracy 0.3492\n",
      "Time taken for 1 epoch: 7.938655853271484 secs\n",
      "\n",
      "Epoch 316 Batch 0 Loss 0.0005 Accuracy 0.3469\n",
      "Epoch 316 Batch 15 Loss 0.0013 Accuracy 0.3422\n",
      "Epoch 316 Batch 30 Loss 0.0010 Accuracy 0.3458\n",
      "Epoch 316 Loss 0.0012 Accuracy 0.3481\n",
      "Time taken for 1 epoch: 8.059555530548096 secs\n",
      "\n",
      "Epoch 317 Batch 0 Loss 0.0002 Accuracy 0.3804\n",
      "Epoch 317 Batch 15 Loss 0.0010 Accuracy 0.3671\n",
      "Epoch 317 Batch 30 Loss 0.0011 Accuracy 0.3510\n",
      "Epoch 317 Loss 0.0010 Accuracy 0.3544\n",
      "Time taken for 1 epoch: 7.761997938156128 secs\n",
      "\n",
      "Epoch 318 Batch 0 Loss 0.0016 Accuracy 0.3585\n",
      "Epoch 318 Batch 15 Loss 0.0007 Accuracy 0.3666\n",
      "Epoch 318 Batch 30 Loss 0.0009 Accuracy 0.3673\n",
      "Epoch 318 Loss 0.0009 Accuracy 0.3608\n",
      "Time taken for 1 epoch: 7.56133508682251 secs\n",
      "\n",
      "Epoch 319 Batch 0 Loss 0.0001 Accuracy 0.3395\n",
      "Epoch 319 Batch 15 Loss 0.0007 Accuracy 0.3525\n",
      "Epoch 319 Batch 30 Loss 0.0010 Accuracy 0.3545\n",
      "Epoch 319 Loss 0.0015 Accuracy 0.3571\n",
      "Time taken for 1 epoch: 7.766069412231445 secs\n",
      "\n",
      "Epoch 320 Batch 0 Loss 0.0002 Accuracy 0.4208\n",
      "Epoch 320 Batch 15 Loss 0.0015 Accuracy 0.3641\n",
      "Epoch 320 Batch 30 Loss 0.0016 Accuracy 0.3625\n",
      "Epoch 320 Loss 0.0016 Accuracy 0.3581\n",
      "Time taken for 1 epoch: 7.5953943729400635 secs\n",
      "\n",
      "Epoch 321 Batch 0 Loss 0.0009 Accuracy 0.3142\n",
      "Epoch 321 Batch 15 Loss 0.0008 Accuracy 0.3650\n",
      "Epoch 321 Batch 30 Loss 0.0014 Accuracy 0.3591\n",
      "Epoch 321 Loss 0.0013 Accuracy 0.3622\n",
      "Time taken for 1 epoch: 7.46886944770813 secs\n",
      "\n",
      "Epoch 322 Batch 0 Loss 0.0003 Accuracy 0.3874\n",
      "Epoch 322 Batch 15 Loss 0.0023 Accuracy 0.3446\n",
      "Epoch 322 Batch 30 Loss 0.0017 Accuracy 0.3556\n",
      "Epoch 322 Loss 0.0017 Accuracy 0.3539\n",
      "Time taken for 1 epoch: 7.686853408813477 secs\n",
      "\n",
      "Epoch 323 Batch 0 Loss 0.0001 Accuracy 0.3594\n",
      "Epoch 323 Batch 15 Loss 0.0006 Accuracy 0.3519\n",
      "Epoch 323 Batch 30 Loss 0.0009 Accuracy 0.3642\n",
      "Epoch 323 Loss 0.0010 Accuracy 0.3585\n",
      "Time taken for 1 epoch: 7.6856279373168945 secs\n",
      "\n",
      "Epoch 324 Batch 0 Loss 0.0002 Accuracy 0.3460\n",
      "Epoch 324 Batch 15 Loss 0.0015 Accuracy 0.3608\n",
      "Epoch 324 Batch 30 Loss 0.0011 Accuracy 0.3596\n",
      "Epoch 324 Loss 0.0011 Accuracy 0.3571\n",
      "Time taken for 1 epoch: 8.07868766784668 secs\n",
      "\n",
      "Epoch 325 Batch 0 Loss 0.0004 Accuracy 0.3876\n",
      "Epoch 325 Batch 15 Loss 0.0018 Accuracy 0.3570\n",
      "Epoch 325 Batch 30 Loss 0.0018 Accuracy 0.3473\n",
      "Epoch 325 Loss 0.0014 Accuracy 0.3471\n",
      "Time taken for 1 epoch: 8.441794633865356 secs\n",
      "\n",
      "Epoch 326 Batch 0 Loss 0.0006 Accuracy 0.3831\n",
      "Epoch 326 Batch 15 Loss 0.0012 Accuracy 0.3564\n",
      "Epoch 326 Batch 30 Loss 0.0012 Accuracy 0.3581\n",
      "Epoch 326 Loss 0.0012 Accuracy 0.3583\n",
      "Time taken for 1 epoch: 7.56490159034729 secs\n",
      "\n",
      "Epoch 327 Batch 0 Loss 0.0009 Accuracy 0.4201\n",
      "Epoch 327 Batch 15 Loss 0.0010 Accuracy 0.3496\n",
      "Epoch 327 Batch 30 Loss 0.0009 Accuracy 0.3571\n",
      "Epoch 327 Loss 0.0008 Accuracy 0.3561\n",
      "Time taken for 1 epoch: 7.799460411071777 secs\n",
      "\n",
      "Epoch 328 Batch 0 Loss 0.0007 Accuracy 0.3576\n",
      "Epoch 328 Batch 15 Loss 0.0007 Accuracy 0.3619\n",
      "Epoch 328 Batch 30 Loss 0.0011 Accuracy 0.3543\n",
      "Epoch 328 Loss 0.0012 Accuracy 0.3546\n",
      "Time taken for 1 epoch: 7.878994703292847 secs\n",
      "\n",
      "Epoch 329 Batch 0 Loss 0.0001 Accuracy 0.3065\n",
      "Epoch 329 Batch 15 Loss 0.0029 Accuracy 0.3515\n",
      "Epoch 329 Batch 30 Loss 0.0026 Accuracy 0.3493\n",
      "Epoch 329 Loss 0.0022 Accuracy 0.3498\n",
      "Time taken for 1 epoch: 8.012652397155762 secs\n",
      "\n",
      "Epoch 330 Batch 0 Loss 0.0002 Accuracy 0.3378\n",
      "Epoch 330 Batch 15 Loss 0.0017 Accuracy 0.3489\n",
      "Epoch 330 Batch 30 Loss 0.0015 Accuracy 0.3517\n",
      "Epoch 330 Loss 0.0016 Accuracy 0.3545\n",
      "Time taken for 1 epoch: 7.90057373046875 secs\n",
      "\n",
      "Epoch 331 Batch 0 Loss 0.0001 Accuracy 0.3073\n",
      "Epoch 331 Batch 15 Loss 0.0005 Accuracy 0.3569\n",
      "Epoch 331 Batch 30 Loss 0.0008 Accuracy 0.3598\n",
      "Epoch 331 Loss 0.0009 Accuracy 0.3526\n",
      "Time taken for 1 epoch: 7.931439638137817 secs\n",
      "\n",
      "Epoch 332 Batch 0 Loss 0.0008 Accuracy 0.3318\n",
      "Epoch 332 Batch 15 Loss 0.0014 Accuracy 0.3474\n",
      "Epoch 332 Batch 30 Loss 0.0016 Accuracy 0.3477\n",
      "Epoch 332 Loss 0.0015 Accuracy 0.3505\n",
      "Time taken for 1 epoch: 7.921797513961792 secs\n",
      "\n",
      "Epoch 333 Batch 0 Loss 0.0001 Accuracy 0.4660\n",
      "Epoch 333 Batch 15 Loss 0.0007 Accuracy 0.3557\n",
      "Epoch 333 Batch 30 Loss 0.0009 Accuracy 0.3521\n",
      "Epoch 333 Loss 0.0010 Accuracy 0.3521\n",
      "Time taken for 1 epoch: 7.83212685585022 secs\n",
      "\n",
      "Epoch 334 Batch 0 Loss 0.0012 Accuracy 0.3979\n",
      "Epoch 334 Batch 15 Loss 0.0015 Accuracy 0.3603\n",
      "Epoch 334 Batch 30 Loss 0.0012 Accuracy 0.3546\n",
      "Epoch 334 Loss 0.0011 Accuracy 0.3536\n",
      "Time taken for 1 epoch: 7.810042381286621 secs\n",
      "\n",
      "Epoch 335 Batch 0 Loss 0.0001 Accuracy 0.3257\n",
      "Epoch 335 Batch 15 Loss 0.0017 Accuracy 0.3498\n",
      "Epoch 335 Batch 30 Loss 0.0014 Accuracy 0.3554\n",
      "Epoch 335 Loss 0.0013 Accuracy 0.3555\n",
      "Time taken for 1 epoch: 7.962694883346558 secs\n",
      "\n",
      "Epoch 336 Batch 0 Loss 0.0002 Accuracy 0.3657\n",
      "Epoch 336 Batch 15 Loss 0.0011 Accuracy 0.3479\n",
      "Epoch 336 Batch 30 Loss 0.0011 Accuracy 0.3543\n",
      "Epoch 336 Loss 0.0009 Accuracy 0.3608\n",
      "Time taken for 1 epoch: 7.7591872215271 secs\n",
      "\n",
      "Epoch 337 Batch 0 Loss 0.0002 Accuracy 0.2936\n",
      "Epoch 337 Batch 15 Loss 0.0004 Accuracy 0.3421\n",
      "Epoch 337 Batch 30 Loss 0.0008 Accuracy 0.3521\n",
      "Epoch 337 Loss 0.0008 Accuracy 0.3500\n",
      "Time taken for 1 epoch: 7.916462659835815 secs\n",
      "\n",
      "Epoch 338 Batch 0 Loss 0.0003 Accuracy 0.3100\n",
      "Epoch 338 Batch 15 Loss 0.0015 Accuracy 0.3528\n",
      "Epoch 338 Batch 30 Loss 0.0012 Accuracy 0.3559\n",
      "Epoch 338 Loss 0.0013 Accuracy 0.3557\n",
      "Time taken for 1 epoch: 7.677355527877808 secs\n",
      "\n",
      "Epoch 339 Batch 0 Loss 0.0008 Accuracy 0.3901\n",
      "Epoch 339 Batch 15 Loss 0.0010 Accuracy 0.3572\n",
      "Epoch 339 Batch 30 Loss 0.0012 Accuracy 0.3540\n",
      "Epoch 339 Loss 0.0010 Accuracy 0.3525\n",
      "Time taken for 1 epoch: 7.79455041885376 secs\n",
      "\n",
      "Epoch 340 Batch 0 Loss 0.0025 Accuracy 0.3448\n",
      "Epoch 340 Batch 15 Loss 0.0011 Accuracy 0.3505\n",
      "Epoch 340 Batch 30 Loss 0.0010 Accuracy 0.3561\n",
      "Epoch 340 Loss 0.0010 Accuracy 0.3538\n",
      "Time taken for 1 epoch: 7.779587030410767 secs\n",
      "\n",
      "Epoch 341 Batch 0 Loss 0.0003 Accuracy 0.3290\n",
      "Epoch 341 Batch 15 Loss 0.0006 Accuracy 0.3441\n",
      "Epoch 341 Batch 30 Loss 0.0007 Accuracy 0.3488\n",
      "Epoch 341 Loss 0.0008 Accuracy 0.3540\n",
      "Time taken for 1 epoch: 7.815016746520996 secs\n",
      "\n",
      "Epoch 342 Batch 0 Loss 0.0000 Accuracy 0.3513\n",
      "Epoch 342 Batch 15 Loss 0.0014 Accuracy 0.3546\n",
      "Epoch 342 Batch 30 Loss 0.0013 Accuracy 0.3520\n",
      "Epoch 342 Loss 0.0027 Accuracy 0.3554\n",
      "Time taken for 1 epoch: 7.649263858795166 secs\n",
      "\n",
      "Epoch 343 Batch 0 Loss 0.0006 Accuracy 0.3535\n",
      "Epoch 343 Batch 15 Loss 0.0016 Accuracy 0.3593\n",
      "Epoch 343 Batch 30 Loss 0.0014 Accuracy 0.3484\n",
      "Epoch 343 Loss 0.0014 Accuracy 0.3495\n",
      "Time taken for 1 epoch: 7.870004653930664 secs\n",
      "\n",
      "Epoch 344 Batch 0 Loss 0.0015 Accuracy 0.2833\n",
      "Epoch 344 Batch 15 Loss 0.0012 Accuracy 0.3424\n",
      "Epoch 344 Batch 30 Loss 0.0011 Accuracy 0.3458\n",
      "Epoch 344 Loss 0.0010 Accuracy 0.3516\n",
      "Time taken for 1 epoch: 7.853184461593628 secs\n",
      "\n",
      "Epoch 345 Batch 0 Loss 0.0018 Accuracy 0.3828\n",
      "Epoch 345 Batch 15 Loss 0.0004 Accuracy 0.3766\n",
      "Epoch 345 Batch 30 Loss 0.0005 Accuracy 0.3672\n",
      "Epoch 345 Loss 0.0005 Accuracy 0.3625\n",
      "Time taken for 1 epoch: 7.448188543319702 secs\n",
      "\n",
      "Epoch 346 Batch 0 Loss 0.0009 Accuracy 0.3529\n",
      "Epoch 346 Batch 15 Loss 0.0006 Accuracy 0.3445\n",
      "Epoch 346 Batch 30 Loss 0.0008 Accuracy 0.3510\n",
      "Epoch 346 Loss 0.0007 Accuracy 0.3520\n",
      "Time taken for 1 epoch: 7.8500096797943115 secs\n",
      "\n",
      "Epoch 347 Batch 0 Loss 0.0000 Accuracy 0.3750\n",
      "Epoch 347 Batch 15 Loss 0.0011 Accuracy 0.3514\n",
      "Epoch 347 Batch 30 Loss 0.0009 Accuracy 0.3532\n",
      "Epoch 347 Loss 0.0009 Accuracy 0.3520\n",
      "Time taken for 1 epoch: 7.907846927642822 secs\n",
      "\n",
      "Epoch 348 Batch 0 Loss 0.0000 Accuracy 0.4182\n",
      "Epoch 348 Batch 15 Loss 0.0008 Accuracy 0.3550\n",
      "Epoch 348 Batch 30 Loss 0.0006 Accuracy 0.3524\n",
      "Epoch 348 Loss 0.0005 Accuracy 0.3581\n",
      "Time taken for 1 epoch: 7.6665122509002686 secs\n",
      "\n",
      "Epoch 349 Batch 0 Loss 0.0007 Accuracy 0.3696\n",
      "Epoch 349 Batch 15 Loss 0.0016 Accuracy 0.3573\n",
      "Epoch 349 Batch 30 Loss 0.0011 Accuracy 0.3537\n",
      "Epoch 349 Loss 0.0011 Accuracy 0.3528\n",
      "Time taken for 1 epoch: 7.750310659408569 secs\n",
      "\n",
      "Epoch 350 Batch 0 Loss 0.0001 Accuracy 0.3062\n",
      "Epoch 350 Batch 15 Loss 0.0009 Accuracy 0.3516\n",
      "Epoch 350 Batch 30 Loss 0.0014 Accuracy 0.3566\n",
      "Epoch 350 Loss 0.0015 Accuracy 0.3575\n",
      "Time taken for 1 epoch: 7.60794472694397 secs\n",
      "\n",
      "Epoch 351 Batch 0 Loss 0.0001 Accuracy 0.4106\n",
      "Epoch 351 Batch 15 Loss 0.0009 Accuracy 0.3642\n",
      "Epoch 351 Batch 30 Loss 0.0007 Accuracy 0.3520\n",
      "Epoch 351 Loss 0.0006 Accuracy 0.3514\n",
      "Time taken for 1 epoch: 7.767928838729858 secs\n",
      "\n",
      "Epoch 352 Batch 0 Loss 0.0051 Accuracy 0.3542\n",
      "Epoch 352 Batch 15 Loss 0.0007 Accuracy 0.3450\n",
      "Epoch 352 Batch 30 Loss 0.0005 Accuracy 0.3561\n",
      "Epoch 352 Loss 0.0005 Accuracy 0.3566\n",
      "Time taken for 1 epoch: 7.7590436935424805 secs\n",
      "\n",
      "Epoch 353 Batch 0 Loss 0.0040 Accuracy 0.3379\n",
      "Epoch 353 Batch 15 Loss 0.0009 Accuracy 0.3557\n",
      "Epoch 353 Batch 30 Loss 0.0013 Accuracy 0.3479\n",
      "Epoch 353 Loss 0.0013 Accuracy 0.3525\n",
      "Time taken for 1 epoch: 7.818618059158325 secs\n",
      "\n",
      "Epoch 354 Batch 0 Loss 0.0002 Accuracy 0.2897\n",
      "Epoch 354 Batch 15 Loss 0.0006 Accuracy 0.3483\n",
      "Epoch 354 Batch 30 Loss 0.0008 Accuracy 0.3508\n",
      "Epoch 354 Loss 0.0008 Accuracy 0.3550\n",
      "Time taken for 1 epoch: 7.756957054138184 secs\n",
      "\n",
      "Epoch 355 Batch 0 Loss 0.0001 Accuracy 0.3286\n",
      "Epoch 355 Batch 15 Loss 0.0016 Accuracy 0.3404\n",
      "Epoch 355 Batch 30 Loss 0.0014 Accuracy 0.3449\n",
      "Epoch 355 Loss 0.0019 Accuracy 0.3495\n",
      "Time taken for 1 epoch: 7.8773181438446045 secs\n",
      "\n",
      "Epoch 356 Batch 0 Loss 0.0001 Accuracy 0.3734\n",
      "Epoch 356 Batch 15 Loss 0.0011 Accuracy 0.3529\n",
      "Epoch 356 Batch 30 Loss 0.0015 Accuracy 0.3557\n",
      "Epoch 356 Loss 0.0016 Accuracy 0.3509\n",
      "Time taken for 1 epoch: 7.868659257888794 secs\n",
      "\n",
      "Epoch 357 Batch 0 Loss 0.0002 Accuracy 0.3293\n",
      "Epoch 357 Batch 15 Loss 0.0005 Accuracy 0.3370\n",
      "Epoch 357 Batch 30 Loss 0.0008 Accuracy 0.3501\n",
      "Epoch 357 Loss 0.0008 Accuracy 0.3531\n",
      "Time taken for 1 epoch: 7.915510177612305 secs\n",
      "\n",
      "Epoch 358 Batch 0 Loss 0.0002 Accuracy 0.2942\n",
      "Epoch 358 Batch 15 Loss 0.0007 Accuracy 0.3423\n",
      "Epoch 358 Batch 30 Loss 0.0006 Accuracy 0.3483\n",
      "Epoch 358 Loss 0.0006 Accuracy 0.3515\n",
      "Time taken for 1 epoch: 7.88027811050415 secs\n",
      "\n",
      "Epoch 359 Batch 0 Loss 0.0004 Accuracy 0.4165\n",
      "Epoch 359 Batch 15 Loss 0.0007 Accuracy 0.3608\n",
      "Epoch 359 Batch 30 Loss 0.0008 Accuracy 0.3492\n",
      "Epoch 359 Loss 0.0008 Accuracy 0.3478\n",
      "Time taken for 1 epoch: 8.036336183547974 secs\n",
      "\n",
      "Epoch 360 Batch 0 Loss 0.0008 Accuracy 0.3061\n",
      "Epoch 360 Batch 15 Loss 0.0011 Accuracy 0.3631\n",
      "Epoch 360 Batch 30 Loss 0.0011 Accuracy 0.3574\n",
      "Epoch 360 Loss 0.0009 Accuracy 0.3574\n",
      "Time taken for 1 epoch: 7.748713970184326 secs\n",
      "\n",
      "Epoch 361 Batch 0 Loss 0.0016 Accuracy 0.3238\n",
      "Epoch 361 Batch 15 Loss 0.0010 Accuracy 0.3568\n",
      "Epoch 361 Batch 30 Loss 0.0012 Accuracy 0.3550\n",
      "Epoch 361 Loss 0.0012 Accuracy 0.3539\n",
      "Time taken for 1 epoch: 7.740608215332031 secs\n",
      "\n",
      "Epoch 362 Batch 0 Loss 0.0002 Accuracy 0.3932\n",
      "Epoch 362 Batch 15 Loss 0.0014 Accuracy 0.3457\n",
      "Epoch 362 Batch 30 Loss 0.0013 Accuracy 0.3500\n",
      "Epoch 362 Loss 0.0012 Accuracy 0.3499\n",
      "Time taken for 1 epoch: 7.907523155212402 secs\n",
      "\n",
      "Epoch 363 Batch 0 Loss 0.0005 Accuracy 0.3189\n",
      "Epoch 363 Batch 15 Loss 0.0020 Accuracy 0.3564\n",
      "Epoch 363 Batch 30 Loss 0.0019 Accuracy 0.3552\n",
      "Epoch 363 Loss 0.0016 Accuracy 0.3547\n",
      "Time taken for 1 epoch: 7.711073875427246 secs\n",
      "\n",
      "Epoch 364 Batch 0 Loss 0.0017 Accuracy 0.4356\n",
      "Epoch 364 Batch 15 Loss 0.0010 Accuracy 0.3524\n",
      "Epoch 364 Batch 30 Loss 0.0017 Accuracy 0.3499\n",
      "Epoch 364 Loss 0.0017 Accuracy 0.3536\n",
      "Time taken for 1 epoch: 7.7697436809539795 secs\n",
      "\n",
      "Epoch 365 Batch 0 Loss 0.0006 Accuracy 0.3192\n",
      "Epoch 365 Batch 15 Loss 0.0011 Accuracy 0.3450\n",
      "Epoch 365 Batch 30 Loss 0.0010 Accuracy 0.3557\n",
      "Epoch 365 Loss 0.0009 Accuracy 0.3579\n",
      "Time taken for 1 epoch: 7.664590120315552 secs\n",
      "\n",
      "Epoch 366 Batch 0 Loss 0.0005 Accuracy 0.3627\n",
      "Epoch 366 Batch 15 Loss 0.0007 Accuracy 0.3558\n",
      "Epoch 366 Batch 30 Loss 0.0007 Accuracy 0.3478\n",
      "Epoch 366 Loss 0.0008 Accuracy 0.3513\n",
      "Time taken for 1 epoch: 7.91491961479187 secs\n",
      "\n",
      "Epoch 367 Batch 0 Loss 0.0000 Accuracy 0.3465\n",
      "Epoch 367 Batch 15 Loss 0.0006 Accuracy 0.3545\n",
      "Epoch 367 Batch 30 Loss 0.0007 Accuracy 0.3531\n",
      "Epoch 367 Loss 0.0007 Accuracy 0.3506\n",
      "Time taken for 1 epoch: 8.03530502319336 secs\n",
      "\n",
      "Epoch 368 Batch 0 Loss 0.0000 Accuracy 0.3451\n",
      "Epoch 368 Batch 15 Loss 0.0011 Accuracy 0.3390\n",
      "Epoch 368 Batch 30 Loss 0.0008 Accuracy 0.3455\n",
      "Epoch 368 Loss 0.0008 Accuracy 0.3494\n",
      "Time taken for 1 epoch: 7.807840824127197 secs\n",
      "\n",
      "Epoch 369 Batch 0 Loss 0.0002 Accuracy 0.3653\n",
      "Epoch 369 Batch 15 Loss 0.0011 Accuracy 0.3599\n",
      "Epoch 369 Batch 30 Loss 0.0009 Accuracy 0.3559\n",
      "Epoch 369 Loss 0.0009 Accuracy 0.3535\n",
      "Time taken for 1 epoch: 7.8993566036224365 secs\n",
      "\n",
      "Epoch 370 Batch 0 Loss 0.0022 Accuracy 0.3061\n",
      "Epoch 370 Batch 15 Loss 0.0011 Accuracy 0.3542\n",
      "Epoch 370 Batch 30 Loss 0.0013 Accuracy 0.3634\n",
      "Epoch 370 Loss 0.0012 Accuracy 0.3636\n",
      "Time taken for 1 epoch: 7.501684188842773 secs\n",
      "\n",
      "Epoch 371 Batch 0 Loss 0.0015 Accuracy 0.3793\n",
      "Epoch 371 Batch 15 Loss 0.0013 Accuracy 0.3461\n",
      "Epoch 371 Batch 30 Loss 0.0013 Accuracy 0.3543\n",
      "Epoch 371 Loss 0.0011 Accuracy 0.3561\n",
      "Time taken for 1 epoch: 7.687948942184448 secs\n",
      "\n",
      "Epoch 372 Batch 0 Loss 0.0006 Accuracy 0.2895\n",
      "Epoch 372 Batch 15 Loss 0.0016 Accuracy 0.3572\n",
      "Epoch 372 Batch 30 Loss 0.0015 Accuracy 0.3555\n",
      "Epoch 372 Loss 0.0015 Accuracy 0.3528\n",
      "Time taken for 1 epoch: 7.89667820930481 secs\n",
      "\n",
      "Epoch 373 Batch 0 Loss 0.0051 Accuracy 0.3064\n",
      "Epoch 373 Batch 15 Loss 0.0019 Accuracy 0.3473\n",
      "Epoch 373 Batch 30 Loss 0.0015 Accuracy 0.3495\n",
      "Epoch 373 Loss 0.0014 Accuracy 0.3513\n",
      "Time taken for 1 epoch: 7.880476474761963 secs\n",
      "\n",
      "Epoch 374 Batch 0 Loss 0.0001 Accuracy 0.3895\n",
      "Epoch 374 Batch 15 Loss 0.0016 Accuracy 0.3596\n",
      "Epoch 374 Batch 30 Loss 0.0012 Accuracy 0.3585\n",
      "Epoch 374 Loss 0.0009 Accuracy 0.3555\n",
      "Time taken for 1 epoch: 7.706027507781982 secs\n",
      "\n",
      "Epoch 375 Batch 0 Loss 0.0016 Accuracy 0.3877\n",
      "Epoch 375 Batch 15 Loss 0.0004 Accuracy 0.3568\n",
      "Epoch 375 Batch 30 Loss 0.0007 Accuracy 0.3548\n",
      "Epoch 375 Loss 0.0006 Accuracy 0.3533\n",
      "Time taken for 1 epoch: 7.865086078643799 secs\n",
      "\n",
      "Epoch 376 Batch 0 Loss 0.0005 Accuracy 0.4225\n",
      "Epoch 376 Batch 15 Loss 0.0010 Accuracy 0.3676\n",
      "Epoch 376 Batch 30 Loss 0.0011 Accuracy 0.3589\n",
      "Epoch 376 Loss 0.0009 Accuracy 0.3555\n",
      "Time taken for 1 epoch: 7.7212488651275635 secs\n",
      "\n",
      "Epoch 377 Batch 0 Loss 0.0001 Accuracy 0.4603\n",
      "Epoch 377 Batch 15 Loss 0.0007 Accuracy 0.3677\n",
      "Epoch 377 Batch 30 Loss 0.0011 Accuracy 0.3583\n",
      "Epoch 377 Loss 0.0012 Accuracy 0.3586\n",
      "Time taken for 1 epoch: 7.644691228866577 secs\n",
      "\n",
      "Epoch 378 Batch 0 Loss 0.0001 Accuracy 0.4089\n",
      "Epoch 378 Batch 15 Loss 0.0005 Accuracy 0.3584\n",
      "Epoch 378 Batch 30 Loss 0.0006 Accuracy 0.3565\n",
      "Epoch 378 Loss 0.0006 Accuracy 0.3613\n",
      "Time taken for 1 epoch: 7.597707509994507 secs\n",
      "\n",
      "Epoch 379 Batch 0 Loss 0.0002 Accuracy 0.4138\n",
      "Epoch 379 Batch 15 Loss 0.0004 Accuracy 0.3544\n",
      "Epoch 379 Batch 30 Loss 0.0004 Accuracy 0.3528\n",
      "Epoch 379 Loss 0.0005 Accuracy 0.3555\n",
      "Time taken for 1 epoch: 7.943156480789185 secs\n",
      "\n",
      "Epoch 380 Batch 0 Loss 0.0007 Accuracy 0.3264\n",
      "Epoch 380 Batch 15 Loss 0.0008 Accuracy 0.3525\n",
      "Epoch 380 Batch 30 Loss 0.0012 Accuracy 0.3528\n",
      "Epoch 380 Loss 0.0012 Accuracy 0.3534\n",
      "Time taken for 1 epoch: 7.9180731773376465 secs\n",
      "\n",
      "Epoch 381 Batch 0 Loss 0.0007 Accuracy 0.3529\n",
      "Epoch 381 Batch 15 Loss 0.0006 Accuracy 0.3675\n",
      "Epoch 381 Batch 30 Loss 0.0007 Accuracy 0.3551\n",
      "Epoch 381 Loss 0.0011 Accuracy 0.3520\n",
      "Time taken for 1 epoch: 7.789691925048828 secs\n",
      "\n",
      "Epoch 382 Batch 0 Loss 0.0000 Accuracy 0.3281\n",
      "Epoch 382 Batch 15 Loss 0.0007 Accuracy 0.3387\n",
      "Epoch 382 Batch 30 Loss 0.0006 Accuracy 0.3483\n",
      "Epoch 382 Loss 0.0008 Accuracy 0.3503\n",
      "Time taken for 1 epoch: 8.024758577346802 secs\n",
      "\n",
      "Epoch 383 Batch 0 Loss 0.0002 Accuracy 0.3838\n",
      "Epoch 383 Batch 15 Loss 0.0005 Accuracy 0.3629\n",
      "Epoch 383 Batch 30 Loss 0.0009 Accuracy 0.3562\n",
      "Epoch 383 Loss 0.0017 Accuracy 0.3517\n",
      "Time taken for 1 epoch: 7.8713719844818115 secs\n",
      "\n",
      "Epoch 384 Batch 0 Loss 0.0007 Accuracy 0.3267\n",
      "Epoch 384 Batch 15 Loss 0.0024 Accuracy 0.3473\n",
      "Epoch 384 Batch 30 Loss 0.0017 Accuracy 0.3442\n",
      "Epoch 384 Loss 0.0015 Accuracy 0.3499\n",
      "Time taken for 1 epoch: 7.841877222061157 secs\n",
      "\n",
      "Epoch 385 Batch 0 Loss 0.0009 Accuracy 0.3176\n",
      "Epoch 385 Batch 15 Loss 0.0007 Accuracy 0.3409\n",
      "Epoch 385 Batch 30 Loss 0.0006 Accuracy 0.3555\n",
      "Epoch 385 Loss 0.0006 Accuracy 0.3572\n",
      "Time taken for 1 epoch: 7.902291774749756 secs\n",
      "\n",
      "Epoch 386 Batch 0 Loss 0.0011 Accuracy 0.3992\n",
      "Epoch 386 Batch 15 Loss 0.0011 Accuracy 0.3468\n",
      "Epoch 386 Batch 30 Loss 0.0007 Accuracy 0.3529\n",
      "Epoch 386 Loss 0.0006 Accuracy 0.3521\n",
      "Time taken for 1 epoch: 7.8180811405181885 secs\n",
      "\n",
      "Epoch 387 Batch 0 Loss 0.0000 Accuracy 0.3594\n",
      "Epoch 387 Batch 15 Loss 0.0005 Accuracy 0.3561\n",
      "Epoch 387 Batch 30 Loss 0.0005 Accuracy 0.3532\n",
      "Epoch 387 Loss 0.0005 Accuracy 0.3522\n",
      "Time taken for 1 epoch: 7.8144636154174805 secs\n",
      "\n",
      "Epoch 388 Batch 0 Loss 0.0023 Accuracy 0.3887\n",
      "Epoch 388 Batch 15 Loss 0.0017 Accuracy 0.3560\n",
      "Epoch 388 Batch 30 Loss 0.0018 Accuracy 0.3592\n",
      "Epoch 388 Loss 0.0015 Accuracy 0.3541\n",
      "Time taken for 1 epoch: 7.738589763641357 secs\n",
      "\n",
      "Epoch 389 Batch 0 Loss 0.0004 Accuracy 0.4052\n",
      "Epoch 389 Batch 15 Loss 0.0010 Accuracy 0.3546\n",
      "Epoch 389 Batch 30 Loss 0.0013 Accuracy 0.3481\n",
      "Epoch 389 Loss 0.0012 Accuracy 0.3536\n",
      "Time taken for 1 epoch: 7.831383466720581 secs\n",
      "\n",
      "Epoch 390 Batch 0 Loss 0.0002 Accuracy 0.3125\n",
      "Epoch 390 Batch 15 Loss 0.0009 Accuracy 0.3407\n",
      "Epoch 390 Batch 30 Loss 0.0014 Accuracy 0.3510\n",
      "Epoch 390 Loss 0.0012 Accuracy 0.3529\n",
      "Time taken for 1 epoch: 7.833397626876831 secs\n",
      "\n",
      "Epoch 391 Batch 0 Loss 0.0001 Accuracy 0.3540\n",
      "Epoch 391 Batch 15 Loss 0.0022 Accuracy 0.3531\n",
      "Epoch 391 Batch 30 Loss 0.0019 Accuracy 0.3539\n",
      "Epoch 391 Loss 0.0016 Accuracy 0.3571\n",
      "Time taken for 1 epoch: 7.664357423782349 secs\n",
      "\n",
      "Epoch 392 Batch 0 Loss 0.0021 Accuracy 0.3624\n",
      "Epoch 392 Batch 15 Loss 0.0010 Accuracy 0.3440\n",
      "Epoch 392 Batch 30 Loss 0.0008 Accuracy 0.3490\n",
      "Epoch 392 Loss 0.0010 Accuracy 0.3504\n",
      "Time taken for 1 epoch: 7.8421690464019775 secs\n",
      "\n",
      "Epoch 393 Batch 0 Loss 0.0004 Accuracy 0.3839\n",
      "Epoch 393 Batch 15 Loss 0.0015 Accuracy 0.3655\n",
      "Epoch 393 Batch 30 Loss 0.0015 Accuracy 0.3592\n",
      "Epoch 393 Loss 0.0013 Accuracy 0.3552\n",
      "Time taken for 1 epoch: 7.767756462097168 secs\n",
      "\n",
      "Epoch 394 Batch 0 Loss 0.0001 Accuracy 0.3201\n",
      "Epoch 394 Batch 15 Loss 0.0006 Accuracy 0.3529\n",
      "Epoch 394 Batch 30 Loss 0.0007 Accuracy 0.3622\n",
      "Epoch 394 Loss 0.0008 Accuracy 0.3591\n",
      "Time taken for 1 epoch: 7.628479480743408 secs\n",
      "\n",
      "Epoch 395 Batch 0 Loss 0.0000 Accuracy 0.3438\n",
      "Epoch 395 Batch 15 Loss 0.0003 Accuracy 0.3691\n",
      "Epoch 395 Batch 30 Loss 0.0005 Accuracy 0.3623\n",
      "Epoch 395 Loss 0.0005 Accuracy 0.3553\n",
      "Time taken for 1 epoch: 7.755337953567505 secs\n",
      "\n",
      "Epoch 396 Batch 0 Loss 0.0001 Accuracy 0.3266\n",
      "Epoch 396 Batch 15 Loss 0.0011 Accuracy 0.3488\n",
      "Epoch 396 Batch 30 Loss 0.0014 Accuracy 0.3457\n",
      "Epoch 396 Loss 0.0012 Accuracy 0.3463\n",
      "Time taken for 1 epoch: 7.895217657089233 secs\n",
      "\n",
      "Epoch 397 Batch 0 Loss 0.0041 Accuracy 0.3419\n",
      "Epoch 397 Batch 15 Loss 0.0012 Accuracy 0.3427\n",
      "Epoch 397 Batch 30 Loss 0.0012 Accuracy 0.3488\n",
      "Epoch 397 Loss 0.0012 Accuracy 0.3508\n",
      "Time taken for 1 epoch: 7.78097677230835 secs\n",
      "\n",
      "Epoch 398 Batch 0 Loss 0.0017 Accuracy 0.3368\n",
      "Epoch 398 Batch 15 Loss 0.0004 Accuracy 0.3518\n",
      "Epoch 398 Batch 30 Loss 0.0005 Accuracy 0.3549\n",
      "Epoch 398 Loss 0.0006 Accuracy 0.3552\n",
      "Time taken for 1 epoch: 7.653080224990845 secs\n",
      "\n",
      "Epoch 399 Batch 0 Loss 0.0014 Accuracy 0.3542\n",
      "Epoch 399 Batch 15 Loss 0.0010 Accuracy 0.3691\n",
      "Epoch 399 Batch 30 Loss 0.0011 Accuracy 0.3561\n",
      "Epoch 399 Loss 0.0015 Accuracy 0.3527\n",
      "Time taken for 1 epoch: 7.718493700027466 secs\n",
      "\n",
      "Epoch 400 Batch 0 Loss 0.0004 Accuracy 0.3705\n",
      "Epoch 400 Batch 15 Loss 0.0015 Accuracy 0.3480\n",
      "Epoch 400 Batch 30 Loss 0.0017 Accuracy 0.3567\n",
      "Epoch 400 Loss 0.0015 Accuracy 0.3603\n",
      "Time taken for 1 epoch: 7.607247829437256 secs\n",
      "\n",
      "Epoch 401 Batch 0 Loss 0.0008 Accuracy 0.3294\n",
      "Epoch 401 Batch 15 Loss 0.0015 Accuracy 0.3612\n",
      "Epoch 401 Batch 30 Loss 0.0011 Accuracy 0.3548\n",
      "Epoch 401 Loss 0.0010 Accuracy 0.3544\n",
      "Time taken for 1 epoch: 7.7357401847839355 secs\n",
      "\n",
      "Epoch 402 Batch 0 Loss 0.0006 Accuracy 0.3238\n",
      "Epoch 402 Batch 15 Loss 0.0009 Accuracy 0.3567\n",
      "Epoch 402 Batch 30 Loss 0.0007 Accuracy 0.3571\n",
      "Epoch 402 Loss 0.0007 Accuracy 0.3562\n",
      "Time taken for 1 epoch: 7.709433317184448 secs\n",
      "\n",
      "Epoch 403 Batch 0 Loss 0.0000 Accuracy 0.3305\n",
      "Epoch 403 Batch 15 Loss 0.0006 Accuracy 0.3522\n",
      "Epoch 403 Batch 30 Loss 0.0009 Accuracy 0.3546\n",
      "Epoch 403 Loss 0.0008 Accuracy 0.3532\n",
      "Time taken for 1 epoch: 7.813807487487793 secs\n",
      "\n",
      "Epoch 404 Batch 0 Loss 0.0008 Accuracy 0.4154\n",
      "Epoch 404 Batch 15 Loss 0.0004 Accuracy 0.3540\n",
      "Epoch 404 Batch 30 Loss 0.0006 Accuracy 0.3524\n",
      "Epoch 404 Loss 0.0019 Accuracy 0.3546\n",
      "Time taken for 1 epoch: 7.698136568069458 secs\n",
      "\n",
      "Epoch 405 Batch 0 Loss 0.0013 Accuracy 0.3085\n",
      "Epoch 405 Batch 15 Loss 0.0017 Accuracy 0.3574\n",
      "Epoch 405 Batch 30 Loss 0.0016 Accuracy 0.3511\n",
      "Epoch 405 Loss 0.0015 Accuracy 0.3513\n",
      "Time taken for 1 epoch: 7.835201978683472 secs\n",
      "\n",
      "Epoch 406 Batch 0 Loss 0.0010 Accuracy 0.3281\n",
      "Epoch 406 Batch 15 Loss 0.0005 Accuracy 0.3517\n",
      "Epoch 406 Batch 30 Loss 0.0007 Accuracy 0.3480\n",
      "Epoch 406 Loss 0.0008 Accuracy 0.3522\n",
      "Time taken for 1 epoch: 7.8234171867370605 secs\n",
      "\n",
      "Epoch 407 Batch 0 Loss 0.0016 Accuracy 0.3325\n",
      "Epoch 407 Batch 15 Loss 0.0012 Accuracy 0.3446\n",
      "Epoch 407 Batch 30 Loss 0.0011 Accuracy 0.3501\n",
      "Epoch 407 Loss 0.0010 Accuracy 0.3506\n",
      "Time taken for 1 epoch: 7.73711633682251 secs\n",
      "\n",
      "Epoch 408 Batch 0 Loss 0.0003 Accuracy 0.3835\n",
      "Epoch 408 Batch 15 Loss 0.0007 Accuracy 0.3586\n",
      "Epoch 408 Batch 30 Loss 0.0009 Accuracy 0.3501\n",
      "Epoch 408 Loss 0.0009 Accuracy 0.3551\n",
      "Time taken for 1 epoch: 7.655098915100098 secs\n",
      "\n",
      "Epoch 409 Batch 0 Loss 0.0054 Accuracy 0.3625\n",
      "Epoch 409 Batch 15 Loss 0.0009 Accuracy 0.3551\n",
      "Epoch 409 Batch 30 Loss 0.0011 Accuracy 0.3521\n",
      "Epoch 409 Loss 0.0010 Accuracy 0.3541\n",
      "Time taken for 1 epoch: 7.685012340545654 secs\n",
      "\n",
      "Epoch 410 Batch 0 Loss 0.0002 Accuracy 0.3454\n",
      "Epoch 410 Batch 15 Loss 0.0010 Accuracy 0.3608\n",
      "Epoch 410 Batch 30 Loss 0.0009 Accuracy 0.3576\n",
      "Epoch 410 Loss 0.0011 Accuracy 0.3585\n",
      "Time taken for 1 epoch: 7.616817474365234 secs\n",
      "\n",
      "Epoch 411 Batch 0 Loss 0.0001 Accuracy 0.3979\n",
      "Epoch 411 Batch 15 Loss 0.0005 Accuracy 0.3448\n",
      "Epoch 411 Batch 30 Loss 0.0008 Accuracy 0.3489\n",
      "Epoch 411 Loss 0.0008 Accuracy 0.3493\n",
      "Time taken for 1 epoch: 7.841813802719116 secs\n",
      "\n",
      "Epoch 412 Batch 0 Loss 0.0002 Accuracy 0.3951\n",
      "Epoch 412 Batch 15 Loss 0.0011 Accuracy 0.3657\n",
      "Epoch 412 Batch 30 Loss 0.0009 Accuracy 0.3592\n",
      "Epoch 412 Loss 0.0008 Accuracy 0.3581\n",
      "Time taken for 1 epoch: 7.514801263809204 secs\n",
      "\n",
      "Epoch 413 Batch 0 Loss 0.0001 Accuracy 0.2956\n",
      "Epoch 413 Batch 15 Loss 0.0015 Accuracy 0.3549\n",
      "Epoch 413 Batch 30 Loss 0.0012 Accuracy 0.3534\n",
      "Epoch 413 Loss 0.0012 Accuracy 0.3529\n",
      "Time taken for 1 epoch: 7.7462990283966064 secs\n",
      "\n",
      "Epoch 414 Batch 0 Loss 0.0079 Accuracy 0.3627\n",
      "Epoch 414 Batch 15 Loss 0.0018 Accuracy 0.3648\n",
      "Epoch 414 Batch 30 Loss 0.0015 Accuracy 0.3605\n",
      "Epoch 414 Loss 0.0013 Accuracy 0.3617\n",
      "Time taken for 1 epoch: 7.542590141296387 secs\n",
      "\n",
      "Epoch 415 Batch 0 Loss 0.0005 Accuracy 0.2764\n",
      "Epoch 415 Batch 15 Loss 0.0011 Accuracy 0.3532\n",
      "Epoch 415 Batch 30 Loss 0.0010 Accuracy 0.3512\n",
      "Epoch 415 Loss 0.0010 Accuracy 0.3514\n",
      "Time taken for 1 epoch: 7.8224992752075195 secs\n",
      "\n",
      "Epoch 416 Batch 0 Loss 0.0001 Accuracy 0.3700\n",
      "Epoch 416 Batch 15 Loss 0.0002 Accuracy 0.3585\n",
      "Epoch 416 Batch 30 Loss 0.0003 Accuracy 0.3460\n",
      "Epoch 416 Loss 0.0003 Accuracy 0.3493\n",
      "Time taken for 1 epoch: 7.932223081588745 secs\n",
      "\n",
      "Epoch 417 Batch 0 Loss 0.0001 Accuracy 0.4009\n",
      "Epoch 417 Batch 15 Loss 0.0008 Accuracy 0.3629\n",
      "Epoch 417 Batch 30 Loss 0.0010 Accuracy 0.3530\n",
      "Epoch 417 Loss 0.0009 Accuracy 0.3511\n",
      "Time taken for 1 epoch: 7.772014379501343 secs\n",
      "\n",
      "Epoch 418 Batch 0 Loss 0.0000 Accuracy 0.3558\n",
      "Epoch 418 Batch 15 Loss 0.0012 Accuracy 0.3544\n",
      "Epoch 418 Batch 30 Loss 0.0009 Accuracy 0.3462\n",
      "Epoch 418 Loss 0.0011 Accuracy 0.3484\n",
      "Time taken for 1 epoch: 8.013587474822998 secs\n",
      "\n",
      "Epoch 419 Batch 0 Loss 0.0001 Accuracy 0.3247\n",
      "Epoch 419 Batch 15 Loss 0.0010 Accuracy 0.3555\n",
      "Epoch 419 Batch 30 Loss 0.0011 Accuracy 0.3689\n",
      "Epoch 419 Loss 0.0009 Accuracy 0.3664\n",
      "Time taken for 1 epoch: 7.434563636779785 secs\n",
      "\n",
      "Epoch 420 Batch 0 Loss 0.0000 Accuracy 0.3255\n",
      "Epoch 420 Batch 15 Loss 0.0008 Accuracy 0.3559\n",
      "Epoch 420 Batch 30 Loss 0.0010 Accuracy 0.3547\n",
      "Epoch 420 Loss 0.0009 Accuracy 0.3555\n",
      "Time taken for 1 epoch: 7.740879535675049 secs\n",
      "\n",
      "Epoch 421 Batch 0 Loss 0.0001 Accuracy 0.2541\n",
      "Epoch 421 Batch 15 Loss 0.0014 Accuracy 0.3534\n",
      "Epoch 421 Batch 30 Loss 0.0013 Accuracy 0.3537\n",
      "Epoch 421 Loss 0.0014 Accuracy 0.3511\n",
      "Time taken for 1 epoch: 7.901149272918701 secs\n",
      "\n",
      "Epoch 422 Batch 0 Loss 0.0003 Accuracy 0.3384\n",
      "Epoch 422 Batch 15 Loss 0.0011 Accuracy 0.3460\n",
      "Epoch 422 Batch 30 Loss 0.0014 Accuracy 0.3602\n",
      "Epoch 422 Loss 0.0013 Accuracy 0.3587\n",
      "Time taken for 1 epoch: 7.661257266998291 secs\n",
      "\n",
      "Epoch 423 Batch 0 Loss 0.0001 Accuracy 0.3565\n",
      "Epoch 423 Batch 15 Loss 0.0016 Accuracy 0.3555\n",
      "Epoch 423 Batch 30 Loss 0.0013 Accuracy 0.3564\n",
      "Epoch 423 Loss 0.0011 Accuracy 0.3567\n",
      "Time taken for 1 epoch: 7.678797483444214 secs\n",
      "\n",
      "Epoch 424 Batch 0 Loss 0.0008 Accuracy 0.3853\n",
      "Epoch 424 Batch 15 Loss 0.0012 Accuracy 0.3685\n",
      "Epoch 424 Batch 30 Loss 0.0011 Accuracy 0.3598\n",
      "Epoch 424 Loss 0.0010 Accuracy 0.3541\n",
      "Time taken for 1 epoch: 7.7727015018463135 secs\n",
      "\n",
      "Epoch 425 Batch 0 Loss 0.0001 Accuracy 0.4203\n",
      "Epoch 425 Batch 15 Loss 0.0007 Accuracy 0.3530\n",
      "Epoch 425 Batch 30 Loss 0.0005 Accuracy 0.3592\n",
      "Epoch 425 Loss 0.0006 Accuracy 0.3575\n",
      "Time taken for 1 epoch: 7.5996551513671875 secs\n",
      "\n",
      "Epoch 426 Batch 0 Loss 0.0002 Accuracy 0.3780\n",
      "Epoch 426 Batch 15 Loss 0.0005 Accuracy 0.3494\n",
      "Epoch 426 Batch 30 Loss 0.0006 Accuracy 0.3547\n",
      "Epoch 426 Loss 0.0008 Accuracy 0.3521\n",
      "Time taken for 1 epoch: 7.627689838409424 secs\n",
      "\n",
      "Epoch 427 Batch 0 Loss 0.0001 Accuracy 0.3535\n",
      "Epoch 427 Batch 15 Loss 0.0012 Accuracy 0.3652\n",
      "Epoch 427 Batch 30 Loss 0.0009 Accuracy 0.3585\n",
      "Epoch 427 Loss 0.0009 Accuracy 0.3601\n",
      "Time taken for 1 epoch: 7.508965015411377 secs\n",
      "\n",
      "Epoch 428 Batch 0 Loss 0.0002 Accuracy 0.3081\n",
      "Epoch 428 Batch 15 Loss 0.0011 Accuracy 0.3527\n",
      "Epoch 428 Batch 30 Loss 0.0011 Accuracy 0.3516\n",
      "Epoch 428 Loss 0.0020 Accuracy 0.3528\n",
      "Time taken for 1 epoch: 7.656606435775757 secs\n",
      "\n",
      "Epoch 429 Batch 0 Loss 0.0002 Accuracy 0.3710\n",
      "Epoch 429 Batch 15 Loss 0.0008 Accuracy 0.3480\n",
      "Epoch 429 Batch 30 Loss 0.0009 Accuracy 0.3557\n",
      "Epoch 429 Loss 0.0009 Accuracy 0.3551\n",
      "Time taken for 1 epoch: 7.801354646682739 secs\n",
      "\n",
      "Epoch 430 Batch 0 Loss 0.0005 Accuracy 0.4057\n",
      "Epoch 430 Batch 15 Loss 0.0009 Accuracy 0.3642\n",
      "Epoch 430 Batch 30 Loss 0.0008 Accuracy 0.3595\n",
      "Epoch 430 Loss 0.0008 Accuracy 0.3570\n",
      "Time taken for 1 epoch: 7.654399871826172 secs\n",
      "\n",
      "Epoch 431 Batch 0 Loss 0.0001 Accuracy 0.4224\n",
      "Epoch 431 Batch 15 Loss 0.0008 Accuracy 0.3590\n",
      "Epoch 431 Batch 30 Loss 0.0006 Accuracy 0.3557\n",
      "Epoch 431 Loss 0.0008 Accuracy 0.3550\n",
      "Time taken for 1 epoch: 7.790027141571045 secs\n",
      "\n",
      "Epoch 432 Batch 0 Loss 0.0001 Accuracy 0.3848\n",
      "Epoch 432 Batch 15 Loss 0.0019 Accuracy 0.3568\n",
      "Epoch 432 Batch 30 Loss 0.0018 Accuracy 0.3562\n",
      "Epoch 432 Loss 0.0015 Accuracy 0.3513\n",
      "Time taken for 1 epoch: 7.854642868041992 secs\n",
      "\n",
      "Epoch 433 Batch 0 Loss 0.0007 Accuracy 0.3237\n",
      "Epoch 433 Batch 15 Loss 0.0006 Accuracy 0.3552\n",
      "Epoch 433 Batch 30 Loss 0.0007 Accuracy 0.3609\n",
      "Epoch 433 Loss 0.0007 Accuracy 0.3628\n",
      "Time taken for 1 epoch: 7.434801816940308 secs\n",
      "\n",
      "Epoch 434 Batch 0 Loss 0.0002 Accuracy 0.3407\n",
      "Epoch 434 Batch 15 Loss 0.0009 Accuracy 0.3352\n",
      "Epoch 434 Batch 30 Loss 0.0011 Accuracy 0.3490\n",
      "Epoch 434 Loss 0.0011 Accuracy 0.3514\n",
      "Time taken for 1 epoch: 7.785413980484009 secs\n",
      "\n",
      "Epoch 435 Batch 0 Loss 0.0007 Accuracy 0.3371\n",
      "Epoch 435 Batch 15 Loss 0.0020 Accuracy 0.3493\n",
      "Epoch 435 Batch 30 Loss 0.0016 Accuracy 0.3541\n",
      "Epoch 435 Loss 0.0013 Accuracy 0.3533\n",
      "Time taken for 1 epoch: 7.724383354187012 secs\n",
      "\n",
      "Epoch 436 Batch 0 Loss 0.0023 Accuracy 0.3734\n",
      "Epoch 436 Batch 15 Loss 0.0015 Accuracy 0.3492\n",
      "Epoch 436 Batch 30 Loss 0.0014 Accuracy 0.3579\n",
      "Epoch 436 Loss 0.0016 Accuracy 0.3564\n",
      "Time taken for 1 epoch: 7.716287612915039 secs\n",
      "\n",
      "Epoch 437 Batch 0 Loss 0.0029 Accuracy 0.3777\n",
      "Epoch 437 Batch 15 Loss 0.0015 Accuracy 0.3399\n",
      "Epoch 437 Batch 30 Loss 0.0013 Accuracy 0.3492\n",
      "Epoch 437 Loss 0.0013 Accuracy 0.3506\n",
      "Time taken for 1 epoch: 7.8209614753723145 secs\n",
      "\n",
      "Epoch 438 Batch 0 Loss 0.0006 Accuracy 0.3451\n",
      "Epoch 438 Batch 15 Loss 0.0009 Accuracy 0.3439\n",
      "Epoch 438 Batch 30 Loss 0.0007 Accuracy 0.3574\n",
      "Epoch 438 Loss 0.0007 Accuracy 0.3580\n",
      "Time taken for 1 epoch: 7.68262791633606 secs\n",
      "\n",
      "Epoch 439 Batch 0 Loss 0.0039 Accuracy 0.3834\n",
      "Epoch 439 Batch 15 Loss 0.0007 Accuracy 0.3503\n",
      "Epoch 439 Batch 30 Loss 0.0012 Accuracy 0.3531\n",
      "Epoch 439 Loss 0.0010 Accuracy 0.3480\n",
      "Time taken for 1 epoch: 7.98274564743042 secs\n",
      "\n",
      "Epoch 440 Batch 0 Loss 0.0001 Accuracy 0.3530\n",
      "Epoch 440 Batch 15 Loss 0.0008 Accuracy 0.3645\n",
      "Epoch 440 Batch 30 Loss 0.0008 Accuracy 0.3564\n",
      "Epoch 440 Loss 0.0008 Accuracy 0.3542\n",
      "Time taken for 1 epoch: 7.6633172035217285 secs\n",
      "\n",
      "Epoch 441 Batch 0 Loss 0.0023 Accuracy 0.3480\n",
      "Epoch 441 Batch 15 Loss 0.0010 Accuracy 0.3586\n",
      "Epoch 441 Batch 30 Loss 0.0012 Accuracy 0.3570\n",
      "Epoch 441 Loss 0.0012 Accuracy 0.3523\n",
      "Time taken for 1 epoch: 7.762599945068359 secs\n",
      "\n",
      "Epoch 442 Batch 0 Loss 0.0045 Accuracy 0.3290\n",
      "Epoch 442 Batch 15 Loss 0.0018 Accuracy 0.3505\n",
      "Epoch 442 Batch 30 Loss 0.0015 Accuracy 0.3490\n",
      "Epoch 442 Loss 0.0014 Accuracy 0.3496\n",
      "Time taken for 1 epoch: 7.808706045150757 secs\n",
      "\n",
      "Epoch 443 Batch 0 Loss 0.0002 Accuracy 0.3763\n",
      "Epoch 443 Batch 15 Loss 0.0017 Accuracy 0.3472\n",
      "Epoch 443 Batch 30 Loss 0.0016 Accuracy 0.3540\n",
      "Epoch 443 Loss 0.0014 Accuracy 0.3595\n",
      "Time taken for 1 epoch: 7.524225234985352 secs\n",
      "\n",
      "Epoch 444 Batch 0 Loss 0.0046 Accuracy 0.4234\n",
      "Epoch 444 Batch 15 Loss 0.0015 Accuracy 0.3571\n",
      "Epoch 444 Batch 30 Loss 0.0012 Accuracy 0.3587\n",
      "Epoch 444 Loss 0.0012 Accuracy 0.3508\n",
      "Time taken for 1 epoch: 7.84909987449646 secs\n",
      "\n",
      "Epoch 445 Batch 0 Loss 0.0019 Accuracy 0.3948\n",
      "Epoch 445 Batch 15 Loss 0.0011 Accuracy 0.3670\n",
      "Epoch 445 Batch 30 Loss 0.0012 Accuracy 0.3602\n",
      "Epoch 445 Loss 0.0012 Accuracy 0.3575\n",
      "Time taken for 1 epoch: 7.642687797546387 secs\n",
      "\n",
      "Epoch 446 Batch 0 Loss 0.0003 Accuracy 0.2825\n",
      "Epoch 446 Batch 15 Loss 0.0007 Accuracy 0.3642\n",
      "Epoch 446 Batch 30 Loss 0.0006 Accuracy 0.3575\n",
      "Epoch 446 Loss 0.0007 Accuracy 0.3567\n",
      "Time taken for 1 epoch: 7.60542631149292 secs\n",
      "\n",
      "Epoch 447 Batch 0 Loss 0.0052 Accuracy 0.3453\n",
      "Epoch 447 Batch 15 Loss 0.0012 Accuracy 0.3582\n",
      "Epoch 447 Batch 30 Loss 0.0012 Accuracy 0.3500\n",
      "Epoch 447 Loss 0.0010 Accuracy 0.3504\n",
      "Time taken for 1 epoch: 7.923876762390137 secs\n",
      "\n",
      "Epoch 448 Batch 0 Loss 0.0001 Accuracy 0.3121\n",
      "Epoch 448 Batch 15 Loss 0.0008 Accuracy 0.3536\n",
      "Epoch 448 Batch 30 Loss 0.0010 Accuracy 0.3475\n",
      "Epoch 448 Loss 0.0009 Accuracy 0.3484\n",
      "Time taken for 1 epoch: 7.955029726028442 secs\n",
      "\n",
      "Epoch 449 Batch 0 Loss 0.0013 Accuracy 0.3240\n",
      "Epoch 449 Batch 15 Loss 0.0011 Accuracy 0.3681\n",
      "Epoch 449 Batch 30 Loss 0.0010 Accuracy 0.3506\n",
      "Epoch 449 Loss 0.0009 Accuracy 0.3523\n",
      "Time taken for 1 epoch: 7.753561735153198 secs\n",
      "\n",
      "Epoch 450 Batch 0 Loss 0.0014 Accuracy 0.3425\n",
      "Epoch 450 Batch 15 Loss 0.0006 Accuracy 0.3616\n",
      "Epoch 450 Batch 30 Loss 0.0011 Accuracy 0.3559\n",
      "Epoch 450 Loss 0.0011 Accuracy 0.3569\n",
      "Time taken for 1 epoch: 7.6504998207092285 secs\n",
      "\n",
      "Epoch 451 Batch 0 Loss 0.0001 Accuracy 0.3421\n",
      "Epoch 451 Batch 15 Loss 0.0009 Accuracy 0.3686\n",
      "Epoch 451 Batch 30 Loss 0.0013 Accuracy 0.3573\n",
      "Epoch 451 Loss 0.0017 Accuracy 0.3557\n",
      "Time taken for 1 epoch: 7.671720504760742 secs\n",
      "\n",
      "Epoch 452 Batch 0 Loss 0.0004 Accuracy 0.3881\n",
      "Epoch 452 Batch 15 Loss 0.0011 Accuracy 0.3621\n",
      "Epoch 452 Batch 30 Loss 0.0013 Accuracy 0.3555\n",
      "Epoch 452 Loss 0.0012 Accuracy 0.3571\n",
      "Time taken for 1 epoch: 7.579703330993652 secs\n",
      "\n",
      "Epoch 453 Batch 0 Loss 0.0013 Accuracy 0.3750\n",
      "Epoch 453 Batch 15 Loss 0.0014 Accuracy 0.3465\n",
      "Epoch 453 Batch 30 Loss 0.0010 Accuracy 0.3463\n",
      "Epoch 453 Loss 0.0010 Accuracy 0.3479\n",
      "Time taken for 1 epoch: 7.99468994140625 secs\n",
      "\n",
      "Epoch 454 Batch 0 Loss 0.0004 Accuracy 0.3281\n",
      "Epoch 454 Batch 15 Loss 0.0008 Accuracy 0.3475\n",
      "Epoch 454 Batch 30 Loss 0.0007 Accuracy 0.3535\n",
      "Epoch 454 Loss 0.0007 Accuracy 0.3530\n",
      "Time taken for 1 epoch: 7.807075262069702 secs\n",
      "\n",
      "Epoch 455 Batch 0 Loss 0.0014 Accuracy 0.4407\n",
      "Epoch 455 Batch 15 Loss 0.0008 Accuracy 0.3589\n",
      "Epoch 455 Batch 30 Loss 0.0007 Accuracy 0.3526\n",
      "Epoch 455 Loss 0.0009 Accuracy 0.3576\n",
      "Time taken for 1 epoch: 7.602816820144653 secs\n",
      "\n",
      "Epoch 456 Batch 0 Loss 0.0011 Accuracy 0.3226\n",
      "Epoch 456 Batch 15 Loss 0.0004 Accuracy 0.3501\n",
      "Epoch 456 Batch 30 Loss 0.0009 Accuracy 0.3540\n",
      "Epoch 456 Loss 0.0011 Accuracy 0.3570\n",
      "Time taken for 1 epoch: 7.665261268615723 secs\n",
      "\n",
      "Epoch 457 Batch 0 Loss 0.0002 Accuracy 0.3151\n",
      "Epoch 457 Batch 15 Loss 0.0006 Accuracy 0.3448\n",
      "Epoch 457 Batch 30 Loss 0.0007 Accuracy 0.3489\n",
      "Epoch 457 Loss 0.0006 Accuracy 0.3501\n",
      "Time taken for 1 epoch: 7.81662917137146 secs\n",
      "\n",
      "Epoch 458 Batch 0 Loss 0.0012 Accuracy 0.3589\n",
      "Epoch 458 Batch 15 Loss 0.0008 Accuracy 0.3697\n",
      "Epoch 458 Batch 30 Loss 0.0008 Accuracy 0.3571\n",
      "Epoch 458 Loss 0.0010 Accuracy 0.3562\n",
      "Time taken for 1 epoch: 7.75996732711792 secs\n",
      "\n",
      "Epoch 459 Batch 0 Loss 0.0000 Accuracy 0.2648\n",
      "Epoch 459 Batch 15 Loss 0.0004 Accuracy 0.3533\n",
      "Epoch 459 Batch 30 Loss 0.0006 Accuracy 0.3548\n",
      "Epoch 459 Loss 0.0007 Accuracy 0.3564\n",
      "Time taken for 1 epoch: 7.695636034011841 secs\n",
      "\n",
      "Epoch 460 Batch 0 Loss 0.0002 Accuracy 0.4191\n",
      "Epoch 460 Batch 15 Loss 0.0019 Accuracy 0.3537\n",
      "Epoch 460 Batch 30 Loss 0.0014 Accuracy 0.3535\n",
      "Epoch 460 Loss 0.0015 Accuracy 0.3599\n",
      "Time taken for 1 epoch: 7.557274341583252 secs\n",
      "\n",
      "Epoch 461 Batch 0 Loss 0.0013 Accuracy 0.3295\n",
      "Epoch 461 Batch 15 Loss 0.0012 Accuracy 0.3498\n",
      "Epoch 461 Batch 30 Loss 0.0011 Accuracy 0.3479\n",
      "Epoch 461 Loss 0.0011 Accuracy 0.3507\n",
      "Time taken for 1 epoch: 7.929471015930176 secs\n",
      "\n",
      "Epoch 462 Batch 0 Loss 0.0001 Accuracy 0.3800\n",
      "Epoch 462 Batch 15 Loss 0.0010 Accuracy 0.3650\n",
      "Epoch 462 Batch 30 Loss 0.0010 Accuracy 0.3615\n",
      "Epoch 462 Loss 0.0011 Accuracy 0.3582\n",
      "Time taken for 1 epoch: 7.605877637863159 secs\n",
      "\n",
      "Epoch 463 Batch 0 Loss 0.0001 Accuracy 0.3277\n",
      "Epoch 463 Batch 15 Loss 0.0006 Accuracy 0.3523\n",
      "Epoch 463 Batch 30 Loss 0.0006 Accuracy 0.3505\n",
      "Epoch 463 Loss 0.0010 Accuracy 0.3529\n",
      "Time taken for 1 epoch: 7.8014562129974365 secs\n",
      "\n",
      "Epoch 464 Batch 0 Loss 0.0006 Accuracy 0.3121\n",
      "Epoch 464 Batch 15 Loss 0.0007 Accuracy 0.3502\n",
      "Epoch 464 Batch 30 Loss 0.0009 Accuracy 0.3527\n",
      "Epoch 464 Loss 0.0008 Accuracy 0.3545\n",
      "Time taken for 1 epoch: 7.720486640930176 secs\n",
      "\n",
      "Epoch 465 Batch 0 Loss 0.0002 Accuracy 0.3421\n",
      "Epoch 465 Batch 15 Loss 0.0006 Accuracy 0.3636\n",
      "Epoch 465 Batch 30 Loss 0.0007 Accuracy 0.3541\n",
      "Epoch 465 Loss 0.0008 Accuracy 0.3527\n",
      "Time taken for 1 epoch: 7.806803941726685 secs\n",
      "\n",
      "Epoch 466 Batch 0 Loss 0.0016 Accuracy 0.3346\n",
      "Epoch 466 Batch 15 Loss 0.0013 Accuracy 0.3684\n",
      "Epoch 466 Batch 30 Loss 0.0013 Accuracy 0.3503\n",
      "Epoch 466 Loss 0.0016 Accuracy 0.3545\n",
      "Time taken for 1 epoch: 7.774904727935791 secs\n",
      "\n",
      "Epoch 467 Batch 0 Loss 0.0031 Accuracy 0.3906\n",
      "Epoch 467 Batch 15 Loss 0.0014 Accuracy 0.3507\n",
      "Epoch 467 Batch 30 Loss 0.0014 Accuracy 0.3485\n",
      "Epoch 467 Loss 0.0012 Accuracy 0.3483\n",
      "Time taken for 1 epoch: 7.8763415813446045 secs\n",
      "\n",
      "Epoch 468 Batch 0 Loss 0.0001 Accuracy 0.3649\n",
      "Epoch 468 Batch 15 Loss 0.0012 Accuracy 0.3496\n",
      "Epoch 468 Batch 30 Loss 0.0009 Accuracy 0.3522\n",
      "Epoch 468 Loss 0.0010 Accuracy 0.3575\n",
      "Time taken for 1 epoch: 7.6447343826293945 secs\n",
      "\n",
      "Epoch 469 Batch 0 Loss 0.0002 Accuracy 0.4041\n",
      "Epoch 469 Batch 15 Loss 0.0004 Accuracy 0.3715\n",
      "Epoch 469 Batch 30 Loss 0.0006 Accuracy 0.3597\n",
      "Epoch 469 Loss 0.0006 Accuracy 0.3569\n",
      "Time taken for 1 epoch: 7.6573920249938965 secs\n",
      "\n",
      "Epoch 470 Batch 0 Loss 0.0005 Accuracy 0.3643\n",
      "Epoch 470 Batch 15 Loss 0.0006 Accuracy 0.3499\n",
      "Epoch 470 Batch 30 Loss 0.0006 Accuracy 0.3574\n",
      "Epoch 470 Loss 0.0006 Accuracy 0.3553\n",
      "Time taken for 1 epoch: 7.758575916290283 secs\n",
      "\n",
      "Epoch 471 Batch 0 Loss 0.0018 Accuracy 0.2905\n",
      "Epoch 471 Batch 15 Loss 0.0012 Accuracy 0.3617\n",
      "Epoch 471 Batch 30 Loss 0.0009 Accuracy 0.3602\n",
      "Epoch 471 Loss 0.0010 Accuracy 0.3555\n",
      "Time taken for 1 epoch: 7.649356365203857 secs\n",
      "\n",
      "Epoch 472 Batch 0 Loss 0.0000 Accuracy 0.3613\n",
      "Epoch 472 Batch 15 Loss 0.0009 Accuracy 0.3571\n",
      "Epoch 472 Batch 30 Loss 0.0011 Accuracy 0.3482\n",
      "Epoch 472 Loss 0.0009 Accuracy 0.3471\n",
      "Time taken for 1 epoch: 8.04678225517273 secs\n",
      "\n",
      "Epoch 473 Batch 0 Loss 0.0009 Accuracy 0.3821\n",
      "Epoch 473 Batch 15 Loss 0.0012 Accuracy 0.3516\n",
      "Epoch 473 Batch 30 Loss 0.0011 Accuracy 0.3578\n",
      "Epoch 473 Loss 0.0010 Accuracy 0.3516\n",
      "Time taken for 1 epoch: 7.949807405471802 secs\n",
      "\n",
      "Epoch 474 Batch 0 Loss 0.0013 Accuracy 0.3643\n",
      "Epoch 474 Batch 15 Loss 0.0016 Accuracy 0.3415\n",
      "Epoch 474 Batch 30 Loss 0.0017 Accuracy 0.3486\n",
      "Epoch 474 Loss 0.0015 Accuracy 0.3528\n",
      "Time taken for 1 epoch: 7.872304201126099 secs\n",
      "\n",
      "Epoch 475 Batch 0 Loss 0.0018 Accuracy 0.3807\n",
      "Epoch 475 Batch 15 Loss 0.0009 Accuracy 0.3573\n",
      "Epoch 475 Batch 30 Loss 0.0010 Accuracy 0.3540\n",
      "Epoch 475 Loss 0.0009 Accuracy 0.3517\n",
      "Time taken for 1 epoch: 7.844005823135376 secs\n",
      "\n",
      "Epoch 476 Batch 0 Loss 0.0000 Accuracy 0.3661\n",
      "Epoch 476 Batch 15 Loss 0.0006 Accuracy 0.3457\n",
      "Epoch 476 Batch 30 Loss 0.0010 Accuracy 0.3541\n",
      "Epoch 476 Loss 0.0010 Accuracy 0.3547\n",
      "Time taken for 1 epoch: 7.657133102416992 secs\n",
      "\n",
      "Epoch 477 Batch 0 Loss 0.0003 Accuracy 0.2905\n",
      "Epoch 477 Batch 15 Loss 0.0013 Accuracy 0.3437\n",
      "Epoch 477 Batch 30 Loss 0.0018 Accuracy 0.3487\n",
      "Epoch 477 Loss 0.0015 Accuracy 0.3511\n",
      "Time taken for 1 epoch: 7.83406662940979 secs\n",
      "\n",
      "Epoch 478 Batch 0 Loss 0.0006 Accuracy 0.2950\n",
      "Epoch 478 Batch 15 Loss 0.0006 Accuracy 0.3455\n",
      "Epoch 478 Batch 30 Loss 0.0007 Accuracy 0.3493\n",
      "Epoch 478 Loss 0.0006 Accuracy 0.3495\n",
      "Time taken for 1 epoch: 7.8632972240448 secs\n",
      "\n",
      "Epoch 479 Batch 0 Loss 0.0001 Accuracy 0.3815\n",
      "Epoch 479 Batch 15 Loss 0.0011 Accuracy 0.3487\n",
      "Epoch 479 Batch 30 Loss 0.0012 Accuracy 0.3514\n",
      "Epoch 479 Loss 0.0010 Accuracy 0.3529\n",
      "Time taken for 1 epoch: 7.800532341003418 secs\n",
      "\n",
      "Epoch 480 Batch 0 Loss 0.0042 Accuracy 0.3643\n",
      "Epoch 480 Batch 15 Loss 0.0010 Accuracy 0.3447\n",
      "Epoch 480 Batch 30 Loss 0.0012 Accuracy 0.3472\n",
      "Epoch 480 Loss 0.0011 Accuracy 0.3512\n",
      "Time taken for 1 epoch: 7.846673965454102 secs\n",
      "\n",
      "Epoch 481 Batch 0 Loss 0.0020 Accuracy 0.3511\n",
      "Epoch 481 Batch 15 Loss 0.0009 Accuracy 0.3443\n",
      "Epoch 481 Batch 30 Loss 0.0008 Accuracy 0.3521\n",
      "Epoch 481 Loss 0.0007 Accuracy 0.3520\n",
      "Time taken for 1 epoch: 7.891350984573364 secs\n",
      "\n",
      "Epoch 482 Batch 0 Loss 0.0013 Accuracy 0.3174\n",
      "Epoch 482 Batch 15 Loss 0.0012 Accuracy 0.3664\n",
      "Epoch 482 Batch 30 Loss 0.0012 Accuracy 0.3614\n",
      "Epoch 482 Loss 0.0011 Accuracy 0.3578\n",
      "Time taken for 1 epoch: 7.69795298576355 secs\n",
      "\n",
      "Epoch 483 Batch 0 Loss 0.0018 Accuracy 0.4462\n",
      "Epoch 483 Batch 15 Loss 0.0013 Accuracy 0.3756\n",
      "Epoch 483 Batch 30 Loss 0.0011 Accuracy 0.3652\n",
      "Epoch 483 Loss 0.0009 Accuracy 0.3648\n",
      "Time taken for 1 epoch: 7.386550426483154 secs\n",
      "\n",
      "Epoch 484 Batch 0 Loss 0.0001 Accuracy 0.2921\n",
      "Epoch 484 Batch 15 Loss 0.0010 Accuracy 0.3512\n",
      "Epoch 484 Batch 30 Loss 0.0008 Accuracy 0.3507\n",
      "Epoch 484 Loss 0.0010 Accuracy 0.3546\n",
      "Time taken for 1 epoch: 7.779463291168213 secs\n",
      "\n",
      "Epoch 485 Batch 0 Loss 0.0001 Accuracy 0.2861\n",
      "Epoch 485 Batch 15 Loss 0.0011 Accuracy 0.3516\n",
      "Epoch 485 Batch 30 Loss 0.0011 Accuracy 0.3493\n",
      "Epoch 485 Loss 0.0014 Accuracy 0.3491\n",
      "Time taken for 1 epoch: 8.00407361984253 secs\n",
      "\n",
      "Epoch 486 Batch 0 Loss 0.0001 Accuracy 0.3558\n",
      "Epoch 486 Batch 15 Loss 0.0009 Accuracy 0.3487\n",
      "Epoch 486 Batch 30 Loss 0.0011 Accuracy 0.3460\n",
      "Epoch 486 Loss 0.0013 Accuracy 0.3538\n",
      "Time taken for 1 epoch: 7.694940567016602 secs\n",
      "\n",
      "Epoch 487 Batch 0 Loss 0.0001 Accuracy 0.4302\n",
      "Epoch 487 Batch 15 Loss 0.0010 Accuracy 0.3580\n",
      "Epoch 487 Batch 30 Loss 0.0011 Accuracy 0.3492\n",
      "Epoch 487 Loss 0.0011 Accuracy 0.3521\n",
      "Time taken for 1 epoch: 7.687472105026245 secs\n",
      "\n",
      "Epoch 488 Batch 0 Loss 0.0009 Accuracy 0.4117\n",
      "Epoch 488 Batch 15 Loss 0.0012 Accuracy 0.3615\n",
      "Epoch 488 Batch 30 Loss 0.0011 Accuracy 0.3601\n",
      "Epoch 488 Loss 0.0010 Accuracy 0.3580\n",
      "Time taken for 1 epoch: 7.644273042678833 secs\n",
      "\n",
      "Epoch 489 Batch 0 Loss 0.0003 Accuracy 0.3805\n",
      "Epoch 489 Batch 15 Loss 0.0010 Accuracy 0.3566\n",
      "Epoch 489 Batch 30 Loss 0.0012 Accuracy 0.3523\n",
      "Epoch 489 Loss 0.0011 Accuracy 0.3582\n",
      "Time taken for 1 epoch: 7.657787322998047 secs\n",
      "\n",
      "Epoch 490 Batch 0 Loss 0.0001 Accuracy 0.3527\n",
      "Epoch 490 Batch 15 Loss 0.0007 Accuracy 0.3555\n",
      "Epoch 490 Batch 30 Loss 0.0006 Accuracy 0.3625\n",
      "Epoch 490 Loss 0.0007 Accuracy 0.3590\n",
      "Time taken for 1 epoch: 7.547029256820679 secs\n",
      "\n",
      "Epoch 491 Batch 0 Loss 0.0000 Accuracy 0.3104\n",
      "Epoch 491 Batch 15 Loss 0.0010 Accuracy 0.3602\n",
      "Epoch 491 Batch 30 Loss 0.0010 Accuracy 0.3531\n",
      "Epoch 491 Loss 0.0009 Accuracy 0.3504\n",
      "Time taken for 1 epoch: 7.865369081497192 secs\n",
      "\n",
      "Epoch 492 Batch 0 Loss 0.0003 Accuracy 0.3679\n",
      "Epoch 492 Batch 15 Loss 0.0009 Accuracy 0.3613\n",
      "Epoch 492 Batch 30 Loss 0.0010 Accuracy 0.3559\n",
      "Epoch 492 Loss 0.0009 Accuracy 0.3525\n",
      "Time taken for 1 epoch: 7.829137086868286 secs\n",
      "\n",
      "Epoch 493 Batch 0 Loss 0.0004 Accuracy 0.3308\n",
      "Epoch 493 Batch 15 Loss 0.0006 Accuracy 0.3544\n",
      "Epoch 493 Batch 30 Loss 0.0007 Accuracy 0.3506\n",
      "Epoch 493 Loss 0.0007 Accuracy 0.3521\n",
      "Time taken for 1 epoch: 7.790461778640747 secs\n",
      "\n",
      "Epoch 494 Batch 0 Loss 0.0000 Accuracy 0.3367\n",
      "Epoch 494 Batch 15 Loss 0.0002 Accuracy 0.3507\n",
      "Epoch 494 Batch 30 Loss 0.0004 Accuracy 0.3552\n",
      "Epoch 494 Loss 0.0005 Accuracy 0.3528\n",
      "Time taken for 1 epoch: 7.785519361495972 secs\n",
      "\n",
      "Epoch 495 Batch 0 Loss 0.0027 Accuracy 0.3692\n",
      "Epoch 495 Batch 15 Loss 0.0008 Accuracy 0.3531\n",
      "Epoch 495 Batch 30 Loss 0.0008 Accuracy 0.3606\n",
      "Epoch 495 Loss 0.0008 Accuracy 0.3535\n",
      "Time taken for 1 epoch: 7.686485528945923 secs\n",
      "\n",
      "Epoch 496 Batch 0 Loss 0.0004 Accuracy 0.4021\n",
      "Epoch 496 Batch 15 Loss 0.0008 Accuracy 0.3620\n",
      "Epoch 496 Batch 30 Loss 0.0007 Accuracy 0.3570\n",
      "Epoch 496 Loss 0.0007 Accuracy 0.3527\n",
      "Time taken for 1 epoch: 7.752979040145874 secs\n",
      "\n",
      "Epoch 497 Batch 0 Loss 0.0007 Accuracy 0.3674\n",
      "Epoch 497 Batch 15 Loss 0.0005 Accuracy 0.3365\n",
      "Epoch 497 Batch 30 Loss 0.0008 Accuracy 0.3465\n",
      "Epoch 497 Loss 0.0008 Accuracy 0.3499\n",
      "Time taken for 1 epoch: 7.923328399658203 secs\n",
      "\n",
      "Epoch 498 Batch 0 Loss 0.0002 Accuracy 0.3196\n",
      "Epoch 498 Batch 15 Loss 0.0010 Accuracy 0.3512\n",
      "Epoch 498 Batch 30 Loss 0.0012 Accuracy 0.3504\n",
      "Epoch 498 Loss 0.0010 Accuracy 0.3509\n",
      "Time taken for 1 epoch: 7.789012908935547 secs\n",
      "\n",
      "Epoch 499 Batch 0 Loss 0.0000 Accuracy 0.4046\n",
      "Epoch 499 Batch 15 Loss 0.0007 Accuracy 0.3398\n",
      "Epoch 499 Batch 30 Loss 0.0011 Accuracy 0.3481\n",
      "Epoch 499 Loss 0.0025 Accuracy 0.3505\n",
      "Time taken for 1 epoch: 7.912517786026001 secs\n",
      "\n",
      "Epoch 500 Batch 0 Loss 0.0001 Accuracy 0.3505\n",
      "Epoch 500 Batch 15 Loss 0.0010 Accuracy 0.3488\n",
      "Epoch 500 Batch 30 Loss 0.0011 Accuracy 0.3514\n",
      "Epoch 500 Loss 0.0013 Accuracy 0.3511\n",
      "Time taken for 1 epoch: 7.852900505065918 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 500\n",
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "  \n",
    "  train_loss.reset_states()\n",
    "  train_accuracy.reset_states()\n",
    "  \n",
    "  # inp -> portuguese, tar -> english\n",
    "  for (batch, (inp, tar)) in enumerate(train_dataset):\n",
    "    train_step(inp, tar)\n",
    "    \n",
    "    if batch % 15 == 0:\n",
    "      print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
    "          epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
    "   \n",
    "  accuracy = train_accuracy.result()\n",
    "  if (epoch + 1) % 1 == 0 and accuracy_saved < accuracy:\n",
    "    accuracy_saved = accuracy\n",
    "    ckpt_save_path = ckpt_manager.save()\n",
    "    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                         ckpt_save_path))\n",
    "    \n",
    "  print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
    "                                                train_loss.result(), \n",
    "                                                accuracy))\n",
    "\n",
    "  print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QfcsSWswSdGV"
   },
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y6APsFrgImLW"
   },
   "source": [
    "The following steps are used for evaluation:\n",
    "\n",
    "* Encode the input sentence using the Portuguese tokenizer (`tokenizer_pt`). Moreover, add the start and end token so the input is equivalent to what the model is trained with. This is the encoder input.\n",
    "* The decoder input is the `start token == tokenizer_en.vocab_size`.\n",
    "* Calculate the padding masks and the look ahead masks.\n",
    "* The `decoder` then outputs the predictions by looking at the `encoder output` and its own output (self-attention).\n",
    "* Select the last word and calculate the argmax of that.\n",
    "* Concatentate the predicted word to the decoder input as pass it to the decoder.\n",
    "* In this approach, the decoder predicts the next word based on the previous words it predicted.\n",
    "\n",
    "Note: The model used here has less capacity to keep the example relatively faster so the predictions maybe less right. To reproduce the results in the paper, use the entire dataset and base transformer model or transformer XL, by changing the hyperparameters above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5buvMlnvyrFm"
   },
   "outputs": [],
   "source": [
    "def evaluate(inp_sentence):\n",
    "  start_token = [tokenizer_pt.vocab_size]\n",
    "  end_token = [tokenizer_pt.vocab_size + 1]\n",
    "  \n",
    "  # inp sentence is portuguese, hence adding the start and end token\n",
    "  inp_sentence = start_token + tokenizer_pt.encode(inp_sentence) + end_token\n",
    "  encoder_input = tf.expand_dims(inp_sentence, 0)\n",
    "  \n",
    "  # as the target is english, the first word to the transformer should be the\n",
    "  # english start token.\n",
    "  decoder_input = [tokenizer_en.vocab_size]\n",
    "  output = tf.expand_dims(decoder_input, 0)\n",
    "    \n",
    "  for i in range(MAX_LENGTH):\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
    "        encoder_input, output)\n",
    "  \n",
    "    # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "    predictions, attention_weights = transformer(encoder_input, \n",
    "                                                 output,\n",
    "                                                 False,\n",
    "                                                 enc_padding_mask,\n",
    "                                                 combined_mask,\n",
    "                                                 dec_padding_mask)\n",
    "    \n",
    "    # select the last word from the seq_len dimension\n",
    "    predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
    "\n",
    "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "    \n",
    "    # return the result if the predicted_id is equal to the end token\n",
    "    if predicted_id == tokenizer_en.vocab_size+1:\n",
    "      return tf.squeeze(output, axis=0), attention_weights\n",
    "    \n",
    "    # concatentate the predicted_id to the output which is given to the decoder\n",
    "    # as its input.\n",
    "    output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "  return tf.squeeze(output, axis=0), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CN-BV43FMBej"
   },
   "outputs": [],
   "source": [
    "def plot_attention_weights(attention, sentence, result, layer):\n",
    "  fig = plt.figure(figsize=(16, 8))\n",
    "  \n",
    "  sentence = tokenizer_pt.encode(sentence)\n",
    "  \n",
    "  attention = tf.squeeze(attention[layer], axis=0)\n",
    "  \n",
    "  for head in range(attention.shape[0]):\n",
    "    ax = fig.add_subplot(2, 4, head+1)\n",
    "    \n",
    "    # plot the attention weights\n",
    "    ax.matshow(attention[head][:-1, :], cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 10}\n",
    "    \n",
    "    ax.set_xticks(range(len(sentence)+2))\n",
    "    ax.set_yticks(range(len(result)))\n",
    "    \n",
    "    ax.set_ylim(len(result)-1.5, -0.5)\n",
    "        \n",
    "    ax.set_xticklabels(\n",
    "        ['<start>']+[tokenizer_pt.decode([i]) for i in sentence]+['<end>'], \n",
    "        fontdict=fontdict, rotation=90)\n",
    "    \n",
    "    ax.set_yticklabels([tokenizer_en.decode([i]) for i in result \n",
    "                        if i < tokenizer_en.vocab_size], \n",
    "                       fontdict=fontdict)\n",
    "    \n",
    "    ax.set_xlabel('Head {}'.format(head+1))\n",
    "  \n",
    "  plt.tight_layout()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lU2_yG_vBGza"
   },
   "outputs": [],
   "source": [
    "def translate(sentence, plot=''):\n",
    "  result, attention_weights = evaluate(sentence)\n",
    "  \n",
    "  predicted_sentence = tokenizer_en.decode([i for i in result \n",
    "                                            if i < tokenizer_en.vocab_size])  \n",
    "\n",
    "#   print(sentence)\n",
    "#   print('------------------')    \n",
    "#   print(predicted_sentence)\n",
    "  print('Input:                 {}'.format(sentence))\n",
    "  print('Predicted translation: {}'.format(predicted_sentence))\n",
    "  \n",
    "  if plot:\n",
    "    plot_attention_weights(attention_weights, sentence, result, plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_predict(ds, count=3, plot=None):\n",
    "    for a, b in ds.take(count):\n",
    "        translate(a.numpy().decode(), plot)\n",
    "        tf.print(b)    \n",
    "        print('---------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:                 Þa wearð hyre mod mycclum onbryrd þuruh þa halgen lare þeah ðe heo þa gyt hæðen wære\n",
      "Predicted translation: þa weorþan hire mod micel onbryrdan þurh se halig lar þeah þe heo þa git hæðen wesan\n",
      "þa weorþan hire mod micel onbryrdan þurh se halig lar þeah þe heo þa git hæðen wesan\n",
      "---------------------------------\n",
      "Input:                 Eugenia þa þæt æðele mæden wel þeah on wisdome and on uðwytegunge\n",
      "Predicted translation: Eugenia þa se æðele mægden wel þeon on wisdom and on uþwitigung\n",
      "Eugenia þa se æðele mægden wel þeon on wisdom and on uþwitigung\n",
      "---------------------------------\n",
      "Input:                 Þa becom hyre on hand þæs halgan apostoles lar Paules þæs mæran ealles manncynnes lareowes\n",
      "Predicted translation: þa becuman heo on hand se halig apostol lar Paulus se mære eal manncynn lareow\n",
      "þa becuman heo on hand se halig apostol lar Paulus se mære eal manncynn lareow\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_predict(train_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:                 il departoit de ses viez choses\n",
      "Predicted translation: il departir de son viez choses\n",
      "il departir de son viez choses\n",
      "---------------------------------\n",
      "Input:                 il repeissoit les famelleus\n",
      "Predicted translation: il departir de son viez choses\n",
      "il repaistre le fameillos\n",
      "---------------------------------\n",
      "Input:                 ofer seoce hi hyra handa settað\n",
      "Predicted translation: ofer seoc hi hira hand settan\n",
      "ofer seoc hi hira hand settan\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_predict(val_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_1MxkSZvz0jX"
   },
   "source": [
    "You can pass different layers and attention blocks of the decoder to the `plot` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t-kFyiOLH0xg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:                 il repeissoit les famelleus\n",
      "Predicted translation: il departir de son viez choses\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'decoder_layer4_block2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-3138dde6fe58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'decoder_layer4_block2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-61-98c204aacfc8>\u001b[0m in \u001b[0;36mtest_predict\u001b[0;34m(ds, count, plot)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'---------------------------------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-60-a519a2765b75>\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(sentence, plot)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mplot_attention_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-59-92cbf92b0646>\u001b[0m in \u001b[0;36mplot_attention_weights\u001b[0;34m(attention, sentence, result, layer)\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_pt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mhead\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'decoder_layer4_block2'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1152x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_predict(val_examples, 1, plot='decoder_layer4_block2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RqQ1fIsLwkGE"
   },
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, you learned about positional encoding, multi-head attention, the importance of masking and how to create a transformer.\n",
    "\n",
    "Try using a different dataset to train the transformer. You can also create the base transformer or transformer XL by changing the hyperparameters above. You can also use the layers defined here to create [BERT](https://arxiv.org/abs/1810.04805) and train state of the art models. Futhermore, you can implement beam search to get better predictions."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "s_qNSzzyaCbD"
   ],
   "name": "transformer.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0e0d890d0561404f91dbbe32c7a968d1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "1607f2ecd0c64125a081503cc9eb2caa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "danger",
       "description": "  0%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3e679a31c15d401dba8cbb929600c15f",
       "max": 1803,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_30eb27c4cec1420db11ad9b6e4eaa1bf",
       "value": 0
      }
     },
     "16484967d5374168afbca17948a94e31": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1717a8bbc4564373affedb4e11e9aab8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "186792f23e564f8bbf0099d63328c6b4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "194bd44ff15947ac85321fc3be00e932": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1d6b5bd7826d46829c4e1a5b05c1e419": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1e08b4b9bd314039b0eabd873f9daa18": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "1fcc5dce2bec42458512e29ba578ae8f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "27129d6afd1f4cf29b44e87687016eee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "info",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_35c90729b03b4ca8ad9e9cff09e42bee",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_915bc88dd2e2445cae18acca9830b170",
       "value": 1
      }
     },
     "2f526e6e7e954a5da83d333b6fbf2982": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "30eb27c4cec1420db11ad9b6e4eaa1bf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "35c90729b03b4ca8ad9e9cff09e42bee": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "36efa5fe9bb5464e9a6edaa460abb58d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_194bd44ff15947ac85321fc3be00e932",
       "placeholder": "​",
       "style": "IPY_MODEL_9c88f0004c864677b38d89ad1686035c",
       "value": " 19838/51785 [00:00&lt;00:00, 198379.24 examples/s]"
      }
     },
     "377c335c948f4c11ac806d381e2dbee6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3e679a31c15d401dba8cbb929600c15f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3e8606466c774b628ddec7d1d877c1a9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3f892beaeb7942fe87dca1b44813098d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c7ae463517d147d3aeda0c6a8739934b",
       "placeholder": "​",
       "style": "IPY_MODEL_90498e8edda745bc86c008126e8277a5",
       "value": " 0/1193 [00:00&lt;?, ? examples/s]"
      }
     },
     "4459a3d62cf945338dcb31b61503a630": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "456a5511e517483f861c6250ccbdfef6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_822a1fcf13864a6592a208f52d3a45fa",
        "IPY_MODEL_f923fd02533a480d9bc74be4f35c84f1"
       ],
       "layout": "IPY_MODEL_9d825f2724af43b79a6fa1aabb479298"
      }
     },
     "458eb96a7a7f415e97ce61aa558a3c64": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4a6b4c6a3ae24f11adf416e62268d387": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_cecfa836b02c4cdebefd9c601386cf7a",
       "placeholder": "​",
       "style": "IPY_MODEL_1717a8bbc4564373affedb4e11e9aab8",
       "value": " 1803/0 [00:00&lt;00:00, 4788.03 examples/s]"
      }
     },
     "5318f903280b41a2b8da23b81709a776": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5a83ae990ed94f5989cf572b0a8af0be": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "6c2c4480920a4533a78996327463a2b4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "73608d85c72a4dc5b3f6f0bd29a329d4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "79b9d229ccb54716b368f4e572222933": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7d3402aa4b0a4ba6839aa97664f649c9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "Dl Completed...: 100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_1d6b5bd7826d46829c4e1a5b05c1e419",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_1e08b4b9bd314039b0eabd873f9daa18",
       "value": 1
      }
     },
     "7d62cecd8db54cb492ee8f11b4c1e29a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "822a1fcf13864a6592a208f52d3a45fa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "Extraction completed...: 100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2f526e6e7e954a5da83d333b6fbf2982",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_1fcc5dce2bec42458512e29ba578ae8f",
       "value": 1
      }
     },
     "827691b2955b4e4caec35c11e2dd0b5e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_73608d85c72a4dc5b3f6f0bd29a329d4",
       "placeholder": "​",
       "style": "IPY_MODEL_b06137696b774319b960f03ad45af6e4",
       "value": " 1/1 [00:07&lt;00:00,  7.59s/ url]"
      }
     },
     "90498e8edda745bc86c008126e8277a5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "915bc88dd2e2445cae18acca9830b170": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "91b14375c2784c63ae71350d0015d808": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "94ce25087ae44d6abfcc0122a6856c37": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "Dl Size...: 100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_377c335c948f4c11ac806d381e2dbee6",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_0e0d890d0561404f91dbbe32c7a968d1",
       "value": 1
      }
     },
     "97d5e6641cda47d29fc5b8e6ebfedb68": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "99283c9c5e2b405ba10e100f1ca39889": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9c10c7e8975543b28496de7965e2654b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_1607f2ecd0c64125a081503cc9eb2caa",
        "IPY_MODEL_ad61b5359af04ccebafd2b485dfb5065"
       ],
       "layout": "IPY_MODEL_ef7190ea813547028606283a17b1a8ba"
      }
     },
     "9c88f0004c864677b38d89ad1686035c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "9c8bf84c78f94ea989e3dcf57705a246": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_f6e68093dbd74cd4828b9985b181d274",
       "placeholder": "​",
       "style": "IPY_MODEL_7d62cecd8db54cb492ee8f11b4c1e29a",
       "value": " 1193/0 [00:00&lt;00:00, 4394.36 examples/s]"
      }
     },
     "9d825f2724af43b79a6fa1aabb479298": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9f9b566492a9442f9b13547047e78498": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a40d47789da04c05b6ac13081930b181": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3e8606466c774b628ddec7d1d877c1a9",
       "placeholder": "​",
       "style": "IPY_MODEL_da64cb26752f478d894a9c921b3d7727",
       "value": " 51785/0 [00:09&lt;00:00, 5587.32 examples/s]"
      }
     },
     "a5cd4ad1711f4478b56711ebf2e4b74e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_b046b2dc15e24cd990322e3f7eea50f2",
        "IPY_MODEL_4a6b4c6a3ae24f11adf416e62268d387"
       ],
       "layout": "IPY_MODEL_99283c9c5e2b405ba10e100f1ca39889"
      }
     },
     "ad61b5359af04ccebafd2b485dfb5065": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_186792f23e564f8bbf0099d63328c6b4",
       "placeholder": "​",
       "style": "IPY_MODEL_b811bbe8caaf41fcb78f7cbe026b8bf9",
       "value": " 0/1803 [00:00&lt;?, ? examples/s]"
      }
     },
     "b046b2dc15e24cd990322e3f7eea50f2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "info",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_f8275a203aa142f0b69360d0d3843bb4",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_6c2c4480920a4533a78996327463a2b4",
       "value": 1
      }
     },
     "b06137696b774319b960f03ad45af6e4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "b6e57d6bd9f64c8c969e34c68dfae31c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_bf5bc178960a4686bbb35afa9bade9a1",
        "IPY_MODEL_36efa5fe9bb5464e9a6edaa460abb58d"
       ],
       "layout": "IPY_MODEL_b8e9f0f21a604037843c20ffbf562690"
      }
     },
     "b811bbe8caaf41fcb78f7cbe026b8bf9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "b8e9f0f21a604037843c20ffbf562690": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b8f2f9964e4b4a2b87b159ac81f70aa7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "bf5bc178960a4686bbb35afa9bade9a1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "danger",
       "description": " 38%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_16484967d5374168afbca17948a94e31",
       "max": 51785,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_db3e6516398e4e479cc101f889ad9cea",
       "value": 19838
      }
     },
     "c7ae463517d147d3aeda0c6a8739934b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cad5e524a43440b39c25e7902fee31a1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_94ce25087ae44d6abfcc0122a6856c37",
        "IPY_MODEL_fcefc03e80f44fce8a4c2889b7d0a8d3"
       ],
       "layout": "IPY_MODEL_79b9d229ccb54716b368f4e572222933"
      }
     },
     "cecfa836b02c4cdebefd9c601386cf7a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d2b1004ba93f444eb9dadcbd29a0db2f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d82d8c2fbed34fb6be93ea0bd2a383fc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_e715e304faf74221ba1479f4ff1a8847",
        "IPY_MODEL_3f892beaeb7942fe87dca1b44813098d"
       ],
       "layout": "IPY_MODEL_d2b1004ba93f444eb9dadcbd29a0db2f"
      }
     },
     "da64cb26752f478d894a9c921b3d7727": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "db3e6516398e4e479cc101f889ad9cea": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "de497e2297a54a3d8dcb55b192d6718f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_27129d6afd1f4cf29b44e87687016eee",
        "IPY_MODEL_9c8bf84c78f94ea989e3dcf57705a246"
       ],
       "layout": "IPY_MODEL_97d5e6641cda47d29fc5b8e6ebfedb68"
      }
     },
     "e0739ade4c4a44ab82febedaa525a817": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "info",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_9f9b566492a9442f9b13547047e78498",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_b8f2f9964e4b4a2b87b159ac81f70aa7",
       "value": 1
      }
     },
     "e715e304faf74221ba1479f4ff1a8847": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "danger",
       "description": "  0%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_4459a3d62cf945338dcb31b61503a630",
       "max": 1193,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_f5db4d1b1cc84a06abe54a3f457f441d",
       "value": 0
      }
     },
     "e84b3b232edb4df282436c2c4e02c7f4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_e0739ade4c4a44ab82febedaa525a817",
        "IPY_MODEL_a40d47789da04c05b6ac13081930b181"
       ],
       "layout": "IPY_MODEL_5318f903280b41a2b8da23b81709a776"
      }
     },
     "ecd08c8feb254d179d43189954695889": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ef7190ea813547028606283a17b1a8ba": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f154c55ba0434a3194a3369d37cf5654": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_7d3402aa4b0a4ba6839aa97664f649c9",
        "IPY_MODEL_827691b2955b4e4caec35c11e2dd0b5e"
       ],
       "layout": "IPY_MODEL_458eb96a7a7f415e97ce61aa558a3c64"
      }
     },
     "f37d5f48d763494a8674e9d7d58800db": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f5db4d1b1cc84a06abe54a3f457f441d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "f6e68093dbd74cd4828b9985b181d274": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f8275a203aa142f0b69360d0d3843bb4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f923fd02533a480d9bc74be4f35c84f1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_f37d5f48d763494a8674e9d7d58800db",
       "placeholder": "​",
       "style": "IPY_MODEL_5a83ae990ed94f5989cf572b0a8af0be",
       "value": " 1/1 [00:07&lt;00:00,  7.54s/ file]"
      }
     },
     "fcefc03e80f44fce8a4c2889b7d0a8d3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ecd08c8feb254d179d43189954695889",
       "placeholder": "​",
       "style": "IPY_MODEL_91b14375c2784c63ae71350d0015d808",
       "value": " 124/124 [00:07&lt;00:00, 16.39 MiB/s]"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
